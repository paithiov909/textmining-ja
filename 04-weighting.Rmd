# 単語頻度の重みづけ {#weighting}

## tidytextによる重みづけ

`tidytext::bind_tf_idf`を使うと単語頻度からTF-IDFを算出することができます。

```{r}
dat_count |>
  tidytext::bind_tf_idf(token, doc_id, n) |>
  dplyr::slice_max(tf_idf, n = 5L)
```

tidytextにおけるTFとIDFは、RMeCabにおけるそれとは採用している計算式が異なるため、計算結果が異なります。TFはRMeCabでは生の索引語頻度（tfの場合）ですが、tidytextでは文書内での相対頻度になります。また、IDFはRMeCabでは対数の底が2であるのに対して、tidytextでは底が10であるなどの違いがあります。

## gibasaによる重みづけ

gibasaはRMeCabにおける単語頻度の重みづけを`tidytext::bind_tf_idf`と同様のスタイルでおこなうことができる関数`gibasa::bind_tf_idf2`を提供しています。

`bind_tf_idf2`はおおむねRMeCabの単語頻度の重みづけの挙動を再現していますが、細かな点が異なります。たとえば、gibasaは「コサイン正規化」がIDFの計算時のみにおこなわれるため、`norm=TRUE`にしてもRMeCabの計算結果とは一致しません。

RMeCabは以下の単語頻度の重みづけをサポートしています。

-   局所的重み（TF）
    -   tf（索引語頻度）
    -   tf2（対数化索引語頻度）
    -   tf3（２進重み）
-   大域的重み（IDF）
    -   idf（文書頻度の逆数）
    -   idf2（大域的IDF）
    -   idf3（確率的IDF）
    -   idf4（エントロピー）
-   正規化
    -   norm（コサイン正規化）

gibasaはこれらの重みづけを再実装しています。ただし、`tf="tf"`はgibasaでは相対頻度になるため、RMeCabの`weight="tf*idf"`に相当する出力を得るには、たとえば次のように計算します。

```{r}
dat_count |>
  gibasa::bind_tf_idf2(token, doc_id, n) |>
  dplyr::mutate(
    tf_idf = n / idf
  ) |>
  dplyr::slice_max(tf_idf, n = 5L)
```

なお、注意点として、RMeCabの単語を数える機能は、品詞情報（POS1とPOS2まで）を確認しながら単語を数えているようなので、ここでのように原形だけを見て数えた結果とは必ずしも一致しません。

## udpipeによる重みづけ

[udpipe](https://bnosac.github.io/udpipe/en/)を使っても単語頻度とTF-IDFを算出できます。また、`udpipe::document_term_frequencies_statistics`では、TF、IDFとTF-IDFにくわえて、[Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25)を計算することができます。

dplyrを使っていればあまり意識する必要はないと思いますが、udpipeのこのあたりの関数の戻り値はdata.tableである点に注意してください。

```{r}
suppressPackageStartupMessages(require(dplyr))

dat |>
  gibasa::prettify(col_select = c("POS1", "Original")) |>
  dplyr::filter(POS1 %in% c("名詞", "動詞", "形容詞")) |>
  dplyr::mutate(
    doc_id = forcats::fct_drop(doc_id),
    token = dplyr::if_else(is.na(Original), token, Original)
  ) |>
  udpipe::document_term_frequencies(document = "doc_id", term = "token") |>
  udpipe::document_term_frequencies_statistics(k = 2.0) |>
  dplyr::slice_max(bm25, n = 5L)
```

## tidyloによる重みづけ

TF-IDFによる単語頻度の重みづけのモチベーションは、索引語のなかでも特定の文書だけに多く出現していて、ほかの文書ではそれほど出現しないような「レアな単語」を調べることにあります。

こうしたことを実現するための値として、[tidylo](https://github.com/juliasilge/tidylo)パッケージでは「重み付きログオッズ（weighted log odds）」を計算することができます。

```{r}
dat_count |>
  tidylo::bind_log_odds(set = doc_id, feature = token, n = n) |>
  dplyr::filter(!is.infinite(log_odds_weighted)) |>
  dplyr::slice_max(log_odds_weighted, n = 5L)
```

ここで用いているデータは小説を改行ごとに一つの文書と見なしていたため、中には次のような極端に短い文書が含まれています。こうした文書では、直観的にはそれほどレアには思われない単語についてもオッズが極端に高くなってしまっているように見えます。

```{r}
dat_txt |>
  dplyr::filter(doc_id %in% c(430, 536, 577, 824)) |>
  dplyr::pull(text)
```

weighted log oddsについては[この資料](https://bookdown.org/Maxine/tidy-text-mining/weighted-log-odds-ratio.html)などを参照してください。
