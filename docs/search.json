[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RとMeCabによる日本語テキストマイニングの前処理",
    "section": "",
    "text": "はじめに",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "index.html#この資料について",
    "href": "index.html#この資料について",
    "title": "RとMeCabによる日本語テキストマイニングの前処理",
    "section": "この資料について",
    "text": "この資料について\n\nこの資料でやりたいこと\ngibasaやその他のRパッケージを使って、RMeCabでできるようなテキストマイニングの前処理をより見通しよくおこなうやり方を紹介します。\n\n\n想定する知識など\nR言語の基本的な使い方の説明はしません。tidyverseなどの使い方については、他の資料を参照してください。参考までに、R言語そのものやtidyverseの使い方についての紹介は次の資料がおすすめです。\n\n私たちのR\nR入門\n\nまた、以降の説明ではRでの日本語テキストの前処理のやり方のみにフォーカスしているため、具体的なテキストデータの分析のやり方には踏み込んでいません。Rでおこなうようなテキストデータの分析の方法については、いずれも英語の資料ですが、次が参考になると思います（3つめは計量言語学っぽい内容の教科書なので、この資料の読者向けではないかもしれません）。\n\nText Mining with R\nSupervised Machine Learning for Text Analysis in R\nAn Introduction to Quantitative Text Analysis for Linguistics",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "index.html#rでテキストマイニングするということ",
    "href": "index.html#rでテキストマイニングするということ",
    "title": "RとMeCabによる日本語テキストマイニングの前処理",
    "section": "Rでテキストマイニングするということ",
    "text": "Rでテキストマイニングするということ\n\nテキストを分析して何がしたいのか\nテキストマイニングに関する入門的な本だと、「テキストマイニングとは何か」みたいな話から入るような気がします。ここでは必ずしも入門的な内容をめざしてはいませんが、しかし、すこし考えてみましょう。テキストマイニングとはなんでしょうか。\n自然言語処理というのは、まあいろいろと思想はあるでしょうが、総じて「テキストを機械的に処理してごにょごにょする」技術のことだと思います。自然言語処理界隈の論文などを眺めていると、その範囲はかなり広くて、文書要約から文書生成といったタスクまで含まれるようです。\nそのなかでもテキストマイニングというと、「テキストから特徴量をつくって何かを分析する」みたいな部分にフォーカスしてくるのではないでしょうか。\n素人考えですが、テキストマイニングとはしたがってデータ分析のことです。そのため、前提としてテキストを分析して何がしたいのか（＝何ができるのか）を見通しよくしておくと、嬉しいことが多い気がします。\n\n\nテキストマイニングでめざすこと・できること\nCRISP-DM (Cross-Industry Standard Process for Data Mining) は、IBMを中心としたコンソーシアムが提案したデータマイニングのための標準プロセスです。\nこれはデータ分析をビジネスに活かすことを念頭においてつくられた「課題ドリブン」なプロセスであるため、場合によってはそのまま採用できないかもしれませんが、こうした標準プロセスを押さえておくことは、分析プロセスを設計するうえで有用だと思います。\nCRISP-DMは以下の6つの段階（phases）を行ったり来たりすることで進められていきます。\n\nBusiness Understanding\nData Understanding\nData Preparation\nModeling\nEvaluation\nDeployment\n\nCRISP-DMはデータ分析を通じて達成したいことから分析をスタートしていく、ある意味でトップダウン的なプロセスです。しかし、データからの知見の発掘はそんなにトップダウン一直線にはうまくいかないものです。いわばボトムアップ的にも、段階を「行ったり来たり」しながら分析を進めるためには、データ分析でとれるカードをなんとなく把握しておく必要があります。\nこれも素人考えですが、私たちがデータ分析でとれるカードというのは、だいたい次の３つくらいのものです。\n\nモデルをつくって何かの回帰をする\nモデルをつくって何かの分類をする\nグループに分けて違いを評価する\n\nそのために、これらの落としどころに持ち込むためのテキストの特徴量をどうにかしてつくること（前処理）が、私たちが実際におこなうテキストマイニングの大きな部分を占めるように思います。\nそして、それらの特徴量は、テキストについて何かを数えた頻度または比率とそれらを変換したものだと思っておくとすっきりします。数を数える「何か」というのは、たとえば語だったり品詞だったり、それらのNgramだったり、その他のタグ付けされた情報だったりします。\n\n\nテキストマイニングの流れ\nテキストマイニングの大まかな流れは、イメージ的には、次のような感じになります。\n\n分析したいテキストをいっぱい集める\n\n\n分析して何がしたいか考える\nそのためにつくるべき特徴量を考える\n\n\n特徴量をつくる\n\n\n正規化などの文字列処理\nトークナイズ・ステミング・レメタイズ\n集計\n特徴量の変換や補完\n\n\n分析する\n\n\n特徴量をつかってデータ分析する\n得られた結果を評価する\n\n\n（必要に応じて）得られた知見を活かす\n\nこの資料では、この流れのなかでも、2にとくにフォーカスして、テキストの前処理のやり方を説明します。",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  gibasaの基本的な使い方",
    "section": "",
    "text": "1.1 テキストデータ\nここでは、audubonパッケージに含まれているaudubon::polanoというデータを例にgibasaの基本的な使い方を説明していきます。このデータは、青空文庫で公開されている、宮沢賢治の「ポラーノの広場」という小説を、改行ごとにひとつの要素としてベクトルにしたものです。\nこのデータを、次のようなかたちのデータフレーム（tibble）にします。\ndat_txt &lt;-\n  tibble::tibble(\n    doc_id = seq_along(audubon::polano) |&gt; as.character(),\n    text = audubon::polano\n  ) |&gt;\n  dplyr::mutate(text = audubon::strj_normalize(text))\n\nstr(dat_txt)\n#&gt; tibble [899 × 2] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: chr [1:899] \"1\" \"2\" \"3\" \"4\" ...\n#&gt;  $ text  : chr [1:899] \"ポラーノの広場\" \"宮沢賢治\" \"前十七等官レオーノ・キュースト誌\" \"宮沢賢治訳述\" ...\nこのかたちのデータフレームは、Text Interchange Formats（TIF）という仕様を念頭においている形式です（ちなみに、このかたちのデータフレームはreadtextパッケージを使うと簡単に得ることができますが、readtextクラスのオブジェクトはdplyrと相性が悪いようなので、使う場合はdplyr::tibbleなどでtibbleにしてしまうことをおすすめします）。\nText Interchange Formats（TIF）は、2017年にrOpenSci Text Workshopで整備された、テキスト分析用のRパッケージのデザインパターンのようなものです。\nTIFでは、コーパス（corpus）、文書単語行列（dtm）、トークン（token）という3種類のオブジェクトの形式が定義されており、異なるパッケージ間で同様の形式を扱うようにすることで、複数のパッケージを通じて便利にテキスト分析を進められるようになっています。\n上のdat_txtは、文書の集合であるコーパスをデータフレームのかたちで保持したものです。この形式のデータフレームは、次のように、tidytextやtokenizersの関数にそのまま渡すことができます。なお、これらの形式のオブジェクトは、TIFの枠組みのなかではトークンと呼ばれます。\ndat_txt |&gt;\n  tidytext::unnest_tokens(token, text) |&gt;\n  head(4)\n#&gt; # A tibble: 4 × 2\n#&gt;   doc_id token   \n#&gt;   &lt;chr&gt;  &lt;chr&gt;   \n#&gt; 1 1      ポラーノ\n#&gt; 2 1      の      \n#&gt; 3 1      広場    \n#&gt; 4 2      宮沢\n\ndat_txt |&gt;\n  tokenizers::tokenize_words() |&gt;\n  head(4)\n#&gt; $`1`\n#&gt; [1] \"ポラーノ\" \"の\"       \"広場\"    \n#&gt; \n#&gt; $`2`\n#&gt; [1] \"宮沢\" \"賢治\"\n#&gt; \n#&gt; $`3`\n#&gt;  [1] \"前\"     \"十七\"   \"等\"     \"官\"     \"レ\"     \"オー\"   \"ノ\"     \"キュー\"\n#&gt;  [9] \"スト\"   \"誌\"    \n#&gt; \n#&gt; $`4`\n#&gt; [1] \"宮沢\" \"賢治\" \"訳述\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>gibasaの基本的な使い方</span>"
    ]
  },
  {
    "objectID": "intro.html#gibasaの使い方",
    "href": "intro.html#gibasaの使い方",
    "title": "1  gibasaの基本的な使い方",
    "section": "1.2 gibasaの使い方",
    "text": "1.2 gibasaの使い方\n\n1.2.1 tokenize\n前節で見たように、tokenizers::tokenize_wordsやこれを利用しているtidytext::unnest_tokensは、日本語のテキストであっても機械的にトークンのかたちに整形する（分かち書きする）ことができます。\ntokenizersパッケージの分かち書きは、内部的には、ICUのBoundary Analysisによるものです。この単語境界判定は、たとえば新聞記事のような、比較的整った文体の文章ではおおむね期待通り分かち書きがおこなわれ、また、日本語と英語などが混ざっている文章であってもとくに気にすることなく、高速に分かち書きできるという強みがあります。\nしかし、手元にある辞書に収録されている語の通りに分かち書きしたい場合や、品詞情報などがほしい場合には、やはりMeCabのような形態素解析器による分かち書きが便利なこともあります。\ngibasaは、そのようなケースにおいて、tidytext::unnest_tokensの代わりに使用できる機能を提供するために開発しているパッケージです。この機能はgibasa::tokenizeという関数として提供していて、次のように使うことができます。\n\ndat &lt;- gibasa::tokenize(dat_txt, text, doc_id)\nstr(dat)\n#&gt; tibble [26,849 × 5] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id     : Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ token_id   : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ...\n#&gt;  $ token      : chr [1:26849] \"ポラーノ\" \"の\" \"広場\" \"宮沢\" ...\n#&gt;  $ feature    : chr [1:26849] \"名詞,一般,*,*,*,*,*\" \"助詞,連体化,*,*,*,*,の,ノ,ノ\" \"名詞,一般,*,*,*,*,広場,ヒロバ,ヒロバ\" \"名詞,固有名詞,人名,姓,*,*,宮沢,ミヤザワ,ミヤザワ\" ...\n\n\n\n1.2.2 prettify\ngibasa::tokenizeの戻り値のデータフレームは、それぞれのトークンについて、MeCabから返される素性情報のすべてを含んでいるfeatureという列を持っています。\nMeCabから返される素性情報は、使用している辞書によって異なります。たとえば、IPA辞書やUniDic（2.1.2, aka unidic-lite）の素性は、次のような情報を持っています。\n\ngibasa::get_dict_features(\"ipa\")\n#&gt; [1] \"POS1\"        \"POS2\"        \"POS3\"        \"POS4\"        \"X5StageUse1\"\n#&gt; [6] \"X5StageUse2\" \"Original\"    \"Yomi1\"       \"Yomi2\"\ngibasa::get_dict_features(\"unidic26\")\n#&gt;  [1] \"POS1\"      \"POS2\"      \"POS3\"      \"POS4\"      \"cType\"     \"cForm\"    \n#&gt;  [7] \"lForm\"     \"lemma\"     \"orth\"      \"pron\"      \"orthBase\"  \"pronBase\" \n#&gt; [13] \"goshu\"     \"iType\"     \"iForm\"     \"fType\"     \"fForm\"     \"kana\"     \n#&gt; [19] \"kanaBase\"  \"form\"      \"formBase\"  \"iConType\"  \"fConType\"  \"aType\"    \n#&gt; [25] \"aConType\"  \"aModeType\"\n\nこうした素性情報をデータフレームの列にパースするには、gibasa::prettifyという関数を利用できます。\nデフォルトではすべての素性についてパースしますが、col_select引数に残したい列名を指定することにより、特定の素性情報だけをパースすることもできます。このかたちのデータフレームは、解析するテキストの文章量によっては、数十万から数百万くらいの行からなることもよくあります。そのような規模のデータフレームについて、いちいちすべての素性をパースしていると、それだけでメモリを余計に消費してしまいます。メモリの消費を抑えるためにも、なるべく後で必要な素性だけをこまめに指定することをおすすめします。\n\nstr(gibasa::prettify(dat))\n#&gt; tibble [26,849 × 13] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id     : Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ token_id   : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ...\n#&gt;  $ token      : chr [1:26849] \"ポラーノ\" \"の\" \"広場\" \"宮沢\" ...\n#&gt;  $ POS1       : chr [1:26849] \"名詞\" \"助詞\" \"名詞\" \"名詞\" ...\n#&gt;  $ POS2       : chr [1:26849] \"一般\" \"連体化\" \"一般\" \"固有名詞\" ...\n#&gt;  $ POS3       : chr [1:26849] NA NA NA \"人名\" ...\n#&gt;  $ POS4       : chr [1:26849] NA NA NA \"姓\" ...\n#&gt;  $ X5StageUse1: chr [1:26849] NA NA NA NA ...\n#&gt;  $ X5StageUse2: chr [1:26849] NA NA NA NA ...\n#&gt;  $ Original   : chr [1:26849] NA \"の\" \"広場\" \"宮沢\" ...\n#&gt;  $ Yomi1      : chr [1:26849] NA \"ノ\" \"ヒロバ\" \"ミヤザワ\" ...\n#&gt;  $ Yomi2      : chr [1:26849] NA \"ノ\" \"ヒロバ\" \"ミヤザワ\" ...\nstr(gibasa::prettify(dat, col_select = c(1, 2)))\n#&gt; tibble [26,849 × 6] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id     : Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ token_id   : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ...\n#&gt;  $ token      : chr [1:26849] \"ポラーノ\" \"の\" \"広場\" \"宮沢\" ...\n#&gt;  $ POS1       : chr [1:26849] \"名詞\" \"助詞\" \"名詞\" \"名詞\" ...\n#&gt;  $ POS2       : chr [1:26849] \"一般\" \"連体化\" \"一般\" \"固有名詞\" ...\nstr(gibasa::prettify(dat, col_select = c(\"POS1\", \"Original\")))\n#&gt; tibble [26,849 × 6] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id     : Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ token_id   : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ...\n#&gt;  $ token      : chr [1:26849] \"ポラーノ\" \"の\" \"広場\" \"宮沢\" ...\n#&gt;  $ POS1       : chr [1:26849] \"名詞\" \"助詞\" \"名詞\" \"名詞\" ...\n#&gt;  $ Original   : chr [1:26849] NA \"の\" \"広場\" \"宮沢\" ...\n\n\n\n1.2.3 pack\ngibasa::packという関数を使うと、トークンの形式のデータフレームから、各トークンを半角スペースで区切ったコーパスの形式のデータフレームにすることができます。\n\ndat_corpus &lt;- dat |&gt;\n  gibasa::pack()\n\nstr(dat_corpus)\n#&gt; tibble [899 × 2] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ text  : chr [1:899] \"ポラーノ の 広場\" \"宮沢 賢治\" \"前 十 七 等 官 レオーノ・キュースト 誌\" \"宮沢 賢治 訳述\" ...\n\nこのかたちのデータフレームはTIFに準拠していたため、他のパッケージと組み合わせて使うのに便利なことがあります。たとえば、このかたちから、次のようにtidytext::unnest_tokensと組み合わせて、もう一度トークンの形式のデータフレームに戻すことができます。\n\ndat_corpus |&gt;\n  tidytext::unnest_tokens(token, text, token = \\(x) {\n    strsplit(x, \" +\")\n  }) |&gt;\n  head(4)\n#&gt; # A tibble: 4 × 2\n#&gt;   doc_id token   \n#&gt;   &lt;fct&gt;  &lt;chr&gt;   \n#&gt; 1 1      ポラーノ\n#&gt; 2 1      の      \n#&gt; 3 1      広場    \n#&gt; 4 2      宮沢\n\nあるいは、次のようにquantedaと組み合わせて使うこともできます。\n\ndat_corpus |&gt;\n  quanteda::corpus() |&gt;\n  quanteda::tokens(what = \"fastestword\", remove_punct = FALSE)\n#&gt; Tokens consisting of 899 documents.\n#&gt; 1 :\n#&gt; [1] \"ポラーノ\" \"の\"       \"広場\"    \n#&gt; \n#&gt; 2 :\n#&gt; [1] \"宮沢\" \"賢治\"\n#&gt; \n#&gt; 3 :\n#&gt; [1] \"前\"                   \"十\"                   \"七\"                  \n#&gt; [4] \"等\"                   \"官\"                   \"レオーノ・キュースト\"\n#&gt; [7] \"誌\"                  \n#&gt; \n#&gt; 4 :\n#&gt; [1] \"宮沢\" \"賢治\" \"訳述\"\n#&gt; \n#&gt; 5 :\n#&gt;  [1] \"その\"     \"ころ\"     \"わたくし\" \"は\"       \"、\"       \"モリーオ\"\n#&gt;  [7] \"市\"       \"の\"       \"博物\"     \"局\"       \"に\"       \"勤め\"    \n#&gt; [ ... and 5 more ]\n#&gt; \n#&gt; 6 :\n#&gt;  [1] \"十\"   \"八\"   \"等\"   \"官\"   \"でし\" \"た\"   \"から\" \"役所\" \"の\"   \"なか\"\n#&gt; [11] \"でも\" \"、\"  \n#&gt; [ ... and 219 more ]\n#&gt; \n#&gt; [ reached max_ndoc ... 893 more documents ]\n\n\n\n1.2.4 lazy_dtなどと組み合わせて使う場合\ngibasa::prettifyはデータフレームにしか使えないため、data.tableなどと組み合わせて使う場合にはtidyr::separateを代わりに使ってください。\n\ndat_toks &lt;- dat |&gt;\n  dtplyr::lazy_dt() |&gt;\n  tidyr::separate(feature,\n    into = gibasa::get_dict_features(),\n    sep = \",\", extra = \"merge\", fill = \"right\"\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(Original == \"*\", token, Original),\n    token = stringr::str_c(token, POS1, POS2, sep = \"/\")\n  ) |&gt;\n  dplyr::select(doc_id, sentence_id, token_id, token) |&gt;\n  dplyr::as_tibble() |&gt;\n  dplyr::mutate(across(where(is.character), ~ dplyr::na_if(., \"*\")))\n\nstr(dat_toks)\n#&gt; tibble [26,849 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id     : Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ token_id   : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ...\n#&gt;  $ token      : chr [1:26849] \"ポラーノ/名詞/一般\" \"の/助詞/連体化\" \"広場/名詞/一般\" \"宮沢/名詞/固有名詞\" ...",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>gibasaの基本的な使い方</span>"
    ]
  },
  {
    "objectID": "dtm.html",
    "href": "dtm.html",
    "title": "2  トークンの集計と文書単語行列への整形",
    "section": "",
    "text": "2.1 トークンの集計",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>トークンの集計と文書単語行列への整形</span>"
    ]
  },
  {
    "objectID": "dtm.html#トークンの集計",
    "href": "dtm.html#トークンの集計",
    "title": "2  トークンの集計と文書単語行列への整形",
    "section": "",
    "text": "2.1.1 品詞などにもとづくしぼりこみ\nトークンを簡単に集計するには、dplyrの関数群を利用するのが便利です。\nたとえば、集計に先立って特定のトークンを素性情報にもとづいて選択するにはdplyr::filterを使います。\n\ndat |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |&gt;\n  dplyr::filter(POS1 %in% c(\"名詞\", \"動詞\", \"形容詞\")) |&gt;\n  dplyr::slice_head(n = 30L) |&gt;\n  reactable::reactable(compact = TRUE)\n\n\n\n\n\n一方で、以下で紹介するようなトークンの再結合を後からやりたい場合には、この方法は適切ではありません。dplyr::filterを使うとデータフレーム中のトークンを抜き取ってしまうため、この操作をした後では、実際の文書のなかでは隣り合っていないトークンどうしが隣接しているように扱われてしまいます。\n品詞などの情報にもとづいてトークンを取捨選択しつつも、トークンの位置関係はとりあえず保持したいという場合には、gibasa::mute_tokensを使います。この関数は、条件にマッチしたトークンをNA_character_に置き換えます（reactableによる出力のなかでは空白として表示されています）。\n\ndat |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |&gt;\n  gibasa::mute_tokens(!POS1 %in% c(\"名詞\", \"動詞\", \"形容詞\")) |&gt;\n  dplyr::slice_head(n = 30L) |&gt;\n  reactable::reactable(compact = TRUE)\n\n\n\n\n\n\n\n2.1.2 品詞などにもとづくトークンの再結合\nトークンを集計する目的によっては、形態素解析された結果の単語では単位として短すぎることがあります。\nたとえば、IPA辞書では「小田急線」は「小田急（名詞・固有名詞）+線（名詞・接尾）」として解析され、「小田急線」という単語としては解析されません。このように、必ずしも直感的な解析結果がえられないことは、UniDicを利用している場合により頻繁に発生します。実際、UniDicでは「水族館」も「水族（名詞・普通名詞）+館（接尾辞・名詞的）」として解析されるなど、IPA辞書よりもかなり細かな単位に解析されます。\n\n# IPA辞書による解析の例\ngibasa::tokenize(c(\n  \"佐藤さんはそのとき小田急線で江の島水族館に向かっていた\",\n  \"秒速5センチメートルは新海誠が監督した映画作品\",\n  \"辛そうで辛くない少し辛いラー油の辛さ\"\n)) |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"POS2\", \"POS3\")) |&gt;\n  reactable::reactable(compact = TRUE)\n\n\n\n\n\n分析の関心によっては、こうした細かくなりすぎたトークンをまとめあげて、もっと長い単位の単語として扱えると便利かもしれません。\ngibasa::collapse_tokensを使うと、渡された条件にマッチする一連のトークンをまとめあげて、新しいトークンにすることができます。\n\ngibasa::tokenize(c(\n  \"佐藤さんはそのとき小田急線で江の島水族館に向かっていた\",\n  \"秒速5センチメートルは新海誠が監督した映画作品\",\n  \"辛そうで辛くない少し辛いラー油の辛さ\"\n)) |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"POS2\", \"POS3\")) |&gt;\n  gibasa::collapse_tokens(\n    (POS1 %in% c(\"名詞\", \"接頭詞\") &\n      !stringr::str_detect(token, \"^[あ-ン]+$\")) |\n      (POS1 %in% c(\"名詞\", \"形容詞\") &\n        POS2 %in% c(\"自立\", \"接尾\", \"数接続\"))\n  ) |&gt;\n  reactable::reactable(compact = TRUE)\n\n\n\n\n\nこの機能は強力ですが、条件を書くには、利用している辞書の品詞体系について理解している必要があります。また、機械的に処理しているにすぎないため、一部のトークンは、かえって意図しないかたちにまとめあげられてしまう場合があります。あるいは、機械学習の特徴量をつくるのが目的であるケースなどでは、単純にNgramを利用したほうが便利かもしれません。\n\n\n2.1.3 原形の集計\ndplyr::countでトークンを文書ごとに集計します。ここでは、IPA辞書の見出し語がある語については「原形（Original）」を、見出し語がない語（未知語）については表層形を数えています。\nMeCabは、未知語であっても品詞の推定をおこないますが、未知語の場合には「読み（Yomi1, Yomi2）」のような一部の素性については情報を返しません。このような未知語の素性については、prettifyした結果のなかでは、NA_character_になっていることに注意してください。\n\ndat_count &lt;- dat |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |&gt;\n  dplyr::filter(POS1 %in% c(\"名詞\", \"動詞\", \"形容詞\")) |&gt;\n  dplyr::mutate(\n    doc_id = forcats::fct_drop(doc_id),\n    token = dplyr::if_else(is.na(Original), token, Original)\n  ) |&gt;\n  dplyr::count(doc_id, token)\n\nstr(dat_count)\n#&gt; tibble [9,723 × 3] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: Factor w/ 876 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 2 3 3 3 3 3 3 ...\n#&gt;  $ token : chr [1:9723] \"ポラーノ\" \"広場\" \"宮沢\" \"賢治\" ...\n#&gt;  $ n     : int [1:9723] 1 1 1 1 1 1 1 1 1 1 ...",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>トークンの集計と文書単語行列への整形</span>"
    ]
  },
  {
    "objectID": "dtm.html#文書単語行列への整形",
    "href": "dtm.html#文書単語行列への整形",
    "title": "2  トークンの集計と文書単語行列への整形",
    "section": "2.2 文書単語行列への整形",
    "text": "2.2 文書単語行列への整形\nこうして集計した縦持ちの頻度表を横持ちにすると、いわゆる文書単語行列になります。\n\ndtm &lt;- dat_count |&gt;\n  tidyr::pivot_wider(\n    id_cols = doc_id,\n    names_from = token,\n    values_from = n,\n    values_fill = 0\n  )\n\ndim(dtm)\n#&gt; [1]  876 2173\n\nただし、このようにtidyr::pivot_widerで単純に横持ちにすることは、非常に大量の列を持つ巨大なデータフレームを作成することになるため、おすすめしません。文書単語行列を作成するには、tidytext::cast_sparseやtidytext::cast_dfmなどを使って、疎行列のオブジェクトにしましょう。\n\ndtm &lt;- dat_count |&gt;\n  tidytext::cast_sparse(doc_id, token, n)\n\ndim(dtm)\n#&gt; [1]  876 2172",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>トークンの集計と文書単語行列への整形</span>"
    ]
  },
  {
    "objectID": "ngram.html",
    "href": "ngram.html",
    "title": "3  N-gram",
    "section": "",
    "text": "3.1 dplyrを使ってNgramを数える方法\ndplyrを使って簡単にやる場合、次のようにすると2-gramを集計できます。\nbigram &lt;- gibasa::ngram_tokenizer(2)\n\ndat_ngram &lt;- dat |&gt;\n  gibasa::prettify(col_select = \"Original\") |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(Original), token, Original)\n  ) |&gt;\n  dplyr::reframe(token = bigram(token, sep = \"-\"), .by = doc_id) |&gt;\n  dplyr::count(doc_id, token)\n\nstr(dat_ngram)\n#&gt; tibble [24,398 × 3] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 3 3 3 3 3 3 4 ...\n#&gt;  $ token : chr [1:24398] \"の-広場\" \"ポラーノ-の\" \"宮沢-賢治\" \"レオーノ・キュースト-誌\" ...\n#&gt;  $ n     : int [1:24398] 1 1 1 1 1 1 1 1 1 1 ...\nなお、RMeCabでできるような「名詞-名詞」の2-gramだけを抽出したいといったケースでは、2-gramをつくる前に品詞でフィルタしてしまうと元の文書内におけるトークンの隣接関係を破壊してしまい、正しい2-gramを抽出することができません。そのようなことをしたい場合には、あらかじめ品詞のNgramもつくったうえで、後から品詞のNgramでフィルタします。\ndat_ngram &lt;- dat |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(Original), token, Original)\n  ) |&gt;\n  dplyr::reframe(\n    token = bigram(token, sep = \"-\"),\n    pos = bigram(POS1, sep = \"-\"), # 品詞のNgramをつくる\n    .by = doc_id\n  ) |&gt;\n  dplyr::filter(pos %in% c(\"名詞-名詞\")) |&gt; # 品詞のNgramでフィルタする\n  dplyr::count(doc_id, token)\n\nstr(dat_ngram)\n#&gt; tibble [890 × 3] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 2 3 3 3 3 3 4 4 5 5 ...\n#&gt;  $ token : chr [1:890] \"宮沢-賢治\" \"レオーノ・キュースト-誌\" \"七-等\" \"十-七\" ...\n#&gt;  $ n     : int [1:890] 1 1 1 1 1 1 1 1 1 1 ...",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>N-gram</span>"
    ]
  },
  {
    "objectID": "ngram.html#quantedaにngramを持ちこむ方法",
    "href": "ngram.html#quantedaにngramを持ちこむ方法",
    "title": "3  N-gram",
    "section": "3.2 quantedaにNgramを持ちこむ方法",
    "text": "3.2 quantedaにNgramを持ちこむ方法\ngibasa::packを使ってNgramの分かち書きをつくることもできます。この場合、次のようにquantedaの枠組みの中でNgramをトークンとして数えることで集計することができます。\n\ndat_ngram &lt;- dat |&gt;\n  gibasa::prettify(col_select = \"Original\") |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(Original), token, Original)) |&gt;\n  gibasa::pack(n = 2)\n\nstr(dat_ngram)\n#&gt; tibble [899 × 2] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ text  : chr [1:899] \"ポラーノ-の の-広場\" \"宮沢-賢治\" \"前-十 十-七 七-等 等-官 官-レオーノ・キュースト レオーノ・キュースト-誌\" \"宮沢-賢治 賢治-訳述\" ...\n\ndat_ngram |&gt;\n  quanteda::corpus() |&gt;\n  quanteda::tokens(what = \"fastestword\") |&gt;\n  quanteda::dfm()\n#&gt; Document-feature matrix of: 899 documents, 10,890 features (99.75% sparse) and 0 docvars.\n#&gt;     features\n#&gt; docs ポラーノ-の の-広場 宮沢-賢治 前-十 十-七 七-等 等-官\n#&gt;    1           1       1         0     0     0     0     0\n#&gt;    2           0       0         1     0     0     0     0\n#&gt;    3           0       0         0     1     1     1     1\n#&gt;    4           0       0         1     0     0     0     0\n#&gt;    5           0       0         0     0     0     0     0\n#&gt;    6           0       0         0     0     0     0     1\n#&gt;     features\n#&gt; docs 官-レオーノ・キュースト レオーノ・キュースト-誌 賢治-訳述\n#&gt;    1                       0                       0         0\n#&gt;    2                       0                       0         0\n#&gt;    3                       1                       1         0\n#&gt;    4                       0                       0         1\n#&gt;    5                       0                       0         0\n#&gt;    6                       0                       0         0\n#&gt; [ reached max_ndoc ... 893 more documents, reached max_nfeat ... 10,880 more features ]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>N-gram</span>"
    ]
  },
  {
    "objectID": "ngram.html#quantedaでngramを数える方法",
    "href": "ngram.html#quantedaでngramを数える方法",
    "title": "3  N-gram",
    "section": "3.3 quantedaでNgramを数える方法",
    "text": "3.3 quantedaでNgramを数える方法\nまた、quantedaの枠組みの中でNgramをつくりながら数えて集計することもできます。\n\ndat_ngram &lt;- dat |&gt;\n  gibasa::prettify(col_select = \"Original\") |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(Original), token, Original)) |&gt;\n  gibasa::pack()\n\nstr(dat_ngram)\n#&gt; tibble [899 × 2] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ text  : chr [1:899] \"ポラーノ の 広場\" \"宮沢 賢治\" \"前 十 七 等 官 レオーノ・キュースト 誌\" \"宮沢 賢治 訳述\" ...\n\ndat_ngram |&gt;\n  quanteda::corpus() |&gt;\n  quanteda::tokens(what = \"fastestword\") |&gt;\n  quanteda::tokens_ngrams(n = 2) |&gt;\n  quanteda::dfm()\n#&gt; Document-feature matrix of: 899 documents, 10,890 features (99.75% sparse) and 0 docvars.\n#&gt;     features\n#&gt; docs ポラーノ_の の_広場 宮沢_賢治 前_十 十_七 七_等 等_官\n#&gt;    1           1       1         0     0     0     0     0\n#&gt;    2           0       0         1     0     0     0     0\n#&gt;    3           0       0         0     1     1     1     1\n#&gt;    4           0       0         1     0     0     0     0\n#&gt;    5           0       0         0     0     0     0     0\n#&gt;    6           0       0         0     0     0     0     1\n#&gt;     features\n#&gt; docs 官_レオーノ・キュースト レオーノ・キュースト_誌 賢治_訳述\n#&gt;    1                       0                       0         0\n#&gt;    2                       0                       0         0\n#&gt;    3                       1                       1         0\n#&gt;    4                       0                       0         1\n#&gt;    5                       0                       0         0\n#&gt;    6                       0                       0         0\n#&gt; [ reached max_ndoc ... 893 more documents, reached max_nfeat ... 10,880 more features ]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>N-gram</span>"
    ]
  },
  {
    "objectID": "weighting.html",
    "href": "weighting.html",
    "title": "4  単語頻度の重みづけ",
    "section": "",
    "text": "4.1 tidytextによる重みづけ\ntidytext::bind_tf_idfを使うと単語頻度からTF-IDFを算出することができます。\ndat_count |&gt;\n  tidytext::bind_tf_idf(token, doc_id, n) |&gt;\n  dplyr::slice_max(tf_idf, n = 5L)\n#&gt; # A tibble: 5 × 6\n#&gt;   doc_id token        n    tf   idf tf_idf\n#&gt;   &lt;fct&gt;  &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 139    去年         1     1  6.78   6.78\n#&gt; 2 880    ざあい       1     1  6.78   6.78\n#&gt; 3 255    あわてる     1     1  6.08   6.08\n#&gt; 4 713    こちら       1     1  6.08   6.08\n#&gt; 5 288    こいつ       1     1  5.39   5.39\ntidytextにおけるTFとIDFは、RMeCabにおけるそれとは採用している計算式が異なるため、計算結果が異なります。TFはRMeCabでは生の索引語頻度（tfの場合）ですが、tidytextでは文書内での相対頻度になります。また、IDFはRMeCabでは対数の底が2であるのに対して、tidytextでは底がexp(1)であるなどの違いがあります。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>単語頻度の重みづけ</span>"
    ]
  },
  {
    "objectID": "weighting.html#gibasaによる重みづけ",
    "href": "weighting.html#gibasaによる重みづけ",
    "title": "4  単語頻度の重みづけ",
    "section": "4.2 gibasaによる重みづけ",
    "text": "4.2 gibasaによる重みづけ\ngibasaはRMeCabにおける単語頻度の重みづけをtidytext::bind_tf_idfと同様のスタイルでおこなうことができる関数gibasa::bind_tf_idf2を提供しています。\nRMeCabは以下の単語頻度の重みづけをサポートしています。\n\n局所的重み（TF）\n\ntf（索引語頻度）\ntf2（対数化索引語頻度）\ntf3（２進重み）\n\n大域的重み（IDF）\n\nidf（文書頻度の逆数）\nidf2（大域的IDF）\nidf3（確率的IDF）\nidf4（エントロピー）\n\n正規化\n\nnorm（コサイン正規化）\n\n\ngibasaはこれらの重みづけを再実装しています。ただし、tf=\"tf\"はgibasaでは相対頻度になるため、RMeCabのweight=\"tf*idf\"に相当する出力を得るには、たとえば次のように計算します。\n\ndat_count |&gt;\n  gibasa::bind_tf_idf2(token, doc_id, n) |&gt;\n  dplyr::mutate(\n    tf_idf = n * idf\n  ) |&gt;\n  dplyr::slice_max(tf_idf, n = 5L)\n#&gt; # A tibble: 6 × 6\n#&gt;   doc_id token     n     tf   idf tf_idf\n#&gt;   &lt;fct&gt;  &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 825    勉強      6 0.0638  8.77   52.6\n#&gt; 2 826    力        6 0.0667  8.77   52.6\n#&gt; 3 822    勉強      5 0.0549  8.77   43.9\n#&gt; 4 715    の       11 0.103   3.74   41.2\n#&gt; 5 113    鞭        4 0.211   9.77   39.1\n#&gt; 6 826    得る      4 0.0444  9.77   39.1\n\nなお、注意点として、RMeCabの単語を数える機能は、品詞情報（POS1, POS2）を確認しながら単語を数えているようなので、ここでのように原形だけを見て数えた結果とは必ずしも一致しません。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>単語頻度の重みづけ</span>"
    ]
  },
  {
    "objectID": "weighting.html#udpipeによる重みづけ",
    "href": "weighting.html#udpipeによる重みづけ",
    "title": "4  単語頻度の重みづけ",
    "section": "4.3 udpipeによる重みづけ",
    "text": "4.3 udpipeによる重みづけ\nudpipeを使っても単語頻度とTF-IDFを算出できます。また、udpipe::document_term_frequencies_statisticsでは、TF、IDFとTF-IDFにくわえて、Okapi BM25を計算することができます。\nudpipe::document_term_frequencies_statisticsには、パラメータとしてkとbを渡すことができます。デフォルト値はそれぞれk=1.2、b=0.5です。kの値を大きくすると、単語の出現数の増加に対してBM25の値もより大きくなりやすくなります。 k=1.2というのは、Elasticsearchでもデフォルト値として採用されている値です。WikipediaやElasticsearchの技術記事によると、kは[1.2, 2.0]、b=.75とした場合に、多くのケースでよい結果が得られるとされています。\ndplyrを使っていればあまり意識する必要はないと思いますが、udpipeのこのあたりの関数の戻り値はdata.tableである点に注意してください。\n\nsuppressPackageStartupMessages(require(dplyr))\n\ndat |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |&gt;\n  dplyr::filter(POS1 %in% c(\"名詞\", \"動詞\", \"形容詞\")) |&gt;\n  dplyr::mutate(\n    doc_id = forcats::fct_drop(doc_id),\n    token = dplyr::if_else(is.na(Original), token, Original)\n  ) |&gt;\n  udpipe::document_term_frequencies(document = \"doc_id\", term = \"token\") |&gt;\n  udpipe::document_term_frequencies_statistics(b = .75) |&gt;\n  dplyr::slice_max(tf_bm25, n = 5L)\n#&gt;     doc_id   term  freq        tf      idf    tf_idf  tf_bm25     bm25\n#&gt;     &lt;fctr&gt; &lt;char&gt; &lt;int&gt;     &lt;num&gt;    &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n#&gt;  1:    381   呑む     2 0.5000000 3.639872 1.8199359 1.699130 6.184616\n#&gt;  2:    398   決闘     2 0.5000000 4.829456 2.4147280 1.699130 8.205875\n#&gt;  3:    864   呑む     2 0.5000000 3.639872 1.8199359 1.699130 6.184616\n#&gt;  4:    859   呑む     3 0.3333333 3.639872 1.2132906 1.670247 6.079487\n#&gt;  5:     60     ん     2 0.4000000 1.900169 0.7600675 1.652923 3.140834\n#&gt;  6:     62     ん     2 0.4000000 1.900169 0.7600675 1.652923 3.140834\n#&gt;  7:    233     ん     2 0.4000000 1.900169 0.7600675 1.652923 3.140834\n#&gt;  8:    260   飛ぶ     2 0.4000000 5.165928 2.0663713 1.652923 8.538884\n#&gt;  9:    428   する     2 0.4000000 1.533619 0.6134476 1.652923 2.534955\n#&gt; 10:    521     君     2 0.4000000 4.578142 1.8312566 1.652923 7.567318\n#&gt; 11:    742     ん     2 0.4000000 1.900169 0.7600675 1.652923 3.140834",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>単語頻度の重みづけ</span>"
    ]
  },
  {
    "objectID": "weighting.html#tidyloによる重みづけ",
    "href": "weighting.html#tidyloによる重みづけ",
    "title": "4  単語頻度の重みづけ",
    "section": "4.4 tidyloによる重みづけ",
    "text": "4.4 tidyloによる重みづけ\nTF-IDFによる単語頻度の重みづけのモチベーションは、索引語のなかでも特定の文書だけに多く出現していて、ほかの文書ではそれほど出現しないような「注目に値する語」を調べることにあります。\nこうしたことを実現するための値として、tidyloパッケージでは「重み付きログオッズ（weighted log odds）」を計算することができます。\n\ndat_count |&gt;\n  tidylo::bind_log_odds(set = doc_id, feature = token, n = n) |&gt;\n  dplyr::filter(is.finite(log_odds_weighted)) |&gt;\n  dplyr::slice_max(log_odds_weighted, n = 5L)\n#&gt; # A tibble: 5 × 4\n#&gt;   doc_id token        n log_odds_weighted\n#&gt;   &lt;fct&gt;  &lt;chr&gt;    &lt;int&gt;             &lt;dbl&gt;\n#&gt; 1 536    する         1             117. \n#&gt; 2 430    する         1             105. \n#&gt; 3 824    わたくし     1             102. \n#&gt; 4 577    いる         1              94.5\n#&gt; 5 465    する         1              94.0\n\nここで用いているデータは小説を改行ごとに一つの文書と見なしていたため、中には次のような極端に短い文書が含まれています。こうした文書では、直観的にはそれほどレアには思われない単語についてもオッズが極端に高くなってしまっているように見えます。\n\ndat_txt |&gt;\n  dplyr::filter(doc_id %in% c(430, 536, 577, 824)) |&gt;\n  dplyr::pull(text)\n#&gt; [1] \"「承知しました。」\"                              \n#&gt; [2] \"「起訴するぞ。」\"                                \n#&gt; [3] \"「きっと遠くでございますわ。もし生きていれば。」\"\n#&gt; [4] \"わたくしは思わずはねあがりました。\"\n\nweighted log oddsについてはこの資料などを参照してください。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>単語頻度の重みづけ</span>"
    ]
  },
  {
    "objectID": "collocation.html",
    "href": "collocation.html",
    "title": "5  コロケーション",
    "section": "",
    "text": "5.1 文書内での共起\n共起関係を数える機能はgibasaには実装されていません。文書内での共起を簡単に数えるには、たとえば次のようにします。\ndat_fcm &lt;- dat_count |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::fcm()\n\ndat_fcm\n#&gt; Feature co-occurrence matrix of: 2,172 by 2,172 features.\n#&gt;                       features\n#&gt; features               ポラーノ 広場 宮沢 賢治 レオーノ・キュースト 七 十 官 等\n#&gt;   ポラーノ                    5   52    0    0                    0  0  2  0  0\n#&gt;   広場                        0    5    0    0                    0  0  2  0  0\n#&gt;   宮沢                        0    0    0    2                    0  0  0  0  0\n#&gt;   賢治                        0    0    0    0                    0  0  0  0  0\n#&gt;   レオーノ・キュースト        0    0    0    0                    0  1  3  3  3\n#&gt;   七                          0    0    0    0                    0  0  9  1  1\n#&gt;   十                          0    0    0    0                    0  0  8  6  6\n#&gt;   官                          0    0    0    0                    0  0  0  0  5\n#&gt;   等                          0    0    0    0                    0  0  0  0  0\n#&gt;   誌                          0    0    0    0                    0  0  0  0  0\n#&gt;                       features\n#&gt; features               誌\n#&gt;   ポラーノ              0\n#&gt;   広場                  0\n#&gt;   宮沢                  0\n#&gt;   賢治                  0\n#&gt;   レオーノ・キュースト  1\n#&gt;   七                    1\n#&gt;   十                    1\n#&gt;   官                    1\n#&gt;   等                    1\n#&gt;   誌                    0\n#&gt; [ reached max_nfeat ... 2,162 more features, reached max_nfeat ... 2,162 more features ]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>コロケーション</span>"
    ]
  },
  {
    "objectID": "collocation.html#任意のウィンドウ内での共起",
    "href": "collocation.html#任意のウィンドウ内での共起",
    "title": "5  コロケーション",
    "section": "5.2 任意のウィンドウ内での共起",
    "text": "5.2 任意のウィンドウ内での共起\n\n5.2.1 共起の集計\nRMeCab::collocateのような任意のウィンドウの中での共起を集計するには、次のようにする必要があります。ここではwindowは前後5個のトークンを見るようにします。\n\ndat_corpus &lt;- dat |&gt;\n  gibasa::pack()\n\ndat_fcm &lt;- dat_corpus |&gt;\n  quanteda::corpus() |&gt;\n  quanteda::tokens(what = \"fastestword\") |&gt;\n  quanteda::fcm(context = \"window\", window = 5)\n\nこうすると、nodeについて共起しているtermとその頻度を確認できます。以下では、「わたくし」というnodeと共起しているtermで頻度が上位20までであるものを表示しています。\n\ndat_fcm &lt;- dat_fcm |&gt;\n  tidytext::tidy() |&gt;\n  dplyr::rename(node = document, term = term) |&gt;\n  dplyr::filter(node == \"わたくし\") |&gt;\n  dplyr::slice_max(count, n = 20)\n\ndat_fcm\n#&gt; # A tibble: 20 × 3\n#&gt;    node     term  count\n#&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;\n#&gt;  1 わたくし は      205\n#&gt;  2 わたくし 。      122\n#&gt;  3 わたくし た      110\n#&gt;  4 わたくし て       99\n#&gt;  5 わたくし 、       91\n#&gt;  6 わたくし まし     90\n#&gt;  7 わたくし を       62\n#&gt;  8 わたくし に       61\n#&gt;  9 わたくし が       51\n#&gt; 10 わたくし し       37\n#&gt; 11 わたくし も       35\n#&gt; 12 わたくし で       33\n#&gt; 13 わたくし ども     28\n#&gt; 14 わたくし と       23\n#&gt; 15 わたくし 」       23\n#&gt; 16 わたくし です     22\n#&gt; 17 わたくし へ       17\n#&gt; 18 わたくし い       15\n#&gt; 19 わたくし 「       15\n#&gt; 20 わたくし から     14\n\n\n\n5.2.2 T値やMI値の算出\nT値やMI値は、たとえば次のようにして計算できます。\nT値については「1.65」を越える場合、その共起が偶然ではないと考える大まかな目安となるそうです。また、MI値については「1.58」を越える場合に共起関係の大まかな目安となります（いずれの値についても「2」などを目安とする場合もあります）。\n\nntok &lt;- dat_corpus |&gt;\n  quanteda::corpus() |&gt;\n  quanteda::tokens(what = \"fastestword\") |&gt;\n  quanteda::ntoken() |&gt;\n  sum()\n\ntotal &lt;- dat_corpus |&gt;\n  quanteda::corpus() |&gt;\n  quanteda::tokens(what = \"fastestword\") |&gt;\n  quanteda::tokens_select(c(\"わたくし\", dat_fcm$term)) |&gt;\n  quanteda::dfm() |&gt;\n  quanteda::colSums()\n\ndat_fcm |&gt;\n  dplyr::select(-node) |&gt;\n  dplyr::mutate(\n    expect = total[term] / ntok * total[\"わたくし\"] * 5 * 2, ## 5はwindowのサイズ\n    t = (count - expect) / sqrt(count),\n    mi = log2(count / expect)\n  )\n#&gt; # A tibble: 20 × 5\n#&gt;    term  count expect       t      mi\n#&gt;    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 は      205  88.1    8.17   1.22  \n#&gt;  2 。      122 161.    -3.53  -0.400 \n#&gt;  3 た      110 108.     0.190  0.0264\n#&gt;  4 て       99 106.    -0.694 -0.0972\n#&gt;  5 、       91 102.    -1.13  -0.162 \n#&gt;  6 まし     90  64.1    2.73   0.489 \n#&gt;  7 を       62  67.4   -0.689 -0.121 \n#&gt;  8 に       61  68.8   -1.00  -0.174 \n#&gt;  9 が       51  51.9   -0.126 -0.0252\n#&gt; 10 し       37  24.1    2.11   0.616 \n#&gt; 11 も       35  31.4    0.615  0.158 \n#&gt; 12 で       33  34.7   -0.290 -0.0710\n#&gt; 13 ども     28   3.11   4.70   3.17  \n#&gt; 14 と       23  28.7   -1.18  -0.317 \n#&gt; 15 」       23  54.8   -6.63  -1.25  \n#&gt; 16 です     22  15.4    1.40   0.512 \n#&gt; 17 へ       17  16.5    0.114  0.0403\n#&gt; 18 い       15  17.4   -0.628 -0.217 \n#&gt; 19 「       15  56.2  -10.6   -1.91  \n#&gt; 20 から     14  20.4   -1.72  -0.546\n\n注意点として、quantedaは全角スペースなどをトークンとして数えないようなので、ここでの総語数（ntok）は、RMeCabの計算で使われる総語数よりも少なくなることがあります。RMeCabでの計算結果と概ね一致させたい場合は、総語数としてgibasa::tokenizeの戻り値の行数を使ってください。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>コロケーション</span>"
    ]
  },
  {
    "objectID": "misc.html",
    "href": "misc.html",
    "title": "6  gibasaのセットアップ",
    "section": "",
    "text": "6.1 Posit Cloud（旧・RStudio Cloud）でのgibasaの使い方\nRによるデータ分析を手軽に試したい場合には、Posit Cloud（旧・RStudio Cloud）のようなクラウド環境が便利かもしれません。\n一方で、Posit Cloudはユーザー権限しかない環境のため、gibasaを使えるようにするまでにはややコツが要ります。とはいえ、gibasaはRMeCabとは異なり、MeCabのバイナリはなくても使える（辞書とmecabrcがあればよい）ので、RMeCabを使う場合ほど複雑なことをする必要はないはずです。\nここでは、Posit Cloudでgibasaを利用できるようにするための手順を簡単に説明します（RMeCabもあわせて試したいという場合には、MeCabのバイナリを自分でビルドする必要があります。その場合はこの記事などを参考にしてください）。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>gibasaのセットアップ</span>"
    ]
  },
  {
    "objectID": "misc.html#posit-cloud旧rstudio-cloudでのgibasaの使い方",
    "href": "misc.html#posit-cloud旧rstudio-cloudでのgibasaの使い方",
    "title": "6  gibasaのセットアップ",
    "section": "",
    "text": "6.1.1 辞書（ipadic, unidic-lite）の配置\nMeCabの辞書は、Terminalタブからpipでインストールできます。ここでは、IPA辞書（ipadic）をインストールします。\npython3 -m pip install ipadic\npython3 -c \"import ipadic; print('dicdir=' + ipadic.DICDIR);\" &gt; ~/.mecabrc\n\n\n6.1.2 gibasaのインストール\ngibasaをインストールします。\ninstall.packages(\"gibasa\")\n\n\n6.1.3 試すには\nうまくいっていると、辞書を指定しない場合はIPA辞書が使われます。\ngibasa::dictionary_info()\ngibasa::tokenize(\"こんにちは\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>gibasaのセットアップ</span>"
    ]
  },
  {
    "objectID": "misc.html#mecabの辞書をビルドするには",
    "href": "misc.html#mecabの辞書をビルドするには",
    "title": "6  gibasaのセットアップ",
    "section": "6.2 MeCabの辞書をビルドするには",
    "text": "6.2 MeCabの辞書をビルドするには\nv1.0.1から、gibasaを使ってMeCabのシステム辞書やユーザー辞書をビルドできるようになりました。以下では、gibasaを使ってMeCabの辞書をビルドする方法を紹介します。\nMeCabの辞書は、各行が次のようなデータからなる「ヘッダーなしCSVファイル」を用意して、それらをもとに生成します。 ...の部分は見出し語の品詞情報で、ビルドしたい辞書によって異なります。IPA辞書の場合、品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音をこの通りの順番で記述します。\n表層形,左文脈ID,右文脈ID,コスト,...\n左文脈IDと右文脈IDは、品詞情報が正確に書かれている場合、空にしておくと自動で補完されます。 しかし、品詞情報を正確に書くには、おそらく当の左文脈IDと右文脈IDを含む出力を確認する必要があるため、ふつうに確認した値で埋めてしまったほうが確実です。\nここでは例として、「月ノ美兎」（ANYCOLOR社が運営する「にじさんじ」所属のVTuberの名前）という語彙を含む文をIPA辞書を使いつつ狙いどおりに解析してみましょう。\n\n6.2.1 必要な品詞情報を確認する\ngibasa::posDebugRcppは、与えられた文字列について、MeCabの-aオプションに相当する解析結果（解析結果になりえるすべての形態素の組み合わせ）を出力する関数です。 ここでの最適解（is_best == \"01\"）である結果について確認すると、「月ノ美兎」という語彙は次のように複数の形態素に分割されてしまっていることがわかります。\n\ngibasa::posDebugRcpp(\"月ノ美兎は箱の中\") |&gt;\n  dplyr::filter(is_best == \"01\")\n#&gt;    doc_id pos_id surface                                 feature stat lcAttr\n#&gt; 1       1      0                         BOS/EOS,*,*,*,*,*,*,*,*   02      0\n#&gt; 2       1     38      月          名詞,一般,*,*,*,*,月,ツキ,ツキ   00   1285\n#&gt; 3       1      4      ノ              記号,一般,*,*,*,*,ノ,ノ,ノ   00      5\n#&gt; 4       1     44      美  名詞,固有名詞,人名,名,*,*,美,ヨシ,ヨシ   00   1291\n#&gt; 5       1     38      兎      名詞,一般,*,*,*,*,兎,ウサギ,ウサギ   00   1285\n#&gt; 6       1     16      は            助詞,係助詞,*,*,*,*,は,ハ,ワ   00    261\n#&gt; 7       1     38      箱          名詞,一般,*,*,*,*,箱,ハコ,ハコ   00   1285\n#&gt; 8       1     24      の            助詞,連体化,*,*,*,*,の,ノ,ノ   00    368\n#&gt; 9       1     66      中 名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ   00   1313\n#&gt; 10      1      0                         BOS/EOS,*,*,*,*,*,*,*,*   03      0\n#&gt;    rcAttr     alpha      beta is_best prob wcost  cost\n#&gt; 1       0      0.00 -22144.50      01    0     0     0\n#&gt; 2    1285  -6190.50 -15954.00      01    1  8537  8254\n#&gt; 3       5  -8874.75 -13269.75      01    1  4929 11833\n#&gt; 4    1291 -13974.00  -8170.50      01    1  7885 18632\n#&gt; 5    1285 -18080.25  -4064.25      01    1  5290 24107\n#&gt; 6     261 -18095.25  -4049.25      01    1  3865 24127\n#&gt; 7    1285 -22729.50    585.00      01    1  6142 30306\n#&gt; 8     368 -23010.00    865.50      01    1  4816 30680\n#&gt; 9    1313 -24007.50   1863.00      01    1  6528 32010\n#&gt; 10      0 -22144.50      0.00      01    1     0 29526\n\nこのような語については、こちらのビネットで説明しているように、制約付き解析を使って次のように強制的に抽出することもできます。\n\ngibasa::posDebugRcpp(\"月ノ\\t*\\n美兎\\t*\\nは箱の中\", partial = TRUE)\n#&gt;   doc_id pos_id surface                                 feature stat lcAttr\n#&gt; 1      1      0                         BOS/EOS,*,*,*,*,*,*,*,*   02      0\n#&gt; 2      1     38    月ノ                     名詞,一般,*,*,*,*,*   01   1285\n#&gt; 3      1     38    美兎                     名詞,一般,*,*,*,*,*   01   1285\n#&gt; 4      1     16      は            助詞,係助詞,*,*,*,*,は,ハ,ワ   00    261\n#&gt; 5      1     38      箱          名詞,一般,*,*,*,*,箱,ハコ,ハコ   00   1285\n#&gt; 6      1     24      の            助詞,連体化,*,*,*,*,の,ノ,ノ   00    368\n#&gt; 7      1     66      中 名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ   00   1313\n#&gt; 8      1      0                         BOS/EOS,*,*,*,*,*,*,*,*   03      0\n#&gt;   rcAttr     alpha      beta is_best prob wcost  cost\n#&gt; 1      0      0.00 -19563.75      01    0     0     0\n#&gt; 2   1285  -6883.50 -12680.25      01    1  9461  9178\n#&gt; 3   1285 -15499.50  -4064.25      01    1 11426 20666\n#&gt; 4    261 -15514.50  -4049.25      01    1  3865 20686\n#&gt; 5   1285 -20148.75    585.00      01    1  6142 26865\n#&gt; 6    368 -20429.25    865.50      01    1  4816 27239\n#&gt; 7   1313 -21426.75   1863.00      01    1  6528 28569\n#&gt; 8      0 -19563.75      0.00      01    1     0 26085\n\n一方で、IPA辞書には、たとえば「早見」のような名詞,固有名詞,人名,姓,...という品詞と、「沙織」のような名詞,固有名詞,人名,名,...という品詞があります。 このような解析結果としてより望ましい品詞を確認するには、正しく解析させたい語（ここでは「月ノ美兎」）と同じような使われ方をする語（たとえば「早見沙織」）を実際に解析してみて、その結果を確認するとよいでしょう。\n\ngibasa::posDebugRcpp(\"早見沙織のラジオ番組\") |&gt;\n  dplyr::filter(is_best == \"01\")\n#&gt;   doc_id pos_id surface                                      feature stat\n#&gt; 1      1      0                              BOS/EOS,*,*,*,*,*,*,*,*   02\n#&gt; 2      1     43    早見 名詞,固有名詞,人名,姓,*,*,早見,ハヤミ,ハヤミ   00\n#&gt; 3      1     44    沙織 名詞,固有名詞,人名,名,*,*,沙織,サオリ,サオリ   00\n#&gt; 4      1     24      の                 助詞,連体化,*,*,*,*,の,ノ,ノ   00\n#&gt; 5      1     38  ラジオ       名詞,一般,*,*,*,*,ラジオ,ラジオ,ラジオ   00\n#&gt; 6      1     38    番組     名詞,一般,*,*,*,*,番組,バングミ,バングミ   00\n#&gt; 7      1      0                              BOS/EOS,*,*,*,*,*,*,*,*   03\n#&gt;   lcAttr rcAttr     alpha      beta is_best prob wcost  cost\n#&gt; 1      0      0      0.00 -10104.75      01    0     0     0\n#&gt; 2   1290   1290  -4362.75  -5742.00      01    1  7472  5817\n#&gt; 3   1291   1291  -5452.50  -4652.25      01    1  8462  7270\n#&gt; 4    368    368  -6658.50  -3446.25      01    1  4816  8878\n#&gt; 5   1285   1285  -7886.25  -2218.50      01    1  3942 10515\n#&gt; 6   1285   1285 -10534.50    429.75      01    1  3469 14046\n#&gt; 7      0      0 -10104.75      0.00      01    1     0 13473\n\nこの結果は狙いどおりのものであるため、「月ノ美兎」を正しく解析するために用意するCSVファイルは、仮に次のように作成しておくことができそうです。\n\nwriteLines(\n  c(\n    \"月ノ,1290,1290,7472,名詞,固有名詞,人名,姓,*,*,月ノ,ツキノ,ツキノ\",\n    \"美兎,1291,1291,8462,名詞,固有名詞,人名,名,*,*,美兎,ミト,ミト\"\n  ),\n  con = (csv_file &lt;- tempfile(fileext = \".csv\"))\n)\n\n\n\n6.2.2 ユーザー辞書のビルド\n試しに、ユーザー辞書をビルドしてみましょう。gibasaでユーザー辞書をビルドするには、gibasa::build_user_dicを使います。 ユーザー辞書をビルドするにはシステム辞書が必要なため、あらかじめシステム辞書（ここではIPA辞書）が適切に配置されていることを確認しておいてください。\n次のようにしてユーザー辞書をビルドできます。\n\ngibasa::build_user_dic(\n  dic_dir = stringr::str_sub(gibasa::dictionary_info()$file_path, end = -8),\n  file = (user_dic &lt;- tempfile(fileext = \".dic\")),\n  csv_file = csv_file,\n  encoding = \"utf8\"\n)\n#&gt; reading /tmp/RtmplNJUWc/file92eb7aae95ff.csv ... 2\n#&gt; \n#&gt; done!\n\nなお、gibasaによる辞書のビルド時の注意点として、「MeCab: 単語の追加方法」で案内されている「コストの自動推定機能」はgibasaからは利用できません。 追加したい見出し語の生起コストは空にせず、必ず適当な値で埋めるようにしてください。\nさて、ビルドしたユーザー辞書を使ってみましょう。\n\ngibasa::dictionary_info(user_dic = user_dic)\n#&gt;                              file_path charset lsize rsize   size type version\n#&gt; 1    /var/lib/mecab/dic/debian/sys.dic   UTF-8  1316  1316 392127    0     102\n#&gt; 2 /tmp/RtmplNJUWc/file92eb15c13bae.dic    utf8  1316  1316      2    1     102\ngibasa::tokenize(\"月ノ美兎は箱の中\", user_dic = user_dic)\n#&gt; # A tibble: 6 × 5\n#&gt;   doc_id sentence_id token_id token feature                                     \n#&gt;   &lt;fct&gt;        &lt;int&gt;    &lt;int&gt; &lt;chr&gt; &lt;chr&gt;                                       \n#&gt; 1 1                1        1 月ノ  名詞,固有名詞,人名,姓,*,*,月ノ,ツキノ,ツキノ\n#&gt; 2 1                1        2 美兎  名詞,固有名詞,人名,名,*,*,美兎,ミト,ミト    \n#&gt; 3 1                1        3 は    助詞,係助詞,*,*,*,*,は,ハ,ワ                \n#&gt; 4 1                1        4 箱    名詞,一般,*,*,*,*,箱,ハコ,ハコ              \n#&gt; 5 1                1        5 の    助詞,連体化,*,*,*,*,の,ノ,ノ                \n#&gt; 6 1                1        6 中    名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ\n\n狙いどおりに解析できているようです。\n\n\n6.2.3 生起コストを調整する\nここまでに紹介したようなやり方で辞書を整備することで、おおむね狙いどおりの解析結果を得られるようになると思われますが、 追加した語のかたちによっては、生起コストをより小さな値に調整しないと、一部の文において正しく切り出されない場合があるかもしれません。\nたとえば、こちらの記事で紹介されているように、 仮に高等学校,1285,1285,5078,名詞,一般,*,*,*,*,高等学校,コウトウガッコウ,コートーガッコーという見出し語を追加したとしても、 与える文によっては高等学校が狙いどおりに切り出されません。\n\nwriteLines(\n  c(\n    \"高等学校,1285,1285,5078,名詞,一般,*,*,*,*,高等学校,コウトウガッコウ,コートーガッコー\"\n  ),\n  con = (csv_file &lt;- tempfile(fileext = \".csv\"))\n)\ngibasa::build_user_dic(\n  dic_dir = stringr::str_sub(gibasa::dictionary_info()$file_path, end = -8),\n  file = (user_dic &lt;- tempfile(fileext = \".dic\")),\n  csv_file = csv_file,\n  encoding = \"utf8\"\n)\n#&gt; reading /tmp/RtmplNJUWc/file92eb44df27bc.csv ... 1\n#&gt; \n#&gt; done!\ngibasa::tokenize(\n  c(\n    \"九州高等学校ゴルフ選手権\",\n    \"地元の高等学校に進学した\",\n    \"帝京高等学校のエースとして活躍\",\n    \"開成高等学校117人が現役合格\",\n    \"マンガを高等学校の授業で使う\"\n  ),\n  user_dic = user_dic\n) |&gt;\n  gibasa::pack()\n#&gt; # A tibble: 5 × 2\n#&gt;   doc_id text                                \n#&gt;   &lt;fct&gt;  &lt;chr&gt;                               \n#&gt; 1 1      九州 高等 学校 ゴルフ 選手権        \n#&gt; 2 2      地元 の 高等 学校 に 進学 し た     \n#&gt; 3 3      帝京 高等 学校 の エース として 活躍\n#&gt; 4 4      開成 高等 学校 117 人 が 現役 合格  \n#&gt; 5 5      マンガ を 高等 学校 の 授業 で 使う\n\nこの例のように、複数の既存の見出し語のほうが優先されてしまう場合には、追加する見出し語の生起コストを小さくすることによって、 狙いどおりの解析結果を得ることができます。\n先ほどの記事のなかで紹介されているのと同じやり方で、適切な生起コストをgibasaを使って求めるには、たとえば次のような関数を用意します（やや複雑なのでバグがあるかもしれません）。\n\ncalc_adjusted_cost &lt;- \\(sentences, target_word, sys_dic = \"\", user_dic = \"\") {\n  sentences_mod &lt;-\n    stringi::stri_replace_all_regex(\n      sentences,\n      pattern = paste0(\"(?&lt;target&gt;(\", target_word, \"))\"),\n      replacement = \"\\n${target}\\t*\\n\",\n      vectorize_all = FALSE\n    )\n  calc_cumcost &lt;- \\(x) {\n    ret &lt;-\n      gibasa::posDebugRcpp(x, sys_dic = sys_dic, user_dic = user_dic, partial = TRUE) |&gt;\n      dplyr::mutate(\n        lcAttr = dplyr::lead(lcAttr, default = 0),\n        cost = purrr::map2_dbl(\n          rcAttr, lcAttr,\n          ~ gibasa::get_transition_cost(.x, .y, sys_dic = sys_dic, user_dic = user_dic)\n        ),\n        wcost = cumsum(wcost),\n        cost = cumsum(cost),\n        ## 1行目のBOS/EOS-&gt;BOS/EOS間の連接コストを足しすぎてしまうので、引く\n        total_cost = wcost + cost - gibasa::get_transition_cost(0, 0, sys_dic = sys_dic, user_dic = user_dic),\n        .by = doc_id\n      ) |&gt;\n      dplyr::slice_tail(n = 1, by = doc_id) |&gt;\n      dplyr::pull(\"total_cost\")\n    ret\n  }\n  cost1 &lt;- calc_cumcost(sentences)\n  cost2 &lt;- calc_cumcost(sentences_mod)\n\n  gibasa::posDebugRcpp(sentences_mod, sys_dic = sys_dic, user_dic = user_dic, partial = TRUE) |&gt;\n    dplyr::filter(surface %in% target_word) |&gt;\n    dplyr::reframe(\n      stat = stat,\n      surface = surface,\n      pos_id = pos_id,\n      feature = feature,\n      lcAttr = lcAttr,\n      rcAttr = rcAttr,\n      current_cost = wcost,\n      adjusted_cost = wcost + (cost1[doc_id] - cost2[doc_id] - 1)\n    ) |&gt;\n    dplyr::slice_min(adjusted_cost, n = 1, by = surface)\n}\n\nこの関数を使って、実際に適切な生起コストを計算してみます。\n\nadjusted_cost &lt;-\n  calc_adjusted_cost(\n    c(\n      \"九州高等学校ゴルフ選手権\",\n      \"地元の高等学校に進学した\",\n      \"帝京高等学校のエースとして活躍\",\n      \"開成高等学校117人が現役合格\",\n      \"マンガを高等学校の授業で使う\"\n    ),\n    target_word = \"高等学校\",\n    user_dic = user_dic\n  )\n\nadjusted_cost\n#&gt;   stat  surface pos_id\n#&gt; 1   00 高等学校      1\n#&gt;                                                        feature lcAttr rcAttr\n#&gt; 1 名詞,一般,*,*,*,*,高等学校,コウトウガッコウ,コートーガッコー   1285   1285\n#&gt;   current_cost adjusted_cost\n#&gt; 1         5078          2539\n\nこの生起コストを使って改めてユーザー辞書をビルドし、結果を確認してみましょう。\n\nadjusted_cost |&gt;\n  tidyr::unite(\n    csv_body,\n    surface, lcAttr, rcAttr, adjusted_cost, feature,\n    sep = \",\"\n  ) |&gt;\n  dplyr::pull(\"csv_body\") |&gt;\n  writeLines(con = (csv_file &lt;- tempfile(fileext = \".csv\")))\n\ngibasa::build_user_dic(\n  dic_dir = stringr::str_sub(gibasa::dictionary_info()$file_path, end = -8),\n  file = (user_dic &lt;- tempfile(fileext = \".dic\")),\n  csv_file = csv_file,\n  encoding = \"utf8\"\n)\n#&gt; reading /tmp/RtmplNJUWc/file92eb7334d99b.csv ... 1\n#&gt; \n#&gt; done!\n\ngibasa::tokenize(\n  c(\n    \"九州高等学校ゴルフ選手権\",\n    \"地元の高等学校に進学した\",\n    \"帝京高等学校のエースとして活躍\",\n    \"開成高等学校117人が現役合格\",\n    \"マンガを高等学校の授業で使う\"\n  ),\n  user_dic = user_dic\n) |&gt;\n  gibasa::pack()\n#&gt; # A tibble: 5 × 2\n#&gt;   doc_id text                               \n#&gt;   &lt;fct&gt;  &lt;chr&gt;                              \n#&gt; 1 1      九州 高等学校 ゴルフ 選手権        \n#&gt; 2 2      地元 の 高等学校 に 進学 し た     \n#&gt; 3 3      帝京 高等学校 の エース として 活躍\n#&gt; 4 4      開成 高等学校 117 人 が 現役 合格  \n#&gt; 5 5      マンガ を 高等学校 の 授業 で 使う\n\n今度はうまくいっていそうです。\n\n\n6.2.4 システム辞書のビルド\nユーザー辞書ではなく、システム辞書をビルドすることもできます。 ただ、ふつうに入手できるIPA辞書のソースの文字コードはEUC-JPであり、UTF-8のCSVファイルと混在させることができないため、扱いに注意が必要です。\nまた、UniDicの2.3.xについては、同梱されているファイルに問題があるようで、そのままではビルドできないという話があるようです（参考）。 UniDicはIPA辞書に比べるとビルドするのにそれなりにメモリが必要なことからも、とくに事情がないかぎりはビルド済みのバイナリ辞書をダウンロードしてきて使ったほうがよいでしょう。\n\nipadic_temp &lt;- tempfile(fileext = \".tar.gz\")\ndownload.file(\"https://github.com/shogo82148/mecab/releases/download/v0.996.11/mecab-ipadic-2.7.0-20070801.tar.gz\", destfile = ipadic_temp)\nuntar(ipadic_temp, exdir = tempdir())\n\ngibasa::build_sys_dic(\n  dic_dir = file.path(tempdir(), \"mecab-ipadic-2.7.0-20070801\"),\n  out_dir = tempdir(),\n  encoding = \"euc-jp\"\n)\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/unk.def ... 40\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Adverb.csv ... 3032\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Conjunction.csv ... 171\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Suffix.csv ... 1393\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Noun.adverbal.csv ... 795\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Noun.others.csv ... 151\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Noun.org.csv ... 16668\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Verb.csv ... 130750\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Noun.place.csv ... 72999\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Noun.csv ... 60477\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Adnominal.csv ... 135\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Noun.number.csv ... 42\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Noun.verbal.csv ... 12146\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Filler.csv ... 19\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Others.csv ... 2\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Noun.adjv.csv ... 3328\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Interjection.csv ... 252\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Postp-col.csv ... 91\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Noun.nai.csv ... 42\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Prefix.csv ... 221\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Noun.name.csv ... 34202\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Symbol.csv ... 208\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Adj.csv ... 27210\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Noun.demonst.csv ... 120\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Noun.proper.csv ... 27327\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Postp.csv ... 146\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/Auxil.csv ... 199\n#&gt; reading /tmp/RtmplNJUWc/mecab-ipadic-2.7.0-20070801/matrix.def ... 1316x1316\n#&gt; \n#&gt; done!\n\n# `dicrc`ファイルをビルドした辞書のあるディレクトリにコピーする\nfile.copy(file.path(tempdir(), \"mecab-ipadic-2.7.0-20070801/dicrc\"), tempdir())\n#&gt; [1] TRUE\n\n# ここでは`mecabrc`ファイルが適切な位置に配置されていないという想定で、\n# `mecabrc`ファイルを偽装している。\nwithr::with_envvar(\n  c(\n    \"MECABRC\" = nullfile()\n  ),\n  gibasa::tokenize(\"月ノ美兎は箱の中\", sys_dic = tempdir())\n)\n#&gt; # A tibble: 8 × 5\n#&gt;   doc_id sentence_id token_id token feature                                \n#&gt;   &lt;fct&gt;        &lt;int&gt;    &lt;int&gt; &lt;chr&gt; &lt;chr&gt;                                  \n#&gt; 1 1                1        1 月    名詞,一般,*,*,*,*,月,ツキ,ツキ         \n#&gt; 2 1                1        2 ノ    記号,一般,*,*,*,*,ノ,ノ,ノ             \n#&gt; 3 1                1        3 美    名詞,固有名詞,人名,名,*,*,美,ヨシ,ヨシ \n#&gt; 4 1                1        4 兎    名詞,一般,*,*,*,*,兎,ウサギ,ウサギ     \n#&gt; 5 1                1        5 は    助詞,係助詞,*,*,*,*,は,ハ,ワ           \n#&gt; 6 1                1        6 箱    名詞,一般,*,*,*,*,箱,ハコ,ハコ         \n#&gt; 7 1                1        7 の    助詞,連体化,*,*,*,*,の,ノ,ノ           \n#&gt; 8 1                1        8 中    名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>gibasaのセットアップ</span>"
    ]
  },
  {
    "objectID": "sessioninfo.html",
    "href": "sessioninfo.html",
    "title": "7  セッション情報",
    "section": "",
    "text": "Note更新情報\n\n\n\n\n\n\n2025-10-05\n\npaithiov909/textvis-recipes: Cookbook to draw KHCoder-like visualizations using Rの内容をこのサイトのAppendicesとして統合しました。\nこれにともなって、従来のAppendix Aの位置を見直しました。\n\n2025-04-22\n\nR-4.5を使ってビルドしなおしました\n\n2024-03-10\n\nサイトの設定を更新しました。内容に変更はありません\n\n2024-03-01\n\nbookdownを利用したサイトからQuarto Booksを利用したサイトに置き換えました。内容に変更はありません\n\n2024-02-28\n\n「Chapter 4 単語頻度の重みづけ」の「gibasaによる重みづけ」のコードに誤りがあったため、修正しました\n\n2024-02-17\n\n「Chapter 3 N-gram」にNgramを品詞でフィルタする場合の説明を追加しました\n「Chapter 4 単語頻度の重みづけ」の「コサイン正規化」についての説明を修正しました。gibasa v1.1.0からnorm=TRUE時の挙動をRMeCabと同じになるように変更したため、「（挙動の）細かな点が異なる」としていた説明を削除しました\n\n2023-12-13\n\n見た目を調整しました。内容に変更はありません\n\n2023-12-12\n\n「想定する知識など」に参考となる他の資料へのリンクを追加しました\nコードブロックの表示のされ方を調整しました\n\n2023-12-03\n\n「Chapter 4 単語頻度の重みづけ」の内容を更新しました\n「Chapter 7 Appendix」に「MeCabの辞書をビルドするには」という節を追加しました\n\n2023-08-02\n\n「tidytextによる重みづけ」についての記述に誤りがあったため、修正しました\n「Posit Cloud（旧・RStudio Cloud）でのgibasaの使い方」の内容を更新しました\n\n\n\n\n\n\n\n\n\n\n\nNoteセッション情報\n\n\n\n\n\n\nsessioninfo::session_info()\n#&gt; ─ Session info ───────────────────────────────────────────────────────────────\n#&gt;  setting  value\n#&gt;  version  R version 4.5.1 (2025-06-13)\n#&gt;  os       Ubuntu 24.04.3 LTS\n#&gt;  system   x86_64, linux-gnu\n#&gt;  ui       X11\n#&gt;  language (EN)\n#&gt;  collate  ja_JP.UTF-8\n#&gt;  ctype    ja_JP.UTF-8\n#&gt;  tz       Asia/Tokyo\n#&gt;  date     2025-10-05\n#&gt;  pandoc   3.8 @ /usr/bin/ (via rmarkdown)\n#&gt;  quarto   1.8.25 @ /opt/quarto/bin/quarto\n#&gt; \n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package      * version  date (UTC) lib source\n#&gt;  audubon      * 0.5.2    2024-04-27 [1] RSPM (R 4.5.0)\n#&gt;  cachem         1.1.0    2024-05-16 [1] RSPM\n#&gt;  cli            3.6.5    2025-04-23 [1] RSPM\n#&gt;  curl           7.0.0    2025-08-19 [1] RSPM\n#&gt;  data.table     1.17.8   2025-07-10 [1] RSPM (R 4.5.0)\n#&gt;  digest         0.6.37   2024-08-19 [1] RSPM\n#&gt;  dplyr        * 1.1.4    2023-11-17 [1] RSPM (R 4.5.0)\n#&gt;  dtplyr       * 1.3.2    2025-09-10 [1] RSPM (R 4.5.0)\n#&gt;  evaluate       1.0.5    2025-08-27 [1] RSPM\n#&gt;  fastmap        1.2.0    2024-05-15 [1] RSPM\n#&gt;  fastmatch      1.1-6    2024-12-23 [1] RSPM (R 4.5.0)\n#&gt;  generics       0.1.4    2025-05-09 [1] RSPM (R 4.5.0)\n#&gt;  gibasa       * 1.1.2    2025-02-16 [1] RSPM (R 4.5.0)\n#&gt;  glue           1.8.0    2024-09-30 [1] RSPM\n#&gt;  htmltools      0.5.8.1  2024-04-04 [1] RSPM\n#&gt;  htmlwidgets    1.6.4    2023-12-06 [1] RSPM\n#&gt;  janeaustenr    1.0.0    2022-08-26 [1] RSPM (R 4.5.0)\n#&gt;  jsonlite       2.0.0    2025-03-27 [1] RSPM\n#&gt;  knitr          1.50     2025-03-16 [1] RSPM\n#&gt;  lattice        0.22-7   2025-04-02 [2] CRAN (R 4.5.1)\n#&gt;  lifecycle      1.0.4    2023-11-07 [1] RSPM\n#&gt;  magrittr       2.0.4    2025-09-12 [1] RSPM\n#&gt;  Matrix         1.7-3    2025-03-11 [2] CRAN (R 4.5.1)\n#&gt;  memoise        2.0.1    2021-11-26 [1] RSPM\n#&gt;  pillar         1.11.1   2025-09-17 [1] RSPM\n#&gt;  pkgconfig      2.0.3    2019-09-22 [1] RSPM\n#&gt;  purrr          1.1.0    2025-07-10 [1] RSPM\n#&gt;  quanteda     * 4.3.1    2025-07-10 [1] RSPM (R 4.5.0)\n#&gt;  R.cache        0.17.0   2025-05-02 [1] RSPM\n#&gt;  R.methodsS3    1.8.2    2022-06-13 [1] RSPM\n#&gt;  R.oo           1.27.1   2025-05-02 [1] RSPM\n#&gt;  R.utils        2.13.0   2025-02-24 [1] RSPM\n#&gt;  R6             2.6.1    2025-02-15 [1] RSPM\n#&gt;  Rcpp           1.1.0    2025-07-02 [1] RSPM\n#&gt;  RcppParallel   5.1.11-1 2025-08-27 [1] RSPM (R 4.5.0)\n#&gt;  reactable    * 0.4.4    2023-03-12 [1] RSPM (R 4.5.0)\n#&gt;  rlang          1.1.6    2025-04-11 [1] RSPM\n#&gt;  rmarkdown      2.30     2025-09-28 [1] RSPM (R 4.5.0)\n#&gt;  sessioninfo    1.2.3    2025-02-05 [1] RSPM\n#&gt;  SnowballC      0.7.1    2023-04-25 [1] RSPM (R 4.5.0)\n#&gt;  stopwords      2.3      2021-10-28 [1] RSPM (R 4.5.0)\n#&gt;  stringi        1.8.7    2025-03-27 [1] RSPM\n#&gt;  styler         1.10.3   2024-04-07 [1] RSPM\n#&gt;  tibble         3.3.0    2025-06-08 [1] RSPM\n#&gt;  tidylo       * 0.2.0    2022-03-22 [1] RSPM (R 4.5.0)\n#&gt;  tidyr        * 1.3.1    2024-01-24 [1] RSPM (R 4.5.0)\n#&gt;  tidyselect     1.2.1    2024-03-11 [1] RSPM (R 4.5.0)\n#&gt;  tidytext     * 0.4.3    2025-07-25 [1] RSPM (R 4.5.0)\n#&gt;  tokenizers     0.3.0    2022-12-22 [1] RSPM (R 4.5.0)\n#&gt;  udpipe       * 0.8.12   2025-09-04 [1] RSPM (R 4.5.0)\n#&gt;  V8             8.0.0    2025-09-27 [1] RSPM (R 4.5.0)\n#&gt;  vctrs          0.6.5    2023-12-01 [1] RSPM\n#&gt;  xfun           0.53     2025-08-19 [1] RSPM\n#&gt;  yaml           2.3.10   2024-07-26 [1] RSPM\n#&gt; \n#&gt;  [1] /usr/local/lib/R/site-library\n#&gt;  [2] /usr/local/lib/R/library\n#&gt;  * ── Packages attached to the search path.\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>セッション情報</span>"
    ]
  },
  {
    "objectID": "cb-preproc-menu.html",
    "href": "cb-preproc-menu.html",
    "title": "Appendix A — 前処理メニュー",
    "section": "",
    "text": "A.1 KH Coder風の可視化のレシピ集\nAppendixでは、beta版より以前のKH Coderを参考にした、実践的な可視化の例をまとめています。論文やレポートに掲載するといったことを想定しているわけではないので、わりと躊躇なくHTMLウィジェットを使っています。\nなんとなく似たような表現をめざしているだけで、KH Coderでおこなわれていた処理をRで再実装することをめざすものではありません。また、試していることが多岐にわたるため、内容やRパッケージの使い方などに誤りがある可能性があります。参考にされるなら、ご自身でもよく調べて確認しながらにしてください。\nKH Coderの機能や可視化をもとにしているのではない表現については、小見出しに🍳という絵文字を付けています。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "cb-preproc-menu.html#使用するデータセット",
    "href": "cb-preproc-menu.html#使用するデータセット",
    "title": "Appendix A — 前処理メニュー",
    "section": "A.2 使用するデータセット",
    "text": "A.2 使用するデータセット\nKH Coderのチュートリアル用のデータを使います。tutorial_data_3x.zipの中に含まれているtutorial_jp/kokoro.xlsというxlsファイルを次のように読み込んでおきます。\n\ntbl &lt;-\n  readxl::read_xls(\"tutorial_jp/kokoro.xls\",\n    col_names = c(\"text\", \"section\", \"chapter\", \"label\"),\n    skip = 1\n  ) |&gt;\n  dplyr::mutate(\n    doc_id = dplyr::row_number(),\n    dplyr::across(where(is.character), ~ audubon::strj_normalize(.))\n  ) |&gt;\n  dplyr::filter(!gibasa::is_blank(text)) |&gt;\n  dplyr::relocate(doc_id, text, section, label, chapter)\n\ntbl\n#&gt; # A tibble: 1,213 × 5\n#&gt;    doc_id text                                             section label chapter\n#&gt;     &lt;int&gt; &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  \n#&gt;  1      1 私はその人を常に先生と呼んでいた。だからここでもただ先生と書くだけで本名は打ち明けない。これは… [1]上_先… 上・一…… 1_01   \n#&gt;  2      2 私が先生と知り合いになったのは鎌倉である。その時私はまだ若々しい書生であった。暑中休暇を利用し… [1]上_先… 上・一…… 1_01   \n#&gt;  3      3 学校の授業が始まるにはまだ大分日数があるので鎌倉におってもよし、帰ってもよいという境遇にいた私… [1]上_先… 上・一…… 1_01   \n#&gt;  4      4 宿は鎌倉でも辺鄙な方角にあった。玉突きだのアイスクリームだのというハイカラなものには長い畷を一… [1]上_先… 上・一…… 1_01   \n#&gt;  5      5 私は毎日海へはいりに出掛けた。古い燻ぶり返った藁葺の間を通り抜けて磯へ下りると、この辺にこれほ… [1]上_先… 上・一…… 1_01   \n#&gt;  6      6 私は実に先生をこの雑沓の間に見付け出したのである。その時海岸には掛茶屋が二軒あった。私はふとし… [1]上_先… 上・一…… 1_01   \n#&gt;  7      7 私がその掛茶屋で先生を見た時は、先生がちょうど着物を脱いでこれから海へ入ろうとするところであっ… [1]上_先… 上・二…… 1_02   \n#&gt;  8      8 その西洋人の優れて白い皮膚の色が、掛茶屋へ入るや否や、すぐ私の注意を惹いた。純粋の日本の浴衣を… [1]上_先… 上・二…… 1_02   \n#&gt;  9      9 彼はやがて自分の傍を顧みて、そこにこごんでいる日本人に、一言二言何かいった。その日本人は砂の上… [1]上_先… 上・二…… 1_02   \n#&gt; 10     10 私は単に好奇心のために、並んで浜辺を下りて行く二人の後姿を見守っていた。すると彼らは真直に波の… [1]上_先… 上・二…… 1_02   \n#&gt; # ℹ 1,203 more rows\n\nこのデータでは、夏目漱石の『こころ』が段落（doc_id）ごとにひとつのテキストとして打ち込まれています。『こころ』は上中下の3部（section）で構成されていて、それぞれの部が複数の章（label, chapter）に分かれています。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "cb-preproc-menu.html#語の抽出a.2.2",
    "href": "cb-preproc-menu.html#語の抽出a.2.2",
    "title": "Appendix A — 前処理メニュー",
    "section": "A.3 語の抽出（A.2.2）",
    "text": "A.3 語の抽出（A.2.2）\ngibasaを使って形態素解析をおこない、語を抽出します。\nこのデータをIPA辞書を使って形態素解析すると、延べ語数は105,000語程度になります。これくらいの語数であれば、形態素解析した結果をデータフレームとしてメモリ上に読み込んでも問題ないでしょうが、ここではより大規模なテキストデータを扱う場合を想定し、結果をDuckDBデータベースに書き込むことにします。\nここではchapterごとにグルーピングしながら、段落は文に分割せずに処理します。MeCabはバッファサイズの都合上、一度に262万字くらいまで一つの文として入力できるはずですが、極端に長い入力に対してはコスト計算ができず、エラーが出る可能性があります。また、多くの文を与えればそれだけ多くの行からなるデータフレームが返されるため、一度に処理する分量は利用している環境にあわせて適当に加減したほうがよいでしょう。\nKH Coderでは、IPA辞書の品詞体系をもとに変更した品詞体系が使われています。そのため、KH Coderで前処理した結果をある程度再現するためには、一部の品詞情報を書き換える必要があります。KH Coder内で使われている品詞体系については、KH Coderのレファレンスを参照してください。\nまた、このデータを使っているチュートリアルでは、強制抽出する語として「一人」「二人」という語を指定しています。こうした語については、本来はMeCabのユーザー辞書に追加してしまったほうがよいですが、簡単に処理するために、ここではgibasaの制約付き解析機能によって「タグ」として抽出しています（KH Coderは強制抽出した語に対して「タグ」という品詞名を与えます）。\n\nsuppressPackageStartupMessages({\n  library(duckdb)\n})\ndrv &lt;- duckdb::duckdb()\n\nif (!fs::file_exists(\"tutorial_jp/kokoro.duckdb\")) {\n  con &lt;- duckdb::dbConnect(drv, dbdir = \"tutorial_jp/kokoro.duckdb\", read_only = FALSE)\n\n  dbCreateTable(\n    con, \"tokens\",\n    data.frame(\n      doc_id = integer(),\n      section = character(),\n      label = character(),\n      token_id = integer(),\n      token = character(),\n      pos = character(),\n      original = character(),\n      stringsAsFactors = FALSE\n    )\n  )\n\n  tbl |&gt;\n    dplyr::group_by(chapter) |&gt;\n    dplyr::group_walk(~ {\n      df &lt;- .x |&gt;\n        dplyr::mutate(\n          text = stringi::stri_replace_all_regex(text, \"(?&lt;codes&gt;([一二三四五六七八九]{1}人))\", \"\\n${codes}\\tタグ\\n\") |&gt;\n            stringi::stri_trim_both()\n        ) |&gt;\n        gibasa::tokenize(text, doc_id, partial = TRUE) |&gt;\n        gibasa::prettify(\n          col_select = c(\"POS1\", \"POS2\", \"POS3\", \"Original\")\n        ) |&gt;\n        dplyr::mutate(\n          pos = dplyr::case_when(\n            (POS1 == \"タグ\") ~ \"タグ\",\n            (is.na(Original) & stringr::str_detect(token, \"^[[:alpha:]]+$\")) ~ \"未知語\",\n            (POS1 == \"感動詞\") ~ \"感動詞\",\n            (POS1 == \"名詞\" & POS2 == \"一般\" & stringr::str_detect(token, \"^[\\\\p{Han}]{1}$\")) ~ \"名詞C\",\n            (POS1 == \"名詞\" & POS2 == \"一般\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"名詞B\",\n            (POS1 == \"名詞\" & POS2 == \"一般\") ~ \"名詞\",\n            (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"地域\") ~ \"地名\",\n            (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"人名\") ~ \"人名\",\n            (POS1 == \"名詞\" & POS2 == \"固有名詞\" & POS3 == \"組織\") ~ \"組織名\",\n            (POS1 == \"名詞\" & POS2 == \"形容動詞語幹\") ~ \"形容動詞\",\n            (POS1 == \"名詞\" & POS2 == \"ナイ形容詞語幹\") ~ \"ナイ形容詞\",\n            (POS1 == \"名詞\" & POS2 == \"固有名詞\") ~ \"固有名詞\",\n            (POS1 == \"名詞\" & POS2 == \"サ変接続\") ~ \"サ変名詞\",\n            (POS1 == \"名詞\" & POS2 == \"副詞可能\") ~ \"副詞可能\",\n            (POS1 == \"動詞\" & POS2 == \"自立\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"動詞B\",\n            (POS1 == \"動詞\" & POS2 == \"自立\") ~ \"動詞\",\n            (POS1 == \"形容詞\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"形容詞B\",\n            (POS1 == \"形容詞\" & POS2 == \"非自立\") ~ \"形容詞（非自立）\",\n            (POS1 == \"形容詞\") ~ \"形容詞\",\n            (POS1 == \"副詞\" & stringr::str_detect(token, \"^[\\\\p{Hiragana}]+$\")) ~ \"副詞B\",\n            (POS1 == \"副詞\") ~ \"副詞\",\n            (POS1 == \"助動詞\" & Original %in% c(\"ない\", \"まい\", \"ぬ\", \"ん\")) ~ \"否定助動詞\",\n            .default = \"その他\"\n          )\n        ) |&gt;\n        dplyr::select(doc_id, section, label, token_id, token, pos, Original) |&gt;\n        dplyr::rename(original = Original)\n\n      dbAppendTable(con, \"tokens\", df)\n    })\n} else {\n  con &lt;- duckdb::dbConnect(drv, dbdir = \"tutorial_jp/kokoro.duckdb\", read_only = TRUE)\n}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "cb-preproc-menu.html#コーディングルールa.2.5",
    "href": "cb-preproc-menu.html#コーディングルールa.2.5",
    "title": "Appendix A — 前処理メニュー",
    "section": "A.4 コーディングルール（A.2.5）",
    "text": "A.4 コーディングルール（A.2.5）\nKH Coderの強力な機能のひとつとして、「コーディングルール」によるトークンへのタグ付け機能があります。KH Coderのコーディングルールはかなり複雑な記法を扱うため、Rで完璧に再現するには相応の手間がかかりますが、一方で、コードを与えるべき抽出語を基本形とマッチングする程度であれば、次のように比較的少ないコード量で似たようなことを実現できます。\n\nrules &lt;- list(\n  \"人の死\" = c(\"死後\", \"死病\", \"死期\", \"死因\", \"死骸\", \"生死\", \"自殺\", \"殉死\", \"頓死\", \"変死\", \"亡\", \"死ぬ\", \"亡くなる\", \"殺す\", \"亡くす\", \"死\"),\n  \"恋愛\" = c(\"愛\", \"恋\", \"愛す\", \"愛情\", \"恋人\", \"愛人\", \"恋愛\", \"失恋\", \"恋しい\"),\n  \"友情\" = c(\"友達\", \"友人\", \"旧友\", \"親友\", \"朋友\", \"友\", \"級友\"),\n  \"信用・不信\" = c(\"信用\", \"信じる\", \"信ずる\", \"不信\", \"疑い\", \"疑惑\", \"疑念\", \"猜疑\", \"狐疑\", \"疑問\", \"疑い深い\", \"疑う\", \"疑る\", \"警戒\"),\n  \"病気\" = c(\"医者\", \"病人\", \"病室\", \"病院\", \"病症\", \"病状\", \"持病\", \"死病\", \"主治医\", \"精神病\", \"仮病\", \"病気\", \"看病\", \"大病\", \"病む\", \"病\")\n)\nrules_chr &lt;- purrr::flatten_chr(rules)\n\ncodes &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(original %in% rules_chr) |&gt;\n  dplyr::collect() |&gt;\n  dplyr::mutate(\n    codings = purrr::map(\n      original,\n      ~ purrr::imap(rules, \\(.x, .y) tibble::tibble(code = .y, flag = . %in% .x)) |&gt;\n        purrr::list_rbind() |&gt;\n        dplyr::filter(flag == TRUE) |&gt;\n        dplyr::select(!flag)\n    )\n  ) |&gt;\n  tidyr::unnest(codings)\n\ncodes\n#&gt; # A tibble: 537 × 8\n#&gt;    doc_id section        label  token_id token    pos      original code      \n#&gt;     &lt;int&gt; &lt;chr&gt;          &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     \n#&gt;  1      2 [1]上_先生と私 上・一       36 友達     名詞     友達     友情      \n#&gt;  2      2 [1]上_先生と私 上・一       96 友達     名詞     友達     友情      \n#&gt;  3      2 [1]上_先生と私 上・一      115 病気     サ変名詞 病気     病気      \n#&gt;  4      2 [1]上_先生と私 上・一      124 友達     名詞     友達     友情      \n#&gt;  5      2 [1]上_先生と私 上・一      128 信じ     動詞     信じる   信用・不信\n#&gt;  6      2 [1]上_先生と私 上・一      132 友達     名詞     友達     友情      \n#&gt;  7      2 [1]上_先生と私 上・一      240 病気     サ変名詞 病気     病気      \n#&gt;  8      3 [1]上_先生と私 上・一       44 友達     名詞     友達     友情      \n#&gt;  9     19 [1]上_先生と私 上・三      207 疑っ     動詞     疑う     信用・不信\n#&gt; 10     21 [1]上_先生と私 上・四      161 亡くなっ 動詞     亡くなる 人の死    \n#&gt; # ℹ 527 more rows\n\nまた、集計するだけだったらquanteda::dictionary()を使うのが手っ取り早いです。\n\nrules &lt;- quanteda::dictionary(rules)\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(original), token, original)) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_lookup(rules)\n\ndfm\n#&gt; Document-feature matrix of: 1,213 documents, 5 features (94.23% sparse) and 0 docvars.\n#&gt;     features\n#&gt; docs 人の死 恋愛 友情 信用・不信 病気\n#&gt;    1      0    0    0          0    0\n#&gt;    2      0    0    4          1    2\n#&gt;    3      0    0    1          0    0\n#&gt;    4      0    0    0          0    0\n#&gt;    6      0    0    0          0    0\n#&gt;    7      0    0    0          0    0\n#&gt; [ reached max_ndoc ... 1,207 more documents ]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "cb-preproc-menu.html#抽出語リストa.3.4",
    "href": "cb-preproc-menu.html#抽出語リストa.3.4",
    "title": "Appendix A — 前処理メニュー",
    "section": "A.5 抽出語リスト（A.3.4）",
    "text": "A.5 抽出語リスト（A.3.4）\n「エクスポート」メニューから得られるような抽出語リストをデータフレームとして得る例です。\nExcel向けの出力は見やすいようにカラムを分けていますが、Rのデータフレームとして扱うならtidyな縦長のデータにしたほうがよいです。\n\nA.5.1 品詞別・上位15語\n\ndplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(original), token, original)) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::slice_max(n, n = 15, by = pos) |&gt;\n  dplyr::collect()\n#&gt; # A tibble: 232 × 3\n#&gt;    token pos       n\n#&gt;    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt;  1 二人  タグ    115\n#&gt;  2 一人  タグ     73\n#&gt;  3 三人  タグ     10\n#&gt;  4 日蓮  人名     10\n#&gt;  5 乃木  人名      8\n#&gt;  6 寧    人名      6\n#&gt;  7 鄭    人名      6\n#&gt;  8 ちよ  人名      6\n#&gt;  9 房州  人名      5\n#&gt; 10 源    人名      5\n#&gt; # ℹ 222 more rows\n\n\n\nA.5.2 頻出150語\n\ndplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(original), token, original)) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::slice_max(n, n = 150) |&gt;\n  dplyr::collect()\n#&gt; # A tibble: 152 × 3\n#&gt;    token  pos        n\n#&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;\n#&gt;  1 先生   名詞     595\n#&gt;  2 K      未知語   411\n#&gt;  3 奥さん 名詞     388\n#&gt;  4 思う   動詞     293\n#&gt;  5 父     名詞C    269\n#&gt;  6 自分   名詞     264\n#&gt;  7 見る   動詞     225\n#&gt;  8 聞く   動詞     218\n#&gt;  9 出る   動詞     179\n#&gt; 10 人     名詞C    176\n#&gt; # ℹ 142 more rows",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "cb-preproc-menu.html#文書抽出語表a.3.5",
    "href": "cb-preproc-menu.html#文書抽出語表a.3.5",
    "title": "Appendix A — 前処理メニュー",
    "section": "A.6 「文書・抽出語」表（A.3.5）",
    "text": "A.6 「文書・抽出語」表（A.3.5）\nいわゆる文書単語行列の例です。dplyr::collectした後にtidyr::pivot_wider()などで横に展開してもよいですが、多くの場合、疎行列のオブジェクトにしてしまったほうが、この後にRでの解析に用いる上では扱いやすいと思われます。quantedaのdfmオブジェクトをふつうの密な行列にしたいときは、as.matrix(dfm)すればよいです。\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_trim(min_termfreq = 75, termfreq_type = \"rank\")\n\nquanteda::docvars(dfm, \"section\") &lt;-\n  dplyr::filter(tbl, doc_id %in% quanteda::docnames(dfm)) |&gt;\n  dplyr::pull(\"section\")\n\ndfm\n#&gt; Document-feature matrix of: 1,189 documents, 76 features (92.93% sparse) and 1 docvar.\n#&gt;     features\n#&gt; docs 行く/動詞 東京/地名 知る/動詞 外/名詞C 先生/名詞 一人/タグ 前/副詞可能\n#&gt;    2         1         1         0        0         1         1           0\n#&gt;    3         0         0         0        0         0         1           0\n#&gt;    4         0         0         0        0         0         0           0\n#&gt;    5         0         0         1        0         0         1           0\n#&gt;    6         1         0         0        1         1         0           0\n#&gt;    7         0         0         0        0         5         1           0\n#&gt;     features\n#&gt; docs 見る/動詞 出る/動詞 見える/動詞\n#&gt;    2         0         0           0\n#&gt;    3         0         0           0\n#&gt;    4         0         0           0\n#&gt;    5         0         0           0\n#&gt;    6         0         0           0\n#&gt;    7         1         0           0\n#&gt; [ reached max_ndoc ... 1,183 more documents, reached max_nfeat ... 66 more features ]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "cb-preproc-menu.html#文書コード表a.3.6",
    "href": "cb-preproc-menu.html#文書コード表a.3.6",
    "title": "Appendix A — 前処理メニュー",
    "section": "A.7 「文書・コード」表（A.3.6）",
    "text": "A.7 「文書・コード」表（A.3.6）\n「文書・コード」行列の例です。コードの出現頻度ではなく「コードの有無をあらわす2値変数」を出力します。\n\ndfm &lt;- codes |&gt;\n  dplyr::count(doc_id, code) |&gt;\n  tidytext::cast_dfm(doc_id, code, n) |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\")\n\nquanteda::docvars(dfm, \"section\") &lt;-\n  dplyr::filter(tbl, doc_id %in% quanteda::docnames(dfm)) |&gt;\n  dplyr::pull(\"section\")\n\ndfm\n#&gt; Document-feature matrix of: 294 documents, 5 features (76.19% sparse) and 1 docvar.\n#&gt;     features\n#&gt; docs 信用・不信 友情 病気 人の死 恋愛\n#&gt;   2           1    1    1      0    0\n#&gt;   3           0    1    0      0    0\n#&gt;   19          1    0    0      0    0\n#&gt;   21          0    0    0      1    0\n#&gt;   37          0    0    0      1    0\n#&gt;   49          0    1    0      0    0\n#&gt; [ reached max_ndoc ... 288 more documents ]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "cb-preproc-menu.html#抽出語文脈ベクトル表a.3.7",
    "href": "cb-preproc-menu.html#抽出語文脈ベクトル表a.3.7",
    "title": "Appendix A — 前処理メニュー",
    "section": "A.8 「抽出語・文脈ベクトル」表（A.3.7）",
    "text": "A.8 「抽出語・文脈ベクトル」表（A.3.7）\n\nA.8.1 word2vec🍳\n\n\n\n\n\n\nCaution\n\n\n\n以下で扱っているベクトルは、KH Coderにおける「抽出語・文脈ベクトル」とは異なるものです\n\n\nKH Coderにおける「文脈ベクトル」は、これを使って抽出語メニューからおこなえるような分析をすることによって、「似たような使われ方をする語を調べる」使い方をするためのものだと思われます。\n「似たような使われ方をする語を調べる」ためであれば、単語埋め込みを使ってもよさそうです。ただし、KH Coderの「文脈ベクトル」を使う場合の「似たような使われ方をする」が、あくまで分析対象とする文書のなかでという意味であるのに対して、単語埋め込みを使う場合では、埋め込みの学習に使われた文書のなかでという意味になってしまう点に注意が必要です。\n試しに、既存のword2vecモデルから単語ベクトルを読み込んでみます。ここでは、Wikipedia2Vecの100次元のものを使います。だいぶ古いモデルですが、MeCab+IPA辞書で分かち書きされた語彙を使っていることが確認できる単語埋め込みとなると（参考）、これくらいの時期につくられたものになりそうです。\n\n# word2vecのテキスト形式のファイルは、先頭行に埋め込みの「行数 列数」が書かれている\nreadr::read_lines(\"tutorial_jp/jawiki_20180420_100d.txt.bz2\", n_max = 1)\n#&gt; [1] \"1593143 100\"\n\n# 下のほうは低頻度語で、全部読み込む必要はないと思われるので、ここでは先頭から1e5行だけ読み込む\nembeddings &lt;-\n  readr::read_delim(\n    \"tutorial_jp/jawiki_20180420_100d.txt.bz2\",\n    delim = \" \",\n    col_names = c(\"token\", paste0(\"dim_\", seq_len(100))),\n    skip = 1,\n    n_max = 1e5,\n    show_col_types = FALSE\n  )\n\n# メモリ上でのサイズ\nlobstr::obj_size(embeddings)\n#&gt; 117.94 MB\n\nこのうち、分析対象の文書に含まれる語彙のベクトルだけを適当に取り出しておきます。\n\nembeddings &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::transmute(\n    doc_id = doc_id,\n    token = token,\n    label = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::distinct(doc_id, token, .keep_all = TRUE) |&gt;\n  dplyr::collect() |&gt;\n  dplyr::inner_join(embeddings, by = dplyr::join_by(token == token))\n\nembeddings\n#&gt; # A tibble: 10,578 × 103\n#&gt;    doc_id token  label      dim_1   dim_2   dim_3   dim_4   dim_5   dim_6  dim_7\n#&gt;     &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1      1 遠慮   遠慮/サ変名詞…  0.653  -0.404  -0.112  -0.242   0.114   0.155  0.0425\n#&gt;  2      2 勧     勧/人名   1.00   -0.378  -0.202   0.621   0.344  -1.16   0.444 \n#&gt;  3      2 工面   工面/サ変名詞…  0.87   -0.640  -0.308  -0.534  -0.351   0.582  0.300 \n#&gt;  4      2 暑中   暑中/名詞……  0.962   0.0949 -0.0537 -0.0839  0.0502 -0.568  0.952 \n#&gt;  5      2 金     金/名詞C  0.389  -0.466   0.103  -0.0886  0.261  -0.453  0.682 \n#&gt;  6      3 変り   変り/名詞…… -0.0866 -0.638   0.234   0.258  -0.0035 -0.183  0.259 \n#&gt;  7      4 行っ   行っ/動詞……  0.428   0.0491  0.158   0.505  -0.436   0.123  0.460 \n#&gt;  8      5 燻     燻/名詞C  0.231  -0.448   0.449   0.158  -0.0838 -0.798  0.818 \n#&gt;  9      6 見付け 見付け/動詞……  0.888  -0.274  -0.0161  0.0271  0.264   0.0593 0.428 \n#&gt; 10     10 広々   広々/副詞……  0.136  -0.0064  0.673   0.406   0.350   0.438  0.496 \n#&gt; # ℹ 10,568 more rows\n#&gt; # ℹ 93 more variables: dim_8 &lt;dbl&gt;, dim_9 &lt;dbl&gt;, dim_10 &lt;dbl&gt;, dim_11 &lt;dbl&gt;,\n#&gt; #   dim_12 &lt;dbl&gt;, dim_13 &lt;dbl&gt;, dim_14 &lt;dbl&gt;, dim_15 &lt;dbl&gt;, dim_16 &lt;dbl&gt;,\n#&gt; #   dim_17 &lt;dbl&gt;, dim_18 &lt;dbl&gt;, dim_19 &lt;dbl&gt;, dim_20 &lt;dbl&gt;, dim_21 &lt;dbl&gt;,\n#&gt; #   dim_22 &lt;dbl&gt;, dim_23 &lt;dbl&gt;, dim_24 &lt;dbl&gt;, dim_25 &lt;dbl&gt;, dim_26 &lt;dbl&gt;,\n#&gt; #   dim_27 &lt;dbl&gt;, dim_28 &lt;dbl&gt;, dim_29 &lt;dbl&gt;, dim_30 &lt;dbl&gt;, dim_31 &lt;dbl&gt;,\n#&gt; #   dim_32 &lt;dbl&gt;, dim_33 &lt;dbl&gt;, dim_34 &lt;dbl&gt;, dim_35 &lt;dbl&gt;, dim_36 &lt;dbl&gt;, …\n\n\n\nA.8.2 独立成分分析（ICA）🍳\nword2vecを含む埋め込み表現は、「独立成分分析（ICA）で次元削減することで、人間にとって解釈性の高い成分を取り出すことができる」ことが知られています（参考）。これを応用すると、ICAで取り出した成分をもとにして、コーディングルールにするとよさそうなカテゴリや語彙を探索できるかもしれません。\n\nica &lt;- embeddings |&gt;\n  dplyr::select(label, dplyr::starts_with(\"dim\")) |&gt;\n  dplyr::distinct() |&gt;\n  tibble::column_to_rownames(\"label\") |&gt;\n  as.matrix() |&gt;\n  fastICA::fastICA(n.comp = 20)\n\ndat &lt;- ica$S |&gt;\n  rlang::as_function(~ {\n    . * sign(moments::skewness(.)) # 正方向のスコアだけを扱うため、歪度が負の成分を変換する\n  })() |&gt;\n  dplyr::as_tibble(\n    .name_repair = ~ paste(\"dim\", stringr::str_pad(seq_along(.), width = 2, pad = \"0\"), sep = \"_\"),\n    rownames = \"label\"\n  ) |&gt;\n  tidyr::pivot_longer(cols = !label, names_to = \"dim\", values_to = \"score\")\n\nここでは正方向のスコアだけを扱うため、歪度が負の成分を正方向に変換する処理をしています。もちろん、実際には負の方向のスコアが大きい語彙とあわせて解釈したほうがわかりやすい成分もありそうなことからも、そのあたりも含めてそれぞれの成分を探索し、ほかの分析とも組み合わせながらコーディングルールを考えるべきでしょう。\n各成分の正方向のスコアが大きい語彙を図示すると次のような感じになります。\n\nlibrary(ggplot2)\n\ndat |&gt;\n  dplyr::slice_max(score, n = 8, by = dim) |&gt;\n  ggplot(aes(x = reorder(label, score), y = score)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(vars(dim), ncol = 4, scales = \"free_y\") +\n  theme_minimal() +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n\n\n\n実際にはこうして得られたスコアの大きい語をそのままコーディングルールとして採用することはしないほうがよいでしょうが、ここから次のようにして「文書・コード」表をつくることもできます。\n\nrules &lt;- dat |&gt;\n  dplyr::slice_max(score, n = 10, by = dim) |&gt;\n  dplyr::reframe(\n    label = list(label),\n    .by = dim\n  ) |&gt;\n  tibble::deframe()\n\ncodes &lt;-\n  embeddings |&gt;\n  dplyr::select(doc_id, label) |&gt;\n  dplyr::filter(label %in% purrr::flatten_chr(rules)) |&gt;\n  dplyr::mutate(\n    codings = purrr::map(\n      label,\n      ~ purrr::imap(rules, \\(.x, .y) tibble::tibble(code = .y, flag = . %in% .x)) |&gt;\n        purrr::list_rbind() |&gt;\n        dplyr::filter(flag == TRUE) |&gt;\n        dplyr::select(!flag)\n    )\n  ) |&gt;\n  tidyr::unnest(codings)\n\ndfm &lt;- codes |&gt;\n  dplyr::count(doc_id, code) |&gt;\n  tidytext::cast_dfm(doc_id, code, n) |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\")\n\ndfm\n#&gt; Document-feature matrix of: 347 documents, 20 features (93.14% sparse) and 0 docvars.\n#&gt;     features\n#&gt; docs dim_04 dim_14 dim_05 dim_01 dim_17 dim_19 dim_06 dim_15 dim_20 dim_08\n#&gt;    2      1      1      0      0      0      0      0      0      0      0\n#&gt;    3      0      0      1      0      0      0      0      0      0      0\n#&gt;    4      0      0      0      1      0      0      0      0      0      0\n#&gt;    5      0      0      0      1      1      1      0      0      0      0\n#&gt;    6      0      0      0      0      0      0      1      0      0      0\n#&gt;    7      1      0      0      0      0      0      0      0      0      0\n#&gt; [ reached max_ndoc ... 341 more documents, reached max_nfeat ... 10 more features ]\n\n\n\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)\n\nsessioninfo::session_info(info = \"packages\")\n#&gt; ═ Session info ═══════════════════════════════════════════════════════════════\n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package      * version  date (UTC) lib source\n#&gt;  audubon        0.5.2    2024-04-27 [1] RSPM (R 4.5.0)\n#&gt;  bit            4.6.0    2025-03-06 [1] RSPM (R 4.5.0)\n#&gt;  bit64          4.6.0-1  2025-01-16 [1] RSPM (R 4.5.0)\n#&gt;  blob           1.2.4    2023-03-17 [1] RSPM\n#&gt;  cachem         1.1.0    2024-05-16 [1] RSPM\n#&gt;  cellranger     1.1.0    2016-07-27 [1] RSPM (R 4.5.0)\n#&gt;  cli            3.6.5    2025-04-23 [1] RSPM\n#&gt;  codetools      0.2-20   2024-03-31 [2] CRAN (R 4.5.1)\n#&gt;  crayon         1.5.3    2024-06-20 [1] RSPM\n#&gt;  curl           7.0.0    2025-08-19 [1] RSPM\n#&gt;  DBI          * 1.2.3    2024-06-02 [1] RSPM (R 4.5.0)\n#&gt;  dbplyr         2.5.1    2025-09-10 [1] RSPM\n#&gt;  digest         0.6.37   2024-08-19 [1] RSPM\n#&gt;  dplyr          1.1.4    2023-11-17 [1] RSPM (R 4.5.0)\n#&gt;  duckdb       * 1.4.0    2025-09-18 [1] RSPM (R 4.5.0)\n#&gt;  evaluate       1.0.5    2025-08-27 [1] RSPM\n#&gt;  farver         2.1.2    2024-05-13 [1] RSPM (R 4.5.0)\n#&gt;  fastICA        1.2-7    2024-12-11 [1] RSPM (R 4.5.0)\n#&gt;  fastmap        1.2.0    2024-05-15 [1] RSPM\n#&gt;  fastmatch      1.1-6    2024-12-23 [1] RSPM (R 4.5.0)\n#&gt;  fs             1.6.6    2025-04-12 [1] RSPM\n#&gt;  generics       0.1.4    2025-05-09 [1] RSPM (R 4.5.0)\n#&gt;  ggplot2      * 4.0.0    2025-09-11 [1] RSPM (R 4.5.0)\n#&gt;  gibasa         1.1.2    2025-02-16 [1] RSPM (R 4.5.0)\n#&gt;  glue           1.8.0    2024-09-30 [1] RSPM\n#&gt;  gtable         0.3.6    2024-10-25 [1] RSPM (R 4.5.0)\n#&gt;  hms            1.1.3    2023-03-21 [1] RSPM (R 4.5.0)\n#&gt;  htmltools      0.5.8.1  2024-04-04 [1] RSPM\n#&gt;  htmlwidgets    1.6.4    2023-12-06 [1] RSPM\n#&gt;  janeaustenr    1.0.0    2022-08-26 [1] RSPM (R 4.5.0)\n#&gt;  jsonlite       2.0.0    2025-03-27 [1] RSPM\n#&gt;  knitr          1.50     2025-03-16 [1] RSPM\n#&gt;  labeling       0.4.3    2023-08-29 [1] RSPM (R 4.5.0)\n#&gt;  lattice        0.22-7   2025-04-02 [2] CRAN (R 4.5.1)\n#&gt;  lifecycle      1.0.4    2023-11-07 [1] RSPM\n#&gt;  lobstr         1.1.2    2022-06-22 [1] RSPM (R 4.5.0)\n#&gt;  magrittr       2.0.4    2025-09-12 [1] RSPM\n#&gt;  Matrix         1.7-3    2025-03-11 [2] CRAN (R 4.5.1)\n#&gt;  memoise        2.0.1    2021-11-26 [1] RSPM\n#&gt;  moments        0.14.1   2022-05-02 [1] RSPM (R 4.5.0)\n#&gt;  pillar         1.11.1   2025-09-17 [1] RSPM\n#&gt;  pkgconfig      2.0.3    2019-09-22 [1] RSPM\n#&gt;  prettyunits    1.2.0    2023-09-24 [1] RSPM\n#&gt;  purrr          1.1.0    2025-07-10 [1] RSPM\n#&gt;  quanteda       4.3.1    2025-07-10 [1] RSPM (R 4.5.0)\n#&gt;  R.cache        0.17.0   2025-05-02 [1] RSPM\n#&gt;  R.methodsS3    1.8.2    2022-06-13 [1] RSPM\n#&gt;  R.oo           1.27.1   2025-05-02 [1] RSPM\n#&gt;  R.utils        2.13.0   2025-02-24 [1] RSPM\n#&gt;  R6             2.6.1    2025-02-15 [1] RSPM\n#&gt;  RColorBrewer   1.1-3    2022-04-03 [1] RSPM (R 4.5.0)\n#&gt;  Rcpp           1.1.0    2025-07-02 [1] RSPM\n#&gt;  RcppParallel   5.1.11-1 2025-08-27 [1] RSPM (R 4.5.0)\n#&gt;  readr          2.1.5    2024-01-10 [1] RSPM (R 4.5.0)\n#&gt;  readxl         1.4.5    2025-03-07 [1] RSPM (R 4.5.0)\n#&gt;  rlang          1.1.6    2025-04-11 [1] RSPM\n#&gt;  rmarkdown      2.30     2025-09-28 [1] RSPM (R 4.5.0)\n#&gt;  S7             0.2.0    2024-11-07 [1] RSPM (R 4.5.0)\n#&gt;  scales         1.4.0    2025-04-24 [1] RSPM (R 4.5.0)\n#&gt;  sessioninfo    1.2.3    2025-02-05 [1] RSPM\n#&gt;  SnowballC      0.7.1    2023-04-25 [1] RSPM (R 4.5.0)\n#&gt;  stopwords      2.3      2021-10-28 [1] RSPM (R 4.5.0)\n#&gt;  stringi        1.8.7    2025-03-27 [1] RSPM\n#&gt;  stringr        1.5.2    2025-09-08 [1] RSPM\n#&gt;  styler         1.10.3   2024-04-07 [1] RSPM\n#&gt;  tibble         3.3.0    2025-06-08 [1] RSPM\n#&gt;  tidyr          1.3.1    2024-01-24 [1] RSPM (R 4.5.0)\n#&gt;  tidyselect     1.2.1    2024-03-11 [1] RSPM (R 4.5.0)\n#&gt;  tidytext       0.4.3    2025-07-25 [1] RSPM (R 4.5.0)\n#&gt;  tokenizers     0.3.0    2022-12-22 [1] RSPM (R 4.5.0)\n#&gt;  tzdb           0.5.0    2025-03-15 [1] RSPM (R 4.5.0)\n#&gt;  utf8           1.2.6    2025-06-08 [1] RSPM\n#&gt;  V8             8.0.0    2025-09-27 [1] RSPM (R 4.5.0)\n#&gt;  vctrs          0.6.5    2023-12-01 [1] RSPM\n#&gt;  vroom          1.6.6    2025-09-19 [1] RSPM (R 4.5.0)\n#&gt;  withr          3.0.2    2024-10-28 [1] RSPM\n#&gt;  xfun           0.53     2025-08-19 [1] RSPM\n#&gt;  yaml           2.3.10   2024-07-26 [1] RSPM\n#&gt; \n#&gt;  [1] /usr/local/lib/R/site-library\n#&gt;  [2] /usr/local/lib/R/library\n#&gt;  * ── Packages attached to the search path.\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>前処理メニュー</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-1.html",
    "href": "cb-words-menu-1.html",
    "title": "Appendix B — 抽出語メニュー1",
    "section": "",
    "text": "B.1 抽出語リスト（A.5.1）\n活用語をクリックするとそれぞれの活用の出現頻度も見れるUIについては、Rだとどうすれば実現できるかわからないです……。\ndat &lt;- dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::filter(n &gt;= 100) |&gt;\n  dplyr::collect()\n\nreactable::reactable(\n  dat,\n  filterable = TRUE,\n  defaultColDef = reactable::colDef(\n    cell = reactablefmtr::data_bars(dat, text_position = \"outside-base\")\n  )\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>抽出語メニュー1</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-1.html#出現回数の分布a.5.2",
    "href": "cb-words-menu-1.html#出現回数の分布a.5.2",
    "title": "Appendix B — 抽出語メニュー1",
    "section": "B.2 出現回数の分布（A.5.2）",
    "text": "B.2 出現回数の分布（A.5.2）\n\nB.2.1 度数分布表\n\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    !pos %in% c(\"その他\", \"名詞B\", \"動詞B\", \"形容詞B\", \"副詞B\", \"否定助動詞\", \"形容詞（非自立）\")\n  ) |&gt;\n  dplyr::count(token, pos) |&gt;\n  dplyr::summarise(\n    degree = sum(n, na.rm = TRUE),\n    .by = n\n  ) |&gt;\n  dplyr::mutate(\n    prop = degree / sum(degree, na.rm = TRUE)\n  ) |&gt;\n  dplyr::arrange(n) |&gt;\n  dplyr::compute() |&gt;\n  dplyr::mutate(\n    cum_degree = cumsum(degree),\n    cum_prop = cumsum(prop)\n  ) |&gt;\n  dplyr::collect()\n\ndat\n#&gt; # A tibble: 98 × 5\n#&gt;        n degree   prop cum_degree cum_prop\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1   2938 0.111        2938    0.111\n#&gt;  2     2   1934 0.0729       4872    0.184\n#&gt;  3     3   1506 0.0568       6378    0.240\n#&gt;  4     4   1248 0.0470       7626    0.287\n#&gt;  5     5    955 0.0360       8581    0.323\n#&gt;  6     6    768 0.0289       9349    0.352\n#&gt;  7     7    868 0.0327      10217    0.385\n#&gt;  8     8    592 0.0223      10809    0.407\n#&gt;  9     9    531 0.0200      11340    0.427\n#&gt; 10    10    570 0.0215      11910    0.449\n#&gt; # ℹ 88 more rows\n\n\n\nB.2.2 折れ線グラフ\n\ndat |&gt;\n  dplyr::filter(cum_prop &lt; .8) |&gt;\n  ggplot(aes(x = n, y = degree)) +\n  geom_line() +\n  theme_bw() +\n  labs(x = \"出現回数\", y = \"度数\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>抽出語メニュー1</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-1.html#文書数の分布a.5.3",
    "href": "cb-words-menu-1.html#文書数の分布a.5.3",
    "title": "Appendix B — 抽出語メニュー1",
    "section": "B.3 文書数の分布（A.5.3）",
    "text": "B.3 文書数の分布（A.5.3）\n\nB.3.1 ヒストグラム🍳\n段落（doc_id）ではなく、章ラベル（label）でグルーピングして集計しています。docfreqについて上でやったのと同様に処理すれば度数分布表にできます。\n\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::mutate(token = paste(token, pos, sep = \"/\")) |&gt;\n  dplyr::count(label, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(label, token, n) |&gt;\n  quanteda.textstats::textstat_frequency() |&gt;\n  dplyr::as_tibble()\n\ndat\n#&gt; # A tibble: 7,140 × 5\n#&gt;    feature   frequency  rank docfreq group\n#&gt;    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n#&gt;  1 の/その他      5801     1     110 all  \n#&gt;  2 た/その他      5396     2     110 all  \n#&gt;  3 。/その他      4648     3     110 all  \n#&gt;  4 は/その他      4148     4     110 all  \n#&gt;  5 に/その他      4104     5     110 all  \n#&gt;  6 、/その他      3646     6     110 all  \n#&gt;  7 て/その他      3403     7     110 all  \n#&gt;  8 を/その他      3216     8     110 all  \n#&gt;  9 私/その他      2694     9     110 all  \n#&gt; 10 が/その他      2194    10     110 all  \n#&gt; # ℹ 7,130 more rows\n\n度数分布をグラフで確認したいだけなら、このかたちからヒストグラムを描いたほうが楽です。\n\ndat |&gt;\n  ggplot(aes(x = docfreq)) +\n  geom_histogram(binwidth = 3) +\n  scale_y_sqrt() +\n  theme_bw() +\n  labs(x = \"文書数\", y = \"度数\")\n\n\n\n\n\n\n\n\n\n\nB.3.2 Zipf’s law🍳\nよく見かけるようなグラフです。言語処理100本ノック 2020の39はこれにあたります。\n\ndat |&gt;\n  ggplot(aes(x = rank, y = frequency)) +\n  geom_line() +\n  geom_smooth(method = lm, formula = y ~ x, se = FALSE) +\n  scale_x_log10() +\n  scale_y_log10() +\n  theme_bw()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>抽出語メニュー1</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-1.html#出現回数文書数のプロットa.5.4",
    "href": "cb-words-menu-1.html#出現回数文書数のプロットa.5.4",
    "title": "Appendix B — 抽出語メニュー1",
    "section": "B.4 出現回数・文書数のプロット（A.5.4）",
    "text": "B.4 出現回数・文書数のプロット（A.5.4）\n図 A.25のようなグラフの例です。ggplot2でgraphics::identify()のようなことをするやり方がわからなかったので、適当に条件を指定してgghighlightでハイライトしています。\n\ndat |&gt;\n  ggplot(aes(x = frequency, y = docfreq)) +\n  geom_jitter() +\n  gghighlight::gghighlight(\n    frequency &gt; 100 & docfreq &lt; 60\n  ) +\n  ggrepel::geom_text_repel(aes(label = feature)) +\n  scale_x_log10() +\n  theme_bw() +\n  labs(x = \"出現回数\", y = \"文書数\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>抽出語メニュー1</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-1.html#kwica.5.5",
    "href": "cb-words-menu-1.html#kwica.5.5",
    "title": "Appendix B — 抽出語メニュー1",
    "section": "B.5 KWIC（A.5.5）",
    "text": "B.5 KWIC（A.5.5）\n\nB.5.1 コンコーダンス\nコンコーダンスはquanteda::kwic()で確認できます。もっとも、KH Coderの提供するコンコーダンス検索やコロケーション統計のほうが明らかにリッチなのと、Rのコンソールは日本語の長めの文字列を表示するのにあまり向いていないというのがあるので、このあたりの機能が必要ならKH Coderを利用したほうがよいでしょう。\n\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(section == \"[1]上_先生と私\") |&gt;\n  dplyr::select(label, token) |&gt;\n  dplyr::collect() |&gt;\n  dplyr::reframe(token = list(token), .by = label) |&gt;\n  tibble::deframe() |&gt;\n  quanteda::as.tokens() |&gt;\n  quanteda::tokens_select(\"[[:punct:]]\", selection = \"remove\", valuetype = \"regex\", padding = FALSE) |&gt;\n  quanteda::tokens_select(\"^[\\\\p{Hiragana}]{1,2}$\", selection = \"remove\", valuetype = \"regex\", padding = TRUE)\n\nquanteda::kwic(dat, pattern = \"^向[いくけこ]$\", window = 5, valuetype = \"regex\")\n#&gt; Keyword-in-context with 18 matches.                                                                    \n#&gt;      [上・二, 183]            海 方 | 向い |  立っ                  \n#&gt;      [上・二, 527]            沖 方 | 向い |  行っ それから 引き返し\n#&gt;      [上・七, 740]            外 方 | 向い |  今 手                 \n#&gt;      [上・八, 622]            私 方 | 向い |  私                    \n#&gt;      [上・八, 697]            私 方 | 向い |  子供                  \n#&gt;    [上・十二, 568]        花 そちら | 向い |  眼 峙                 \n#&gt;    [上・十二, 628]          方角 足 | 向け |  それから 私           \n#&gt;    [上・十四, 247]            庭 方 | 向い |  庭 この間             \n#&gt;    [上・十五, 661]      奥さん 差し | 向い |  話 なけれ             \n#&gt;    [上・十六, 265]            眼 私 | 向け |  そうして 客 来        \n#&gt;    [上・十七, 447]    世の中 どっち | 向い |  面白                  \n#&gt;  [上・二十五, 557]        世間 背中 | 向け |  人 苦味               \n#&gt;  [上・二十六, 480]                  | 向い |  歩い やがて 若葉      \n#&gt;  [上・三十三, 686]            庭 方 | 向い |  澄まし 烟草           \n#&gt;  [上・三十三, 771] 先生 ちょっと 顔 | 向け | 直し 奥さん 言葉       \n#&gt;  [上・三十四, 415]               下 | 向い |  私 父                 \n#&gt;  [上・三十四, 436]   突然 奥さん 方 | 向い |  静 お前               \n#&gt;  [上・三十五, 425]            庭 方 | 向い |  笑っ しかし\n\nなお、上の例ではひらがな1～2文字の語をpaddingしつつ除外したので、一部の助詞などは表示されていません（それぞれの窓のなかでトークンとして数えられてはいます）。\n\n\nB.5.2 コロケーション\nたとえば、前後5個のwindow内のコロケーション（nodeを含めて11語の窓ということ）の合計については次のように確認できます。\n\ndat |&gt;\n  quanteda::fcm(context = \"window\", window = 5) |&gt;\n  tidytext::tidy() |&gt;\n  dplyr::rename(node = document, term = term) |&gt;\n  dplyr::filter(node == \"向い\") |&gt;\n  dplyr::slice_max(count, n = 10)\n#&gt; # A tibble: 30 × 3\n#&gt;    node  term     count\n#&gt;    &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1 向い  庭           4\n#&gt;  2 向い  奥さん       2\n#&gt;  3 向い  立っ         1\n#&gt;  4 向い  眼           1\n#&gt;  5 向い  やがて       1\n#&gt;  6 向い  沖           1\n#&gt;  7 向い  それから     1\n#&gt;  8 向い  引き返し     1\n#&gt;  9 向い  烟草         1\n#&gt; 10 向い  しかし       1\n#&gt; # ℹ 20 more rows\n\n「左合計」や「右合計」については、たとえば次のようにして確認できます。paddingしなければtidyr::separate_wider_delim()で展開して位置ごとに集計することもできそうです。\n\ndat |&gt;\n  quanteda::kwic(pattern = \"^向[いくけこ]$\", window = 5, valuetype = \"regex\") |&gt;\n  dplyr::as_tibble() |&gt;\n  dplyr::select(docname, keyword, pre, post) |&gt;\n  tidyr::pivot_longer(\n    c(pre, post),\n    names_to = \"window\",\n    values_to = \"term\",\n    values_transform = ~ strsplit(., \" \", fixed = TRUE)\n  ) |&gt;\n  tidyr::unnest(term) |&gt;\n  dplyr::count(window, term, sort = TRUE)\n#&gt; # A tibble: 57 × 3\n#&gt;    window term           n\n#&gt;    &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;\n#&gt;  1 post   \"\"            17\n#&gt;  2 pre    \"\"            14\n#&gt;  3 pre    \"方\"           9\n#&gt;  4 post   \"私\"           3\n#&gt;  5 pre    \"庭\"           3\n#&gt;  6 pre    \"私\"           3\n#&gt;  7 post   \"それから\"     2\n#&gt;  8 pre    \"奥さん\"       2\n#&gt;  9 post   \"お前\"         1\n#&gt; 10 post   \"この間\"       1\n#&gt; # ℹ 47 more rows\n\n\n\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)\n\nsessioninfo::session_info(info = \"packages\")\n#&gt; ═ Session info ═══════════════════════════════════════════════════════════════\n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package            * version  date (UTC) lib source\n#&gt;  audubon              0.5.2    2024-04-27 [1] RSPM (R 4.5.0)\n#&gt;  blob                 1.2.4    2023-03-17 [1] RSPM\n#&gt;  cachem               1.1.0    2024-05-16 [1] RSPM\n#&gt;  cellranger           1.1.0    2016-07-27 [1] RSPM (R 4.5.0)\n#&gt;  cli                  3.6.5    2025-04-23 [1] RSPM\n#&gt;  codetools            0.2-20   2024-03-31 [2] CRAN (R 4.5.1)\n#&gt;  crosstalk            1.2.2    2025-08-26 [1] RSPM (R 4.5.0)\n#&gt;  curl                 7.0.0    2025-08-19 [1] RSPM\n#&gt;  DBI                * 1.2.3    2024-06-02 [1] RSPM (R 4.5.0)\n#&gt;  dbplyr               2.5.1    2025-09-10 [1] RSPM\n#&gt;  digest               0.6.37   2024-08-19 [1] RSPM\n#&gt;  dplyr                1.1.4    2023-11-17 [1] RSPM (R 4.5.0)\n#&gt;  duckdb             * 1.4.0    2025-09-18 [1] RSPM (R 4.5.0)\n#&gt;  evaluate             1.0.5    2025-08-27 [1] RSPM\n#&gt;  farver               2.1.2    2024-05-13 [1] RSPM (R 4.5.0)\n#&gt;  fastmap              1.2.0    2024-05-15 [1] RSPM\n#&gt;  fastmatch            1.1-6    2024-12-23 [1] RSPM (R 4.5.0)\n#&gt;  generics             0.1.4    2025-05-09 [1] RSPM (R 4.5.0)\n#&gt;  gghighlight          0.5.0    2025-06-14 [1] RSPM (R 4.5.0)\n#&gt;  ggplot2            * 4.0.0    2025-09-11 [1] RSPM (R 4.5.0)\n#&gt;  ggrepel              0.9.6    2024-09-07 [1] RSPM (R 4.5.0)\n#&gt;  gibasa               1.1.2    2025-02-16 [1] RSPM (R 4.5.0)\n#&gt;  glue                 1.8.0    2024-09-30 [1] RSPM\n#&gt;  gtable               0.3.6    2024-10-25 [1] RSPM (R 4.5.0)\n#&gt;  htmltools            0.5.8.1  2024-04-04 [1] RSPM\n#&gt;  htmlwidgets          1.6.4    2023-12-06 [1] RSPM\n#&gt;  janeaustenr          1.0.0    2022-08-26 [1] RSPM (R 4.5.0)\n#&gt;  jsonlite             2.0.0    2025-03-27 [1] RSPM\n#&gt;  knitr                1.50     2025-03-16 [1] RSPM\n#&gt;  labeling             0.4.3    2023-08-29 [1] RSPM (R 4.5.0)\n#&gt;  lattice              0.22-7   2025-04-02 [2] CRAN (R 4.5.1)\n#&gt;  lifecycle            1.0.4    2023-11-07 [1] RSPM\n#&gt;  magrittr             2.0.4    2025-09-12 [1] RSPM\n#&gt;  Matrix               1.7-3    2025-03-11 [2] CRAN (R 4.5.1)\n#&gt;  memoise              2.0.1    2021-11-26 [1] RSPM\n#&gt;  mgcv                 1.9-3    2025-04-04 [2] CRAN (R 4.5.1)\n#&gt;  nlme                 3.1-168  2025-03-31 [2] CRAN (R 4.5.1)\n#&gt;  nsyllable            1.0.1    2022-02-28 [1] RSPM (R 4.5.0)\n#&gt;  pillar               1.11.1   2025-09-17 [1] RSPM\n#&gt;  pkgconfig            2.0.3    2019-09-22 [1] RSPM\n#&gt;  purrr                1.1.0    2025-07-10 [1] RSPM\n#&gt;  quanteda             4.3.1    2025-07-10 [1] RSPM (R 4.5.0)\n#&gt;  quanteda.textstats   0.97.2   2024-09-03 [1] RSPM (R 4.5.0)\n#&gt;  R.cache              0.17.0   2025-05-02 [1] RSPM\n#&gt;  R.methodsS3          1.8.2    2022-06-13 [1] RSPM\n#&gt;  R.oo                 1.27.1   2025-05-02 [1] RSPM\n#&gt;  R.utils              2.13.0   2025-02-24 [1] RSPM\n#&gt;  R6                   2.6.1    2025-02-15 [1] RSPM\n#&gt;  RColorBrewer         1.1-3    2022-04-03 [1] RSPM (R 4.5.0)\n#&gt;  Rcpp                 1.1.0    2025-07-02 [1] RSPM\n#&gt;  RcppParallel         5.1.11-1 2025-08-27 [1] RSPM (R 4.5.0)\n#&gt;  reactable            0.4.4    2023-03-12 [1] RSPM (R 4.5.0)\n#&gt;  reactablefmtr        2.0.0    2022-03-16 [1] RSPM (R 4.5.0)\n#&gt;  reactR               0.6.1    2024-09-14 [1] RSPM (R 4.5.0)\n#&gt;  readxl               1.4.5    2025-03-07 [1] RSPM (R 4.5.0)\n#&gt;  rlang                1.1.6    2025-04-11 [1] RSPM\n#&gt;  rmarkdown            2.30     2025-09-28 [1] RSPM (R 4.5.0)\n#&gt;  S7                   0.2.0    2024-11-07 [1] RSPM (R 4.5.0)\n#&gt;  sass                 0.4.10   2025-04-11 [1] RSPM\n#&gt;  scales               1.4.0    2025-04-24 [1] RSPM (R 4.5.0)\n#&gt;  sessioninfo          1.2.3    2025-02-05 [1] RSPM\n#&gt;  SnowballC            0.7.1    2023-04-25 [1] RSPM (R 4.5.0)\n#&gt;  stopwords            2.3      2021-10-28 [1] RSPM (R 4.5.0)\n#&gt;  stringi              1.8.7    2025-03-27 [1] RSPM\n#&gt;  stringr              1.5.2    2025-09-08 [1] RSPM\n#&gt;  styler               1.10.3   2024-04-07 [1] RSPM\n#&gt;  tibble               3.3.0    2025-06-08 [1] RSPM\n#&gt;  tidyr                1.3.1    2024-01-24 [1] RSPM (R 4.5.0)\n#&gt;  tidyselect           1.2.1    2024-03-11 [1] RSPM (R 4.5.0)\n#&gt;  tidytext             0.4.3    2025-07-25 [1] RSPM (R 4.5.0)\n#&gt;  tokenizers           0.3.0    2022-12-22 [1] RSPM (R 4.5.0)\n#&gt;  utf8                 1.2.6    2025-06-08 [1] RSPM\n#&gt;  V8                   8.0.0    2025-09-27 [1] RSPM (R 4.5.0)\n#&gt;  vctrs                0.6.5    2023-12-01 [1] RSPM\n#&gt;  withr                3.0.2    2024-10-28 [1] RSPM\n#&gt;  xfun                 0.53     2025-08-19 [1] RSPM\n#&gt;  yaml                 2.3.10   2024-07-26 [1] RSPM\n#&gt; \n#&gt;  [1] /usr/local/lib/R/site-library\n#&gt;  [2] /usr/local/lib/R/library\n#&gt;  * ── Packages attached to the search path.\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>抽出語メニュー1</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-2.html",
    "href": "cb-words-menu-2.html",
    "title": "Appendix C — 抽出語メニュー2",
    "section": "",
    "text": "C.1 関連語検索（A.5.6）",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>抽出語メニュー2</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-2.html#関連語検索a.5.6",
    "href": "cb-words-menu-2.html#関連語検索a.5.6",
    "title": "Appendix C — 抽出語メニュー2",
    "section": "",
    "text": "C.1.1 関連語のリスト\n「確率差」や「確率比」については、いちおう計算はできた気がしますが、あっているのかよくわからないです。また、このやり方はそれなりの数の共起について計算をしなければならず、共起行列が大きくなると大変そうなため、あまりおすすめしません。\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    section == \"[1]上_先生と私\",\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\")\n\ndat &lt;- dfm |&gt;\n  quanteda::fcm() |&gt;\n  tidytext::tidy() |&gt;\n  dplyr::rename(target = document, co_occur = count) |&gt;\n  rlang::as_function(~ {\n    col_sums &lt;- quanteda::colSums(dfm)\n    dplyr::reframe(.,\n      term = term,\n      target_occur = col_sums[target],\n      term_occur = col_sums[term],\n      co_occur = co_occur,\n      .by = target\n    )\n  })() |&gt;\n  dplyr::mutate(\n    p_x = target_occur / quanteda::ndoc(dfm),\n    p_y = term_occur / quanteda::ndoc(dfm),\n    p_xy = (co_occur / quanteda::ndoc(dfm)) / p_x,\n    differential = p_xy - p_y, # 確率差\n    lift = p_xy / p_y, # 確率比（リフト）,\n    jaccard = co_occur / (target_occur + term_occur - co_occur),\n    dice = 2 * co_occur / (target_occur + term_occur)\n  ) |&gt;\n  dplyr::select(target, term, differential, lift, jaccard, dice)\n\ndat\n#&gt; # A tibble: 26,820 × 6\n#&gt;    target    term            differential  lift jaccard    dice\n#&gt;    &lt;chr&gt;     &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 先生/名詞 知り合い/名詞       0.00790  2.10  0.0151  0.0297 \n#&gt;  2 先生/名詞 海水浴/名詞         0.000176 1.05  0.00376 0.00749\n#&gt;  3 先生/名詞 帰る/動詞           0.0150   1.20  0.0848  0.156  \n#&gt;  4 先生/名詞 断る/動詞           0.000529 1.05  0.0112  0.0221 \n#&gt;  5 先生/名詞 一人/タグ           0.0129   1.40  0.0443  0.0848 \n#&gt;  6 先生/名詞 大分/地名          -0.00145  0.839 0.00746 0.0148 \n#&gt;  7 先生/名詞 鎌倉/地名           0.00430  1.40  0.0150  0.0295 \n#&gt;  8 先生/名詞 探す/動詞          -0.00162  0.699 0.00375 0.00746\n#&gt;  9 先生/名詞 建てる/動詞         0.000176 1.05  0.00376 0.00749\n#&gt; 10 先生/名詞 通り抜ける/動詞    -0.00342  0.525 0.00373 0.00743\n#&gt; # ℹ 26,810 more rows\n\n\n\nC.1.2 共起ネットワーク\n「先生/名詞」と関連の強そうな語の共起を図示した例です。\n「先生/名詞」と共起している語のうち、出現回数が上位20位以内である語がtargetである共起を抽出したうえで、それらのなかからJaccard係数が大きい順に75個だけ残しています。「先生/名詞」という語そのものは図に含めていません。\n\ndat |&gt;\n  dplyr::inner_join(\n    dplyr::filter(dat, target == \"先生/名詞\") |&gt; dplyr::select(term),\n    by = dplyr::join_by(target == term)\n  ) |&gt;\n  dplyr::filter(target %in% names(quanteda::topfeatures(dfm, 20))) |&gt;\n  dplyr::slice_max(jaccard, n = 75) |&gt;\n  tidygraph::as_tbl_graph(directed = FALSE) |&gt;\n  tidygraph::to_minimum_spanning_tree() |&gt;\n  purrr::pluck(\"mst\") |&gt;\n  dplyr::mutate(\n    community = factor(tidygraph::group_leading_eigen())\n  ) |&gt;\n  ggraph::ggraph(layout = \"fr\") +\n  ggraph::geom_edge_link(aes(width = sqrt(lift), alpha = jaccard)) +\n  ggraph::geom_node_point(aes(colour = community), show.legend = FALSE) +\n  ggraph::geom_node_text(aes(label = name, colour = community), repel = TRUE, show.legend = FALSE) +\n  ggraph::theme_graph()\n\n\n\n\n\n\n\n\n\n\nC.1.3 アソシエーション分析🍳\n英語だとこのメニューの名前は「Word Association」となっているので、ふつうにアソシエーション分析すればいいと思いました。\narulesのtransactionsオブジェクトをつくるには、quantedaのfcmオブジェクトから変換すればOKです（arulesをアタッチしている必要があります）。\n\nlibrary(arules)\nlibrary(arulesViz)\n\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\") |&gt;\n  quanteda::fcm() |&gt;\n  as(\"nMatrix\") |&gt;\n  as(\"transactions\")\n\narules::apriori()でアソシエーションルールを抽出します。\n\nrules &lt;-\n  arules::apriori(\n    dat,\n    parameter = list(\n      support = 0.075,\n      confidence = 0.8,\n      minlen = 2,\n      maxlen = 2, # LHS+RHSの長さ。変えないほうがよい\n      maxtime = 5\n    ),\n    control = list(verbose = FALSE)\n  )\n\nこの形式のオブジェクトはas(rules, \"data.frame\")のようにしてデータフレームに変換できます。tibbleにしたい場合には次のようにすればよいです。\n\nas(rules, \"data.frame\") |&gt;\n  dplyr::mutate(across(where(is.numeric), ~ signif(., digits = 3))) |&gt;\n  tidyr::separate_wider_delim(rules, delim = \" =&gt; \", names = c(\"lhs\", \"rhs\")) |&gt;\n  dplyr::arrange(desc(lift))\n#&gt; # A tibble: 49 × 7\n#&gt;    lhs               rhs         support confidence coverage  lift count\n#&gt;    &lt;chr&gt;             &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 {黙る/動詞}       {聞く/動詞}  0.102       0.829   0.123   2.05   301\n#&gt;  2 {付ける/動詞}     {聞く/動詞}  0.11        0.822   0.133   2.03   323\n#&gt;  3 {過ぎる/動詞}     {自分/名詞}  0.0887      0.906   0.0978  1.84   261\n#&gt;  4 {感じ/名詞}       {自分/名詞}  0.0938      0.879   0.107   1.79   276\n#&gt;  5 {分る/動詞}       {出る/動詞}  0.105       0.812   0.13    1.78   310\n#&gt;  6 {打ち明ける/動詞} {自分/名詞}  0.0887      0.861   0.103   1.75   261\n#&gt;  7 {人間/名詞}       {自分/名詞}  0.17        0.858   0.198   1.75   501\n#&gt;  8 {繰り返す/動詞}   {自分/名詞}  0.0917      0.852   0.108   1.73   270\n#&gt;  9 {信じる/動詞}     {思う/動詞}  0.0781      0.927   0.0842  1.71   230\n#&gt; 10 {疑う/動詞}       {自分/名詞}  0.0856      0.824   0.104   1.68   252\n#&gt; # ℹ 39 more rows\n\n\n\nC.1.4 散布図🍳\n\nplot(rules, engine = \"html\")\n#&gt; To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n\n\n\n\n\n\n\nC.1.5 バルーンプロット🍳\n\nplot(rules, method = \"grouped\", engine = \"html\")\n\n\n\n\n\n\n\nC.1.6 ネットワーク図🍳\n\nplot(rules, method = \"graph\", engine = \"html\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>抽出語メニュー2</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-2.html#対応分析a.5.7",
    "href": "cb-words-menu-2.html#対応分析a.5.7",
    "title": "Appendix C — 抽出語メニュー2",
    "section": "C.2 対応分析（A.5.7）",
    "text": "C.2 対応分析（A.5.7）\n\nC.2.1 コレスポンデンス分析\n段落（doc_id）内の頻度で語彙を削ってから部（section）ごとに集計するために、ややめんどうなことをしています。\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_trim(\n    min_termfreq = 75,\n    termfreq_type = \"rank\",\n    min_docfreq = 30,\n    docfreq_type = \"count\"\n  )\n\nこうしてdoc_idごとに集計したdfmオブジェクトを一度tidytext::tidy()して3つ組のデータフレームに戻し、sectionのラベルを結合します。このデータフレームをもう一度tidytext::cast_dfm()で疎行列に変換して、quanteda.textmodels::textmodel_ca()を使って対応分析にかけます。\n\nca_fit &lt;- dfm |&gt;\n  tidytext::tidy() |&gt;\n  dplyr::left_join(\n    dplyr::select(tbl, doc_id, section),\n    by = dplyr::join_by(document == doc_id)\n  ) |&gt;\n  tidytext::cast_dfm(section, term, count) |&gt;\n  quanteda.textmodels::textmodel_ca(nd = 2, sparse = TRUE)\n\nこの関数は疎行列に対して計算をおこなえるため、比較的大きな行列を渡しても大丈夫そうです。\n\n\nC.2.2 バイプロット\ncaパッケージを読み込んでいるとplot()でバイプロットを描けます。factoextra::fviz_ca_biplot()でも描けますが、見た目はplot()のとあまり変わらないです。\n\nlibrary(ca)\ndat &lt;- plot(ca_fit)\n\n\n\n\n\n\n\n\n\n\nC.2.3 バイプロット（バブルプロット）\nggplot2でバイプロットを描画するには、たとえば次のようにします。ggrepel::geom_text_repel()でラベルを出す語彙の選択の仕方は、もうすこし工夫したほうがよいかもしれないです。\nなお、このコードはCorrespondence Analysis visualization using ggplot | R-bloggersを参考にしました。\n\ntf &lt;- dfm |&gt;\n  tidytext::tidy() |&gt;\n  dplyr::left_join(\n    dplyr::select(tbl, doc_id, section),\n    by = dplyr::join_by(document == doc_id)\n  ) |&gt;\n  dplyr::summarise(tf = sum(count), .by = term) |&gt;\n  dplyr::pull(tf, term)\n\n# modified from https://www.r-bloggers.com/2019/08/correspondence-analysis-visualization-using-ggplot/\nmake_ca_plot_df &lt;- function(ca.plot.obj, row.lab = \"Rows\", col.lab = \"Columns\") {\n  tibble::tibble(\n    Label = c(\n      rownames(ca.plot.obj$rows),\n      rownames(ca.plot.obj$cols)\n    ),\n    Dim1 = c(\n      ca.plot.obj$rows[, 1],\n      ca.plot.obj$cols[, 1]\n    ),\n    Dim2 = c(\n      ca.plot.obj$rows[, 2],\n      ca.plot.obj$cols[, 2]\n    ),\n    Variable = c(\n      rep(row.lab, nrow(ca.plot.obj$rows)),\n      rep(col.lab, nrow(ca.plot.obj$cols))\n    )\n  )\n}\ndat &lt;- dat |&gt;\n  make_ca_plot_df(row.lab = \"Construction\", col.lab = \"Medium\") |&gt;\n  dplyr::mutate(\n    Size = dplyr::if_else(Variable == \"Construction\", mean(tf), tf[Label])\n  )\n# 非ASCII文字のラベルに対してwarningを出さないようにする\nsuppressWarnings({\n  ca_sum &lt;- summary(ca_fit)\n  dim_var_percs &lt;- ca_sum$scree[, \"values2\"]\n})\n\ndat |&gt;\n  ggplot(aes(x = Dim1, y = Dim2, col = Variable, label = Label)) +\n  geom_vline(xintercept = 0, lty = \"dashed\", alpha = .5) +\n  geom_hline(yintercept = 0, lty = \"dashed\", alpha = .5) +\n  geom_jitter(aes(size = Size), alpha = .3, show.legend = FALSE) +\n  ggrepel::geom_label_repel(\n    data = \\(x) dplyr::filter(x, Variable == \"Construction\"),\n    show.legend = FALSE\n  ) +\n  ggrepel::geom_text_repel(\n    data = \\(x) dplyr::filter(x, Variable == \"Medium\", sqrt(Dim1^2 + Dim2^2) &gt; 0.25),\n    show.legend = FALSE\n  ) +\n  scale_x_continuous(\n    limits = range(dat$Dim1) +\n      c(diff(range(dat$Dim1)) * -0.2, diff(range(dat$Dim1)) * 0.2)\n  ) +\n  scale_y_continuous(\n    limits = range(dat$Dim2) +\n      c(diff(range(dat$Dim2)) * -0.2, diff(range(dat$Dim2)) * 0.2)\n  ) +\n  scale_size_area(max_size = 16) +\n  labs(\n    x = paste0(\"Dimension 1 (\", signif(dim_var_percs[1], 3), \"%)\"),\n    y = paste0(\"Dimension 2 (\", signif(dim_var_percs[2], 3), \"%)\")\n  ) +\n  theme_classic()\n#&gt; Warning: ggrepel: 22 unlabeled data points (too many overlaps). Consider\n#&gt; increasing max.overlaps",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>抽出語メニュー2</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-2.html#多次元尺度構成法a.5.8",
    "href": "cb-words-menu-2.html#多次元尺度構成法a.5.8",
    "title": "Appendix C — 抽出語メニュー2",
    "section": "C.3 多次元尺度構成法（A.5.8）",
    "text": "C.3 多次元尺度構成法（A.5.8）\n\nC.3.1 MDS・バブルプロット\nMASS::isoMDS()よりMASS::sammon()のほうがたぶん見やすいです。\n\nsimil &lt;- dfm |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\") |&gt;\n  proxyC::simil(margin = 2, method = \"jaccard\")\n\ndat &lt;- MASS::sammon(1 - simil, k = 2) |&gt;\n  purrr::pluck(\"points\")\n#&gt; Initial stress        : 0.62279\n#&gt; stress after   0 iters: 0.62279\n\n\ndat &lt;- dat |&gt;\n  dplyr::as_tibble(\n    rownames = \"label\",\n    .name_repair = ~ c(\"Dim1\", \"Dim2\")\n  ) |&gt;\n  dplyr::mutate(\n    size = tf[label],\n    clust = (hclust(\n      proxyC::dist(dat, method = \"euclidean\") |&gt; as.dist(),\n      method = \"ward.D2\"\n    ) |&gt; cutree(k = 6))[label]\n  )\n\ndat |&gt;\n  ggplot(aes(x = Dim1, y = Dim2, label = label, col = factor(clust))) +\n  geom_point(aes(size = size), alpha = .3, show.legend = FALSE) +\n  ggrepel::geom_text_repel(show.legend = FALSE) +\n  scale_size_area(max_size = 16) +\n  theme_classic()\n#&gt; Warning: ggrepel: 22 unlabeled data points (too many overlaps). Consider\n#&gt; increasing max.overlaps\n\n\n\n\n\n\n\n\n\n\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)\n\nsessioninfo::session_info(info = \"packages\")\n#&gt; ═ Session info ═══════════════════════════════════════════════════════════════\n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package             * version  date (UTC) lib source\n#&gt;  arules              * 1.7-11   2025-05-29 [1] RSPM (R 4.5.0)\n#&gt;  arulesViz           * 1.5.4    2025-08-21 [1] RSPM (R 4.5.0)\n#&gt;  audubon               0.5.2    2024-04-27 [1] RSPM (R 4.5.0)\n#&gt;  blob                  1.2.4    2023-03-17 [1] RSPM\n#&gt;  ca                  * 0.71.1   2020-01-24 [1] RSPM (R 4.5.0)\n#&gt;  cachem                1.1.0    2024-05-16 [1] RSPM\n#&gt;  cellranger            1.1.0    2016-07-27 [1] RSPM (R 4.5.0)\n#&gt;  cli                   3.6.5    2025-04-23 [1] RSPM\n#&gt;  codetools             0.2-20   2024-03-31 [2] CRAN (R 4.5.1)\n#&gt;  crosstalk             1.2.2    2025-08-26 [1] RSPM (R 4.5.0)\n#&gt;  curl                  7.0.0    2025-08-19 [1] RSPM\n#&gt;  data.table            1.17.8   2025-07-10 [1] RSPM (R 4.5.0)\n#&gt;  DBI                 * 1.2.3    2024-06-02 [1] RSPM (R 4.5.0)\n#&gt;  dbplyr                2.5.1    2025-09-10 [1] RSPM\n#&gt;  digest                0.6.37   2024-08-19 [1] RSPM\n#&gt;  dplyr                 1.1.4    2023-11-17 [1] RSPM (R 4.5.0)\n#&gt;  duckdb              * 1.4.0    2025-09-18 [1] RSPM (R 4.5.0)\n#&gt;  evaluate              1.0.5    2025-08-27 [1] RSPM\n#&gt;  farver                2.1.2    2024-05-13 [1] RSPM (R 4.5.0)\n#&gt;  fastmap               1.2.0    2024-05-15 [1] RSPM\n#&gt;  fastmatch             1.1-6    2024-12-23 [1] RSPM (R 4.5.0)\n#&gt;  foreach               1.5.2    2022-02-02 [1] RSPM (R 4.5.0)\n#&gt;  generics              0.1.4    2025-05-09 [1] RSPM (R 4.5.0)\n#&gt;  ggforce               0.5.0    2025-06-18 [1] RSPM (R 4.5.0)\n#&gt;  ggplot2             * 4.0.0    2025-09-11 [1] RSPM (R 4.5.0)\n#&gt;  ggraph                2.2.2    2025-08-24 [1] RSPM (R 4.5.0)\n#&gt;  ggrepel               0.9.6    2024-09-07 [1] RSPM (R 4.5.0)\n#&gt;  gibasa                1.1.2    2025-02-16 [1] RSPM (R 4.5.0)\n#&gt;  glmnet                4.1-10   2025-07-17 [1] RSPM (R 4.5.0)\n#&gt;  glue                  1.8.0    2024-09-30 [1] RSPM\n#&gt;  graphlayouts          1.2.2    2025-01-23 [1] RSPM (R 4.5.0)\n#&gt;  gridExtra             2.3      2017-09-09 [1] RSPM (R 4.5.0)\n#&gt;  gtable                0.3.6    2024-10-25 [1] RSPM (R 4.5.0)\n#&gt;  htmltools             0.5.8.1  2024-04-04 [1] RSPM\n#&gt;  htmlwidgets           1.6.4    2023-12-06 [1] RSPM\n#&gt;  httr                  1.4.7    2023-08-15 [1] RSPM (R 4.5.0)\n#&gt;  igraph                2.1.4    2025-01-23 [1] RSPM (R 4.5.0)\n#&gt;  iterators             1.0.14   2022-02-05 [1] RSPM (R 4.5.0)\n#&gt;  janeaustenr           1.0.0    2022-08-26 [1] RSPM (R 4.5.0)\n#&gt;  jsonlite              2.0.0    2025-03-27 [1] RSPM\n#&gt;  knitr                 1.50     2025-03-16 [1] RSPM\n#&gt;  labeling              0.4.3    2023-08-29 [1] RSPM (R 4.5.0)\n#&gt;  lattice               0.22-7   2025-04-02 [2] CRAN (R 4.5.1)\n#&gt;  lazyeval              0.2.2    2019-03-15 [1] RSPM\n#&gt;  lifecycle             1.0.4    2023-11-07 [1] RSPM\n#&gt;  magrittr              2.0.4    2025-09-12 [1] RSPM\n#&gt;  MASS                  7.3-65   2025-02-28 [2] CRAN (R 4.5.1)\n#&gt;  Matrix              * 1.7-3    2025-03-11 [2] CRAN (R 4.5.1)\n#&gt;  memoise               2.0.1    2021-11-26 [1] RSPM\n#&gt;  pillar                1.11.1   2025-09-17 [1] RSPM\n#&gt;  pkgconfig             2.0.3    2019-09-22 [1] RSPM\n#&gt;  plotly                4.11.0   2025-06-19 [1] RSPM (R 4.5.0)\n#&gt;  polyclip              1.10-7   2024-07-23 [1] RSPM (R 4.5.0)\n#&gt;  proxyC                0.5.2    2025-04-25 [1] RSPM (R 4.5.0)\n#&gt;  purrr                 1.1.0    2025-07-10 [1] RSPM\n#&gt;  quanteda              4.3.1    2025-07-10 [1] RSPM (R 4.5.0)\n#&gt;  quanteda.textmodels   0.9.10   2025-02-10 [1] RSPM (R 4.5.0)\n#&gt;  R.cache               0.17.0   2025-05-02 [1] RSPM\n#&gt;  R.methodsS3           1.8.2    2022-06-13 [1] RSPM\n#&gt;  R.oo                  1.27.1   2025-05-02 [1] RSPM\n#&gt;  R.utils               2.13.0   2025-02-24 [1] RSPM\n#&gt;  R6                    2.6.1    2025-02-15 [1] RSPM\n#&gt;  RColorBrewer          1.1-3    2022-04-03 [1] RSPM (R 4.5.0)\n#&gt;  Rcpp                  1.1.0    2025-07-02 [1] RSPM\n#&gt;  RcppParallel          5.1.11-1 2025-08-27 [1] RSPM (R 4.5.0)\n#&gt;  readxl                1.4.5    2025-03-07 [1] RSPM (R 4.5.0)\n#&gt;  registry              0.5-1    2019-03-05 [1] RSPM (R 4.5.0)\n#&gt;  rlang                 1.1.6    2025-04-11 [1] RSPM\n#&gt;  rmarkdown             2.30     2025-09-28 [1] RSPM (R 4.5.0)\n#&gt;  RSpectra              0.16-2   2024-07-18 [1] RSPM (R 4.5.0)\n#&gt;  S7                    0.2.0    2024-11-07 [1] RSPM (R 4.5.0)\n#&gt;  scales                1.4.0    2025-04-24 [1] RSPM (R 4.5.0)\n#&gt;  seriation             1.5.8    2025-08-20 [1] RSPM (R 4.5.0)\n#&gt;  sessioninfo           1.2.3    2025-02-05 [1] RSPM\n#&gt;  shape                 1.4.6.1  2024-02-23 [1] RSPM (R 4.5.0)\n#&gt;  SnowballC             0.7.1    2023-04-25 [1] RSPM (R 4.5.0)\n#&gt;  stopwords             2.3      2021-10-28 [1] RSPM (R 4.5.0)\n#&gt;  stringi               1.8.7    2025-03-27 [1] RSPM\n#&gt;  stringr               1.5.2    2025-09-08 [1] RSPM\n#&gt;  styler                1.10.3   2024-04-07 [1] RSPM\n#&gt;  survival              3.8-3    2024-12-17 [2] CRAN (R 4.5.1)\n#&gt;  tibble                3.3.0    2025-06-08 [1] RSPM\n#&gt;  tidygraph             1.3.1    2024-01-30 [1] RSPM (R 4.5.0)\n#&gt;  tidyr                 1.3.1    2024-01-24 [1] RSPM (R 4.5.0)\n#&gt;  tidyselect            1.2.1    2024-03-11 [1] RSPM (R 4.5.0)\n#&gt;  tidytext              0.4.3    2025-07-25 [1] RSPM (R 4.5.0)\n#&gt;  tokenizers            0.3.0    2022-12-22 [1] RSPM (R 4.5.0)\n#&gt;  TSP                   1.2-5    2025-05-27 [1] RSPM (R 4.5.0)\n#&gt;  tweenr                2.0.3    2024-02-26 [1] RSPM (R 4.5.0)\n#&gt;  utf8                  1.2.6    2025-06-08 [1] RSPM\n#&gt;  V8                    8.0.0    2025-09-27 [1] RSPM (R 4.5.0)\n#&gt;  vctrs                 0.6.5    2023-12-01 [1] RSPM\n#&gt;  viridis               0.6.5    2024-01-29 [1] RSPM (R 4.5.0)\n#&gt;  viridisLite           0.4.2    2023-05-02 [1] RSPM (R 4.5.0)\n#&gt;  visNetwork            2.1.4    2025-09-04 [1] RSPM (R 4.5.0)\n#&gt;  withr                 3.0.2    2024-10-28 [1] RSPM\n#&gt;  xfun                  0.53     2025-08-19 [1] RSPM\n#&gt;  yaml                  2.3.10   2024-07-26 [1] RSPM\n#&gt; \n#&gt;  [1] /usr/local/lib/R/site-library\n#&gt;  [2] /usr/local/lib/R/library\n#&gt;  * ── Packages attached to the search path.\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>抽出語メニュー2</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-3.html",
    "href": "cb-words-menu-3.html",
    "title": "Appendix D — 抽出語メニュー3",
    "section": "",
    "text": "D.1 階層的クラスター分析（A.5.9）",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>抽出語メニュー3</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-3.html#階層的クラスター分析a.5.9",
    "href": "cb-words-menu-3.html#階層的クラスター分析a.5.9",
    "title": "Appendix D — 抽出語メニュー3",
    "section": "",
    "text": "D.1.1 非類似度のヒートマップ🍳\nJaccard係数を指定して非類似度のヒートマップを描くと、そもそもパターンがほとんど見えませんでした……。\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n)\n\ndat &lt;- dfm |&gt;\n  quanteda::dfm_trim(min_termfreq = 30, termfreq_type = \"rank\") |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\") |&gt;\n  proxyC::simil(margin = 2, method = \"dice\") |&gt;\n  rlang::as_function(~ 1 - .)()\n\nfactoextra::fviz_dist(as.dist(dat))\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the factoextra package.\n#&gt;   Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\n\n\nD.1.2 階層的クラスタリング\n\nclusters &lt;-\n  as.dist(dat) |&gt;\n  hclust(method = \"ward.D2\")\n\n\n\nD.1.3 シルエット分析🍳\n\nfactoextra::fviz_nbclust(\n  as.matrix(dat),\n  FUNcluster = factoextra::hcut,\n  k.max = ceiling(sqrt(nrow(dat)))\n)\n\n\n\n\n\n\n\n\n\ncluster::silhouette(cutree(clusters, k = 5), dist = dat) |&gt;\n  factoextra::fviz_silhouette(print.summary = FALSE) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nD.1.4 デンドログラム\nデンドログラムについては、似たような表現を手軽に実現できる方法が見つけられません。ラベルの位置が左右反転していますが、factoextra::fviz_dend(horiz = TRUE)とするのが簡単かもしれないです。\n\nfactoextra::fviz_dend(clusters, k = 5, horiz = TRUE, labels_track_height = 0.3)\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n#&gt; ℹ The deprecated feature was likely used in the factoextra package.\n#&gt;   Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n#&gt; Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n#&gt; of ggplot2 3.3.4.\n#&gt; ℹ The deprecated feature was likely used in the factoextra package.\n#&gt;   Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\n\n\nD.1.5 デンドログラムと棒グラフ\nKH Coderのソースコードを見た感じ、デンドログラムと一緒に語の出現回数を描いている表現は、やや独特なことをしています。むしろ語の出現回数のほうが主な情報になってよいなら、ふつうの棒グラフの横にlegendry::scale_y_dendro()でデンドログラムを描くことができます。\n\ndfm |&gt;\n  quanteda::dfm_trim(min_termfreq = 30, termfreq_type = \"rank\") |&gt;\n  quanteda::colSums() |&gt;\n  tibble::enframe() |&gt;\n  dplyr::mutate(\n    clust = (clusters |&gt; cutree(k = 5))[name]\n  ) |&gt;\n  ggplot(aes(x = value, y = name, fill = factor(clust))) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  scale_x_sqrt() +\n  legendry::scale_y_dendro(clust = clusters) +\n  labs(x = \"出現回数\", y = element_blank()) +\n  theme_bw()\n#&gt; Warning: `label` cannot be a &lt;ggplot2::element_blank&gt; object.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>抽出語メニュー3</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-3.html#共起ネットワークa.5.10",
    "href": "cb-words-menu-3.html#共起ネットワークa.5.10",
    "title": "Appendix D — 抽出語メニュー3",
    "section": "D.2 共起ネットワーク（A.5.10）",
    "text": "D.2 共起ネットワーク（A.5.10）\n\nD.2.1 グラフの作成\n描画するグラフをtbl_graphとして作成します。\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n)\n\ndat &lt;- dfm |&gt;\n  quanteda::dfm_trim(min_termfreq = 45, termfreq_type = \"count\") |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\") |&gt;\n  proxyC::simil(margin = 2, method = \"jaccard\", rank = 3) |&gt;\n  as.matrix() |&gt;\n  tidygraph::as_tbl_graph(directed = FALSE) |&gt;\n  dplyr::distinct() |&gt; # 重複を削除\n  tidygraph::activate(edges) |&gt;\n  dplyr::filter(from != to)\n\ndat\n#&gt; # A tbl_graph: 47 nodes and 82 edges\n#&gt; #\n#&gt; # An undirected simple graph with 2 components\n#&gt; #\n#&gt; # Edge Data: 82 × 3 (active)\n#&gt;     from    to weight\n#&gt;    &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n#&gt;  1     1    11 0.137 \n#&gt;  2     1    17 0.139 \n#&gt;  3     2     4 0.120 \n#&gt;  4     2    25 0.125 \n#&gt;  5     3    10 0.0957\n#&gt;  6     3    25 0.106 \n#&gt;  7     3    30 0.0909\n#&gt;  8     4    21 0.120 \n#&gt;  9     5    44 0.171 \n#&gt; 10     5    46 0.180 \n#&gt; # ℹ 72 more rows\n#&gt; #\n#&gt; # Node Data: 47 × 1\n#&gt;   name     \n#&gt;   &lt;chr&gt;    \n#&gt; 1 先生/名詞\n#&gt; 2 帰る/動詞\n#&gt; 3 一人/タグ\n#&gt; # ℹ 44 more rows\n\n\n\nD.2.2 相関係数の計算\nggraph::geom_edge_link2()のalphaに渡す相関係数を計算します。このあたりのコードは書くのが難しかったので、あまりスマートなやり方ではないかもしれません。\nKH Coderには、それぞれの共起が文書集合内のどのあたりの位置に出現したかを概観できるようにするために、共起ネットワーク中のエッジについて、共起の出現位置との相関係数によって塗り分ける機能があります。これを実現するには、まずそれぞれの文書について文書集合内での通し番号を振ったうえで、それぞれの文書についてエッジとして描きたい共起の有無を1, 0で表してから、通し番号とのあいだの相関係数を計算します。\nまず、共起ネットワーク中に描きこむ共起と、それらを含む文書番号をリストアップした縦長のデータフレームをつくります。\n\nnodes &lt;- tidygraph::activate(dat, nodes) |&gt; dplyr::pull(\"name\")\nfrom &lt;- nodes[tidygraph::activate(dat, edges) |&gt; dplyr::pull(\"from\")]\nto &lt;- nodes[tidygraph::activate(dat, edges) |&gt; dplyr::pull(\"to\")]\n\nhas_coocurrences &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::filter(token %in% nodes) |&gt;\n  dplyr::collect() |&gt;\n  dplyr::reframe(\n    from = from,\n    to = to,\n    has_from = purrr::map_lgl(from, ~ . %in% token),\n    has_to = purrr::map_lgl(to, ~ . %in% token),\n    .by = doc_id\n  ) |&gt;\n  dplyr::filter(has_from & has_to) |&gt;\n  dplyr::group_by(from, to) |&gt;\n  dplyr::reframe(doc_id = doc_id)\n\nhas_coocurrences\n#&gt; # A tibble: 2,164 × 3\n#&gt;    from          to       doc_id\n#&gt;    &lt;chr&gt;         &lt;chr&gt;     &lt;int&gt;\n#&gt;  1 お嬢さん/名詞 K/未知語   1034\n#&gt;  2 お嬢さん/名詞 K/未知語   1035\n#&gt;  3 お嬢さん/名詞 K/未知語   1041\n#&gt;  4 お嬢さん/名詞 K/未知語   1042\n#&gt;  5 お嬢さん/名詞 K/未知語   1045\n#&gt;  6 お嬢さん/名詞 K/未知語   1046\n#&gt;  7 お嬢さん/名詞 K/未知語   1048\n#&gt;  8 お嬢さん/名詞 K/未知語   1049\n#&gt;  9 お嬢さん/名詞 K/未知語   1052\n#&gt; 10 お嬢さん/名詞 K/未知語   1054\n#&gt; # ℹ 2,154 more rows\n\n次に、このデータフレームを共起ごとにグルーピングして、共起の有無と通し番号とのあいだの相関係数を含むデータフレームをつくります。\n\ncorrelations &lt;- has_coocurrences |&gt;\n  dplyr::group_by(from, to) |&gt;\n  dplyr::group_map(\\(.x, .y) {\n    tibble::tibble(\n      doc_number = seq_len(nrow(tbl)),\n      from = which(nodes == .y$from),\n      to = which(nodes == .y$to)\n    ) |&gt;\n      dplyr::group_by(from, to) |&gt;\n      dplyr::summarise(\n        cor = cor(doc_number, as.numeric(doc_number %in% .x[[\"doc_id\"]])),\n        .groups = \"drop\"\n      )\n  }) |&gt;\n  purrr::list_rbind()\n\ncorrelations\n#&gt; # A tibble: 82 × 3\n#&gt;     from    to     cor\n#&gt;    &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt;  1    44    46  0.292 \n#&gt;  2    44    45  0.139 \n#&gt;  3     3    25  0.104 \n#&gt;  4     3    30  0.0467\n#&gt;  5     3    10  0.145 \n#&gt;  6     5    46  0.246 \n#&gt;  7     5    44  0.199 \n#&gt;  8    29    44  0.158 \n#&gt;  9    29    36  0.104 \n#&gt; 10     1    17 -0.176 \n#&gt; # ℹ 72 more rows\n\n最後に、相関係数をtbl_graphのエッジと結合します。\n\ndat &lt;- dat |&gt;\n  tidygraph::activate(edges) |&gt;\n  dplyr::left_join(correlations, by = dplyr::join_by(from == from, to == to))\n\n\n\nD.2.3 共起ネットワーク\n上の処理が間違っていなければ、文書集合の後のほうによく出てくる共起であるほど、エッジの色が濃くなっているはずです。\n\ndat |&gt;\n  tidygraph::activate(nodes) |&gt;\n  dplyr::mutate(\n    community = factor(tidygraph::group_leading_eigen())\n  ) |&gt;\n  ggraph::ggraph(layout = \"fr\") +\n  ggraph::geom_edge_link2(\n    aes(\n      alpha = dplyr::percent_rank(cor) + .01, # パーセンタイルが0だと透明になってしまうので、適当に下駄をはかせる\n      width = dplyr::percent_rank(weight) + 1\n    ),\n    colour = \"red\"\n  ) +\n  ggraph::geom_node_point(aes(colour = community), show.legend = FALSE) +\n  ggraph::geom_node_label(aes(colour = community, label = name), repel = TRUE, show.legend = FALSE) +\n  ggraph::theme_graph()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>抽出語メニュー3</span>"
    ]
  },
  {
    "objectID": "cb-words-menu-3.html#自己組織化マップa.5.11",
    "href": "cb-words-menu-3.html#自己組織化マップa.5.11",
    "title": "Appendix D — 抽出語メニュー3",
    "section": "D.3 自己組織化マップ（A.5.11）",
    "text": "D.3 自己組織化マップ（A.5.11）\n\nD.3.1 自己組織化マップ（SOM）\nSOMの実装としては、KH Coderはsomを使っているようですが、kohonenを使ったほうがよいです。\n行列が非常に大きい場合にはkohonen::som(mode = \"online\")としてもよいでしょうが、一般にバッチ型のほうが収束が早く、数十ステップ程度回せば十分とされます。\n与える単語文書行列は、ここではtidytext::bind_tf_idf()を使ってTF-IDFで重みづけし、上位100語ほど抽出します。\n\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::bind_tf_idf(token, doc_id, n) |&gt;\n  tidytext::cast_dfm(doc_id, token, tf_idf) |&gt;\n  quanteda::dfm_trim(\n    min_termfreq = 100,\n    termfreq_type = \"rank\"\n  ) |&gt;\n  as.matrix() |&gt;\n  scale() |&gt;\n  t()\n\nsom_fit &lt;-\n  kohonen::som(\n    dat,\n    grid = kohonen::somgrid(20, 16, \"hexagonal\"),\n    rlen = 50, # 学習回数\n    alpha = c(0.05, 0.01),\n    radius = 8,\n    dist.fcts = \"sumofsquares\",\n    mode = \"batch\",\n    init = aweSOM::somInit(dat, 20, 16)\n  )\n\n\naweSOM::somQuality(som_fit, dat)\n#&gt; \n#&gt; ## Quality measures:\n#&gt;  * Quantization error     :  66.18159 \n#&gt;  * (% explained variance) :  94.1 \n#&gt;  * Topographic error      :  0.38 \n#&gt;  * Kaski-Lagus error      :  23.57054 \n#&gt;  \n#&gt; ## Number of obs. per map cell:\n#&gt;   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n#&gt;   1   0   0   1   0   0   1   1   0   0   1   0   0   1   0   0   1   1   0   2 \n#&gt;  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n#&gt;   1   0   1   0   0   0   0   0   0   0   1   0   0   1   1   0   0   0   0   0 \n#&gt;  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n#&gt;   0   0   0   1   1   0   0   1   0   0   0   0   0   0   0   1   0   0   0   1 \n#&gt;  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n#&gt;   0   0   0   0   0   0   1   1   0   0   0   0   2   1   0   0   0   0   1   0 \n#&gt;  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n#&gt;   1   0   0   0   1   0   0   0   0   0   1   0   0   0   0   1   0   0   1   1 \n#&gt; 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n#&gt;   0   0   2   0   0   0   0   0   0   0   0   1   1   0   0   2   0   0   0   0 \n#&gt; 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n#&gt;   0   0   0   0   0   1   0   1   1   0   0   0   0   0   0   1   1   1   0   0 \n#&gt; 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n#&gt;   1   0   0   1   0   0   0   1   0   1   0   0   0   1   0   0   0   1   1   2 \n#&gt; 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n#&gt;   0   0   1   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n#&gt; 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n#&gt;   1   0   0   0   0   0   0   0   1   0   0   1   0   0   0   1   0   0   0   0 \n#&gt; 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n#&gt;   0   1   0   0   0   0   0   1   0   0   0   0   0   1   0   0   1   0   1   0 \n#&gt; 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n#&gt;   1   0   1   1   0   2   0   0   1   1   1   0   0   1   1   0   0   0   1   0 \n#&gt; 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n#&gt;   0   0   1   0   1   1   0   0   0   1   0   0   1   1   0   0   1   0   0   1 \n#&gt; 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n#&gt;   1   0   0   0   0   0   0   0   0   0   0   0   1   0   1   0   0   0   0   0 \n#&gt; 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n#&gt;   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0 \n#&gt; 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n#&gt;   1   0   1   1   0   0   0   0   0   1   1   0   0   1   1   0   1   1   0   1\n\n\n\nD.3.2 U-Matrix\nU-matrixは「各ノードの参照ベクトルが近傍ノードと異なる度合いで色づけする方法」（自己組織化マップ入門）です。暖色の箇所はデータ密度が低い「山間部」で、寒色の箇所はデータ密度が高い「平野部」みたいなイメージ、写像の勾配が急峻になっている箇所を境にしてクラスタが分かれていると判断するみたいな見方をします。\n\naweSOM::aweSOMsmoothdist(som_fit)\n\n\n\n\n\n\n\n\n\naweSOM::aweSOMplot(\n  som_fit,\n  data = dat,\n  type = \"UMatrix\"\n)\n\n\n\n\n\n\n\n\n\n\n\nD.3.3 ヒットマップ🍳\n色を付けるためのクラスタリングをしておきます。一部の「山間部」や「盆地」がクラスタになって、後はその他の部分みたいな感じに分かれるようですが、解釈するのに便利な感じで分かれてはくれなかったりします。\n\nclusters &lt;- som_fit |&gt;\n  purrr::pluck(\"codes\", 1) |&gt; # 参照ベクトル（codebook vectors）は`codes`にリストとして格納されている\n  dist() |&gt;\n  hclust(method = \"ward.D2\") |&gt;\n  cutree(k = 10)\n\nヒットマップ（hitmap, proportion map）は以下のような可視化の方法です。ノードの中の六角形は各ノードが保持する参照ベクトルの数（比率）を表しています。ノードの背景色が上のコードで得たクラスタに対応します。\n\naweSOM::aweSOMplot(\n  som_fit,\n  data = dat,\n  type = \"Hitmap\",\n  superclass = clusters\n)\n\n\n\n\n\n\n\n\n\n\n\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)\n\nsessioninfo::session_info(info = \"packages\")\n#&gt; ═ Session info ═══════════════════════════════════════════════════════════════\n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package      * version  date (UTC) lib source\n#&gt;  abind          1.4-8    2024-09-12 [1] RSPM (R 4.5.0)\n#&gt;  audubon        0.5.2    2024-04-27 [1] RSPM (R 4.5.0)\n#&gt;  aweSOM         1.3      2022-08-30 [1] RSPM (R 4.5.0)\n#&gt;  backports      1.5.0    2024-05-23 [1] RSPM\n#&gt;  blob           1.2.4    2023-03-17 [1] RSPM\n#&gt;  broom          1.0.10   2025-09-13 [1] RSPM (R 4.5.0)\n#&gt;  cachem         1.1.0    2024-05-16 [1] RSPM\n#&gt;  car            3.1-3    2024-09-27 [1] RSPM (R 4.5.0)\n#&gt;  carData        3.0-5    2022-01-06 [1] RSPM (R 4.5.0)\n#&gt;  cellranger     1.1.0    2016-07-27 [1] RSPM (R 4.5.0)\n#&gt;  class          7.3-23   2025-01-01 [2] CRAN (R 4.5.1)\n#&gt;  cli            3.6.5    2025-04-23 [1] RSPM\n#&gt;  cluster        2.1.8.1  2025-03-12 [2] CRAN (R 4.5.1)\n#&gt;  codetools      0.2-20   2024-03-31 [2] CRAN (R 4.5.1)\n#&gt;  curl           7.0.0    2025-08-19 [1] RSPM\n#&gt;  DBI          * 1.2.3    2024-06-02 [1] RSPM (R 4.5.0)\n#&gt;  dbplyr         2.5.1    2025-09-10 [1] RSPM\n#&gt;  dendextend     1.19.1   2025-07-15 [1] RSPM (R 4.5.0)\n#&gt;  digest         0.6.37   2024-08-19 [1] RSPM\n#&gt;  dotCall64      1.2      2024-10-04 [1] RSPM (R 4.5.0)\n#&gt;  dplyr          1.1.4    2023-11-17 [1] RSPM (R 4.5.0)\n#&gt;  duckdb       * 1.4.0    2025-09-18 [1] RSPM (R 4.5.0)\n#&gt;  e1071          1.7-16   2024-09-16 [1] RSPM (R 4.5.0)\n#&gt;  evaluate       1.0.5    2025-08-27 [1] RSPM\n#&gt;  factoextra     1.0.7    2020-04-01 [1] RSPM (R 4.5.0)\n#&gt;  farver         2.1.2    2024-05-13 [1] RSPM (R 4.5.0)\n#&gt;  fastmap        1.2.0    2024-05-15 [1] RSPM\n#&gt;  fastmatch      1.1-6    2024-12-23 [1] RSPM (R 4.5.0)\n#&gt;  fields         17.1     2025-09-08 [1] RSPM (R 4.5.0)\n#&gt;  Formula        1.2-5    2023-02-24 [1] RSPM (R 4.5.0)\n#&gt;  generics       0.1.4    2025-05-09 [1] RSPM (R 4.5.0)\n#&gt;  ggforce        0.5.0    2025-06-18 [1] RSPM (R 4.5.0)\n#&gt;  ggplot2      * 4.0.0    2025-09-11 [1] RSPM (R 4.5.0)\n#&gt;  ggpubr         0.6.1    2025-06-27 [1] RSPM (R 4.5.0)\n#&gt;  ggraph         2.2.2    2025-08-24 [1] RSPM (R 4.5.0)\n#&gt;  ggrepel        0.9.6    2024-09-07 [1] RSPM (R 4.5.0)\n#&gt;  ggsignif       0.6.4    2022-10-13 [1] RSPM (R 4.5.0)\n#&gt;  gibasa         1.1.2    2025-02-16 [1] RSPM (R 4.5.0)\n#&gt;  glue           1.8.0    2024-09-30 [1] RSPM\n#&gt;  graphlayouts   1.2.2    2025-01-23 [1] RSPM (R 4.5.0)\n#&gt;  gridExtra      2.3      2017-09-09 [1] RSPM (R 4.5.0)\n#&gt;  gtable         0.3.6    2024-10-25 [1] RSPM (R 4.5.0)\n#&gt;  htmltools      0.5.8.1  2024-04-04 [1] RSPM\n#&gt;  htmlwidgets    1.6.4    2023-12-06 [1] RSPM\n#&gt;  httpuv         1.6.16   2025-04-16 [1] RSPM\n#&gt;  igraph         2.1.4    2025-01-23 [1] RSPM (R 4.5.0)\n#&gt;  isoband        0.2.7    2022-12-20 [1] RSPM (R 4.5.0)\n#&gt;  janeaustenr    1.0.0    2022-08-26 [1] RSPM (R 4.5.0)\n#&gt;  jsonlite       2.0.0    2025-03-27 [1] RSPM\n#&gt;  knitr          1.50     2025-03-16 [1] RSPM\n#&gt;  kohonen        3.0.12   2023-06-09 [1] RSPM (R 4.5.0)\n#&gt;  labeling       0.4.3    2023-08-29 [1] RSPM (R 4.5.0)\n#&gt;  later          1.4.4    2025-08-27 [1] RSPM\n#&gt;  lattice        0.22-7   2025-04-02 [2] CRAN (R 4.5.1)\n#&gt;  legendry       0.2.4    2025-09-14 [1] RSPM (R 4.5.0)\n#&gt;  lifecycle      1.0.4    2023-11-07 [1] RSPM\n#&gt;  magrittr       2.0.4    2025-09-12 [1] RSPM\n#&gt;  maps           3.4.3    2025-05-26 [1] RSPM (R 4.5.0)\n#&gt;  MASS           7.3-65   2025-02-28 [2] CRAN (R 4.5.1)\n#&gt;  Matrix         1.7-3    2025-03-11 [2] CRAN (R 4.5.1)\n#&gt;  memoise        2.0.1    2021-11-26 [1] RSPM\n#&gt;  mime           0.13     2025-03-17 [1] RSPM\n#&gt;  pillar         1.11.1   2025-09-17 [1] RSPM\n#&gt;  pkgconfig      2.0.3    2019-09-22 [1] RSPM\n#&gt;  plyr           1.8.9    2023-10-02 [1] RSPM (R 4.5.0)\n#&gt;  polyclip       1.10-7   2024-07-23 [1] RSPM (R 4.5.0)\n#&gt;  promises       1.3.3    2025-05-29 [1] RSPM\n#&gt;  proxy          0.4-27   2022-06-09 [1] RSPM (R 4.5.0)\n#&gt;  proxyC         0.5.2    2025-04-25 [1] RSPM (R 4.5.0)\n#&gt;  purrr          1.1.0    2025-07-10 [1] RSPM\n#&gt;  quanteda       4.3.1    2025-07-10 [1] RSPM (R 4.5.0)\n#&gt;  R.cache        0.17.0   2025-05-02 [1] RSPM\n#&gt;  R.methodsS3    1.8.2    2022-06-13 [1] RSPM\n#&gt;  R.oo           1.27.1   2025-05-02 [1] RSPM\n#&gt;  R.utils        2.13.0   2025-02-24 [1] RSPM\n#&gt;  R6             2.6.1    2025-02-15 [1] RSPM\n#&gt;  RColorBrewer   1.1-3    2022-04-03 [1] RSPM (R 4.5.0)\n#&gt;  Rcpp           1.1.0    2025-07-02 [1] RSPM\n#&gt;  RcppParallel   5.1.11-1 2025-08-27 [1] RSPM (R 4.5.0)\n#&gt;  readxl         1.4.5    2025-03-07 [1] RSPM (R 4.5.0)\n#&gt;  reshape2       1.4.4    2020-04-09 [1] RSPM (R 4.5.0)\n#&gt;  rlang          1.1.6    2025-04-11 [1] RSPM\n#&gt;  rmarkdown      2.30     2025-09-28 [1] RSPM (R 4.5.0)\n#&gt;  rstatix        0.7.2    2023-02-01 [1] RSPM (R 4.5.0)\n#&gt;  S7             0.2.0    2024-11-07 [1] RSPM (R 4.5.0)\n#&gt;  scales         1.4.0    2025-04-24 [1] RSPM (R 4.5.0)\n#&gt;  sessioninfo    1.2.3    2025-02-05 [1] RSPM\n#&gt;  shiny          1.11.1   2025-07-03 [1] RSPM\n#&gt;  SnowballC      0.7.1    2023-04-25 [1] RSPM (R 4.5.0)\n#&gt;  spam           2.11-1   2025-01-20 [1] RSPM (R 4.5.0)\n#&gt;  stopwords      2.3      2021-10-28 [1] RSPM (R 4.5.0)\n#&gt;  stringi        1.8.7    2025-03-27 [1] RSPM\n#&gt;  stringr        1.5.2    2025-09-08 [1] RSPM\n#&gt;  styler         1.10.3   2024-04-07 [1] RSPM\n#&gt;  tibble         3.3.0    2025-06-08 [1] RSPM\n#&gt;  tidygraph      1.3.1    2024-01-30 [1] RSPM (R 4.5.0)\n#&gt;  tidyr          1.3.1    2024-01-24 [1] RSPM (R 4.5.0)\n#&gt;  tidyselect     1.2.1    2024-03-11 [1] RSPM (R 4.5.0)\n#&gt;  tidytext       0.4.3    2025-07-25 [1] RSPM (R 4.5.0)\n#&gt;  tokenizers     0.3.0    2022-12-22 [1] RSPM (R 4.5.0)\n#&gt;  tweenr         2.0.3    2024-02-26 [1] RSPM (R 4.5.0)\n#&gt;  utf8           1.2.6    2025-06-08 [1] RSPM\n#&gt;  V8             8.0.0    2025-09-27 [1] RSPM (R 4.5.0)\n#&gt;  vctrs          0.6.5    2023-12-01 [1] RSPM\n#&gt;  viridis        0.6.5    2024-01-29 [1] RSPM (R 4.5.0)\n#&gt;  viridisLite    0.4.2    2023-05-02 [1] RSPM (R 4.5.0)\n#&gt;  withr          3.0.2    2024-10-28 [1] RSPM\n#&gt;  xfun           0.53     2025-08-19 [1] RSPM\n#&gt;  xtable         1.8-4    2019-04-21 [1] RSPM\n#&gt;  yaml           2.3.10   2024-07-26 [1] RSPM\n#&gt; \n#&gt;  [1] /usr/local/lib/R/site-library\n#&gt;  [2] /usr/local/lib/R/library\n#&gt;  * ── Packages attached to the search path.\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>抽出語メニュー3</span>"
    ]
  },
  {
    "objectID": "cb-docs-menu.html",
    "href": "cb-docs-menu.html",
    "title": "Appendix E — 文書メニュー",
    "section": "",
    "text": "E.1 文書検索（A.6.1）",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>文書メニュー</span>"
    ]
  },
  {
    "objectID": "cb-docs-menu.html#文書検索a.6.1",
    "href": "cb-docs-menu.html#文書検索a.6.1",
    "title": "Appendix E — 文書メニュー",
    "section": "",
    "text": "E.1.1 TF-IDF\nKWICの結果を検索語のTF-IDFの降順で並び替える例です。\n\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(section == \"[1]上_先生と私\") |&gt;\n  dplyr::select(label, token) |&gt;\n  dplyr::collect()\n\ndat |&gt;\n  dplyr::reframe(token = list(token), .by = label) |&gt;\n  tibble::deframe() |&gt;\n  quanteda::as.tokens() |&gt;\n  quanteda::tokens_select(\"[[:punct:]]\", selection = \"remove\", valuetype = \"regex\", padding = FALSE) |&gt;\n  quanteda::kwic(pattern = \"^向[いくけこ]$\", window = 5, valuetype = \"regex\") |&gt;\n  dplyr::as_tibble() |&gt;\n  dplyr::select(docname, pre, keyword, post) |&gt;\n  dplyr::left_join(\n    dat |&gt;\n      dplyr::count(label, token) |&gt;\n      tidytext::bind_tf_idf(token, label, n),\n    by = dplyr::join_by(docname == label, keyword == token)\n  ) |&gt;\n  dplyr::arrange(desc(tf_idf))\n#&gt; # A tibble: 18 × 8\n#&gt;    docname    pre                      keyword post      n      tf   idf  tf_idf\n#&gt;    &lt;chr&gt;      &lt;chr&gt;                    &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 上・三十四 いっ た なり 下 を       向い    た 私 …     2 0.00251  1.19 0.00298\n#&gt;  2 上・三十四 突然 奥さん の 方 を     向い    た 静 …     2 0.00251  1.19 0.00298\n#&gt;  3 上・八     は 私 の 方 を           向い    て いっ…     2 0.00239  1.19 0.00283\n#&gt;  4 上・八     また 私 の 方 を         向い    た 子供…     2 0.00239  1.19 0.00283\n#&gt;  5 上・二     て 海 の 方 を           向い    て 立っ…     2 0.00217  1.19 0.00257\n#&gt;  6 上・二     まで 沖 の 方 へ         向い    て 行っ…     2 0.00217  1.19 0.00257\n#&gt;  7 上・十二   な 方角 へ 足 を         向け    た それ…     1 0.00117  2.20 0.00257\n#&gt;  8 上・十六   の 眼 を 私 に           向け    た そう…     1 0.00117  2.20 0.00256\n#&gt;  9 上・三十三 先生 は ちょっと 顔 だけ 向け    直し て…     1 0.00112  2.20 0.00246\n#&gt; 10 上・二十五 た 世間 に 背中 を       向け    た 人 …     1 0.00107  2.20 0.00236\n#&gt; 11 上・十七   う 世の中 の どっち を   向い    て も …     1 0.00122  1.19 0.00145\n#&gt; 12 上・十二   花 より も そちら を     向い    て 眼 …     1 0.00117  1.19 0.00139\n#&gt; 13 上・七     は 外 の 方 を           向い    て 今 …     1 0.00116  1.19 0.00138\n#&gt; 14 上・十四   に 庭 の 方 を           向い    た その…     1 0.00115  1.19 0.00137\n#&gt; 15 上・三十三 は 庭 の 方 を           向い    て 澄ま…     1 0.00112  1.19 0.00133\n#&gt; 16 上・三十五 は 庭 の 方 を           向い    て 笑っ…     1 0.00110  1.19 0.00131\n#&gt; 17 上・十五   は また 奥さん と 差し   向い    で 話 …     1 0.00109  1.19 0.00129\n#&gt; 18 上・二十六 を し て よそ を         向い    て 歩い…     1 0.00106  1.19 0.00126\n\n\n\nE.1.2 LexRank🍳\nLexRankは、TF-IDFで重みづけした文書間の類似度行列についてページランクを計算することで、文書集合のなかから「重要な文書」を抽出する手法です。\n\ndat &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  )\n\ndfm &lt;- dat |&gt;\n  dplyr::count(label, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::bind_tf_idf(token, label, n) |&gt;\n  dplyr::inner_join(\n    dat |&gt;\n      dplyr::select(doc_id, label, token) |&gt;\n      dplyr::collect(),\n    by = dplyr::join_by(label == label, token == token)\n  ) |&gt;\n  tidytext::cast_dfm(doc_id, token, tf_idf)\n\n文書間のコサイン類似度を得て、PageRankを計算します。quanteda.textstats::textstat_simil()はproxyC::simil()と処理としては同じですが、戻り値がtextstat_simil_symm_sparseというS4クラスのオブジェクトになっていて、as.data.frame()で縦長のデータフレームに変換できます。\n\nscores &lt;- dfm |&gt;\n  quanteda.textstats::textstat_simil(\n    margin = \"documents\",\n    method = \"cosine\",\n    min_simil = .6 # LexRankの文脈でいうところのthreshold\n  ) |&gt;\n  as.data.frame() |&gt;\n  dplyr::mutate(weight = 1) |&gt; # 閾値以上のエッジしかないので、重みはすべて1にする\n  # dplyr::rename(weight = cosine) |&gt; # あるいは、閾値を指定せずに、コサイン類似度をそのまま重みとして使う（continuous LexRank）\n  igraph::graph_from_data_frame(directed = FALSE) |&gt;\n  igraph::page_rank(directed = FALSE, damping = .85) |&gt;\n  purrr::pluck(\"vector\")\n\nLexRankは抽出型の要約アルゴリズムということになっていますが、必ずしも要約的な文書が得られるわけではありません。文書集合のなかでも類似度が比較的高そうな文書をn件取り出してきてサブセットをつくるみたいな使い方ならできるかもしれないです。\n\nsort(scores, decreasing = TRUE) |&gt;\n  tibble::enframe() |&gt;\n  dplyr::left_join(\n    dplyr::select(tbl, doc_id, text, chapter),\n    by = dplyr::join_by(name == doc_id)\n  ) |&gt;\n  dplyr::slice_head(n = 5)\n#&gt; # A tibble: 5 × 4\n#&gt;   name    value text                                                     chapter\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;                                                    &lt;chr&gt;  \n#&gt; 1 1123  0.00702 「馬鹿だ」とやがてKが答えました。「僕は馬鹿だ」          3_41   \n#&gt; 2 1027  0.00490 私はKに手紙を見せました。Kは何ともいいませんでしたけれども、自分の所へこの姉から同じような意味の書状が二、三… 3_22   \n#&gt; 3 1025  0.00461 「Kの事件が一段落ついた後で、私は彼の姉の夫から長い封書を受け取りました。Kの養子に行った先は、この人の親類に… 3_22   \n#&gt; 4 1122  0.00461 私は二度同じ言葉を繰り返しました。そうして、その言葉がKの上にどう影響するかを見詰めていました。…… 3_41   \n#&gt; 5 1028  0.00382 私はKと同じような返事を彼の義兄宛で出しました。その中に、万一の場合には私がどうでもするから、安心するようにと… 3_22",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>文書メニュー</span>"
    ]
  },
  {
    "objectID": "cb-docs-menu.html#クラスター分析a.6.2",
    "href": "cb-docs-menu.html#クラスター分析a.6.2",
    "title": "Appendix E — 文書メニュー",
    "section": "E.2 クラスター分析（A.6.2）",
    "text": "E.2 クラスター分析（A.6.2）\n\nE.2.1 LSI🍳\n文書単語行列（または、単語文書行列）に対して特異値分解をおこなって、行列の次元を削減する手法をLSIといいます。潜在的意味インデキシング（Latent Semantic Indexing, LSI）というのは情報検索の分野での呼び方で、自然言語処理の文脈だと潜在意味解析（Latent Semantic Analysis, LSA）というらしいです。\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_trim(min_termfreq = 10) |&gt;\n  quanteda::dfm_tfidf(scheme_tf = \"prop\") |&gt;\n  rlang::as_function(\n    ~ quanteda::dfm_subset(., quanteda::rowSums(.) &gt; 0)\n  )()\n\ndfm\n#&gt; Document-feature matrix of: 1,099 documents, 332 features (97.22% sparse) and 0 docvars.\n#&gt;     features\n#&gt; docs  行く/動詞  当人/名詞  東京/地名  遊ぶ/動詞 学校/名詞 留まる/動詞\n#&gt;    2 0.03223983 0.06058653 0.03769972 0.05936909 0           0        \n#&gt;    3 0          0          0          0          0.3165641   0.1219662\n#&gt;    4 0          0          0          0          0           0        \n#&gt;    5 0          0          0          0          0           0        \n#&gt;    6 0.07307694 0          0          0          0           0        \n#&gt;    7 0          0          0          0          0           0        \n#&gt;     features\n#&gt; docs 友達/名詞  金/名詞C 一つ/名詞  知る/動詞\n#&gt;    2 0.2123140 0.1010305 0         0         \n#&gt;    3 0.1203113 0.1145013 0         0         \n#&gt;    4 0         0         0.2960317 0         \n#&gt;    5 0         0         0         0.06012324\n#&gt;    6 0         0         0         0         \n#&gt;    7 0         0         0         0         \n#&gt; [ reached max_ndoc ... 1,093 more documents, reached max_nfeat ... 322 more features ]\n\nここでは300列くらいしかないので大したことないですが、特徴量の数が多い文書単語行列をas.matrix()すると、メモリ上でのサイズが大きいオブジェクトになってしまい、扱いづらいです。そこで、もとの文書単語行列のもつ情報をできるだけ保持しつつ、行列の次元を削減したいというときに、LSIを利用することができます。\nとくに、文書のクラスタリングをおこなう場合では、どの語彙がどのクラスタに属する要因になっているかみたいなことはどうせ確認できないので、特徴量は適当に削減してしまって問題ないと思います。\n\nlobstr::obj_size(dfm)\n#&gt; 244.78 kB\nlobstr::obj_size(as.matrix(dfm))\n#&gt; 3.02 MB\n\nquanteda:textmodels::textmodel_lsa(margin = \"documents\")とすると、特異値分解（Truncated SVD）の \\(D \\simeq D_{k} = U_{k}\\Sigma{}_{k}V^{T}_{k}\\) という式における \\(V_{k}\\) が戻り値にそのまま残ります（margin=\"features\"だと \\(U_{k}\\) がそのまま残り、\"both\"で両方ともそのまま残ります）。\n特異値分解する行列 \\(D\\) について、いま、行側に文書・列側に単語がある持ち方をしています。ここでは、行列 \\(D\\) をランク \\(k\\) の行列 \\(D_{k}\\) で近似したい（ランク削減したい）というより、特徴量を減らしたい（ \\(k\\) 列の行列にしてしまいたい）と思っているため、dfmに \\(V_{k}\\) をかけます。\n\nmat &lt;- quanteda.textmodels::textmodel_lsa(dfm, nd = 50, margin = \"documents\")\nmat &lt;- dfm %*% mat$features\n\nstr(mat)\n#&gt; Formal class 'dgeMatrix' [package \"Matrix\"] with 4 slots\n#&gt;   ..@ Dim     : int [1:2] 1099 50\n#&gt;   ..@ Dimnames:List of 2\n#&gt;   .. ..$ docs: chr [1:1099] \"2\" \"3\" \"4\" \"5\" ...\n#&gt;   .. ..$     : NULL\n#&gt;   ..@ x       : num [1:54950] 0.0512 0.0259 0.0294 0.0372 0.0577 ...\n#&gt;   ..@ factors : list()\n\n\n\nE.2.2 階層的クラスタリング\nLSIで次元を削減した行列について、クラスタリングをおこないます。ここでは、文書間の距離としてコサイン距離を使うことにします。\n文書間の距離のイメージです。\n\ng1 &lt;- mat |&gt;\n  proxyC::simil(margin = 1, method = \"ejaccard\") |&gt;\n  rlang::as_function(~ 1 - .[1:100, 1:100])() |&gt;\n  as.dist() |&gt;\n  factoextra::fviz_dist() +\n  theme(axis.text.x = element_blank(), axis.text.y = element_blank()) +\n  labs(title = \"ejaccard\")\n#&gt; Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#&gt; ℹ Please use tidy evaluation idioms with `aes()`.\n#&gt; ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n#&gt; ℹ The deprecated feature was likely used in the factoextra package.\n#&gt;   Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\ng2 &lt;- mat |&gt;\n  proxyC::simil(margin = 1, method = \"edice\") |&gt;\n  rlang::as_function(~ 1 - .[1:100, 1:100])() |&gt;\n  as.dist() |&gt;\n  factoextra::fviz_dist() +\n  theme(axis.text.x = element_blank(), axis.text.y = element_blank()) +\n  labs(title = \"edice\")\n\ng3 &lt;- mat |&gt;\n  proxyC::simil(margin = 1, method = \"cosine\") |&gt;\n  rlang::as_function(~ 1 - .[1:100, 1:100])() |&gt;\n  as.dist() |&gt;\n  factoextra::fviz_dist() +\n  theme(axis.text.x = element_blank(), axis.text.y = element_blank()) +\n  labs(title = \"cosine\")\n\npatchwork::wrap_plots(g1, g2, g3, nrow = 3)\n\n\n\n\n\n\n\n\n階層的クラスタリングは非階層的なアルゴリズムに比べると計算量が多いため、個体数が増えるとクラスタリングするのにやや時間がかかることがあります。\n\ndat &lt;- mat |&gt;\n  proxyC::simil(margin = 1, method = \"cosine\") |&gt;\n  rlang::as_function(~ 1 - .)()\n\nclusters &lt;-\n  as.dist(dat) |&gt;\n  hclust(method = \"ward.D2\")\n\ncluster::silhouette(cutree(clusters, k = 5), dist = dat) |&gt;\n  factoextra::fviz_silhouette(print.summary = FALSE) +\n  theme_classic() +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nE.2.3 非階層的クラスタリング🍳\n必ずしもクラスタの階層構造を確認したいわけではない場合、kmeans()だと計算が高速かもしれません。\nただ、「K-meansはクラスタ中心からのユークリッド距離でクラスタを分ける」（機械学習帳）ため、特徴量の数が増えてくるとクラスタの比率がおかしくなりがちです。\n\nclusters &lt;- kmeans(mat, centers = 5, iter.max = 100, algorithm = \"Lloyd\")\n\ncluster::silhouette(clusters$cluster, dist = dat) |&gt;\n  factoextra::fviz_silhouette(print.summary = FALSE) +\n  theme_classic() +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\nspherical k-meansの実装であるskmeansだとクラスタの比率はいくらかマシになるかもしれません。\n\nclusters &lt;-\n  skmeans::skmeans(\n    as.matrix(mat),\n    k = 5,\n    method = \"pclust\",\n    control = list(maxiter = 100)\n  )\n\ncluster::silhouette(clusters$cluster, dist = dat) |&gt;\n  factoextra::fviz_silhouette(print.summary = FALSE) +\n  theme_classic() +\n  theme(axis.text.x = element_blank())",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>文書メニュー</span>"
    ]
  },
  {
    "objectID": "cb-docs-menu.html#トピックモデルa.6.3-4",
    "href": "cb-docs-menu.html#トピックモデルa.6.3-4",
    "title": "Appendix E — 文書メニュー",
    "section": "E.3 トピックモデル（A.6.3-4）",
    "text": "E.3 トピックモデル（A.6.3-4）\n\nE.3.1 トピック数の探索\nLDAのトピック数の探索は、実際にfitしてみて指標のよかったトピック数を採用するみたいなやり方をするようです。\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n)\n\nここでは、トピック数を5から15まで変化させます。seededlda::textmodel_lda(auto_iter=TRUE)とすると、ステップがmax_iter以下であっても条件によってギブスサンプリングを打ち切る挙動になります。\n\ndivergence &lt;-\n  purrr::map(5:15, \\(.x) {\n    lda_fit &lt;-\n      seededlda::textmodel_lda(dfm, k = .x, batch_size = 0.2, auto_iter = TRUE, verbose = FALSE)\n    tibble::tibble(\n      topics = .x,\n      Deveaud2014 = seededlda::divergence(lda_fit, regularize = FALSE),\n      WatanabeBaturo2023 = seededlda::divergence(lda_fit, min_size = .04, regularize = TRUE)\n    )\n  }) |&gt;\n  purrr::list_rbind()\n\nDeveaud2014という列は、ldatuningで確認できる同名の値と同じ指標です。WatanabeBaturo2023という列は、Deveaud2014についてトピックの比率が閾値を下回るときにペナルティを加えるように修正した指標です。どちらも大きいほうがよい指標なので、基本的には値が大きくなっているトピック数を選びます。\n\nggplot(divergence, aes(topics)) +\n  geom_line(aes(y = Deveaud2014, color = \"Deveaud2014\")) +\n  geom_line(aes(y = WatanabeBaturo2023, color = \"WatanabeBaturo2023\")) +\n  scale_x_continuous(breaks = 5:15) +\n  theme_bw() +\n  ylab(\"Divergence\")\n\n\n\n\n\n\n\n\n\n\nE.3.2 Distributed LDA\n\nlda_fit &lt;-\n  seededlda::textmodel_lda(dfm, k = 9, batch_size = 0.2, verbose = FALSE)\n\nseededlda::sizes(lda_fit)\n#&gt;     topic1     topic2     topic3     topic4     topic5     topic6     topic7 \n#&gt; 0.11265899 0.10963053 0.09408439 0.13375732 0.12502524 0.11987684 0.10624874 \n#&gt;     topic8     topic9 \n#&gt; 0.10004038 0.09867757\n\n\n\nE.3.3 トピックとその出現位置\n\ndat &lt;- tbl |&gt;\n  dplyr::transmute(\n    doc_id = doc_id,\n    topic = seededlda::topics(lda_fit)[as.character(doc_id)],\n  ) |&gt;\n  dplyr::filter(!is.na(topic)) # dfmをつくった時点で単語を含まない文書はトピックの割り当てがないため、取り除く\n\ndat |&gt;\n  ggplot(aes(x = doc_id)) +\n  geom_raster(aes(y = topic, fill = topic), show.legend = FALSE) +\n  theme_classic() +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\n\n\nE.3.4 単語の生起確率\n\ndat &lt;-\n  t(lda_fit$phi) |&gt;\n  dplyr::as_tibble(\n    .name_repair = ~ paste0(\"topic\", seq_along(.)),\n    rownames = \"word\"\n  ) |&gt;\n  tidyr::pivot_longer(starts_with(\"topic\"), names_to = \"topic\", values_to = \"phi\") |&gt;\n  dplyr::mutate(phi = signif(phi, 3)) |&gt;\n  dplyr::slice_max(phi, n = 20, by = topic)\n\nreactable::reactable(\n  dat,\n  filterable = TRUE,\n  defaultColDef = reactable::colDef(\n    cell = reactablefmtr::data_bars(dat, text_position = \"outside-base\")\n  )\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>文書メニュー</span>"
    ]
  },
  {
    "objectID": "cb-docs-menu.html#ナイーブベイズa.6.6-8",
    "href": "cb-docs-menu.html#ナイーブベイズa.6.6-8",
    "title": "Appendix E — 文書メニュー",
    "section": "E.4 ナイーブベイズ（A.6.6-8）",
    "text": "E.4 ナイーブベイズ（A.6.6-8）\nquanteda.textmodels::textmodel_nb()で分類する例です。ここでは、LexRankの節で抽出したscoresの付いている文書を使って学習します。交差検証はしません。\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", # \"名詞B\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_tfidf(scheme_tf = \"prop\")\n\nlabels &lt;- tbl |&gt;\n  dplyr::mutate(section = factor(section, labels = c(\"上\", \"中\", \"下\"))) |&gt;\n  dplyr::filter(doc_id %in% quanteda::docnames(dfm)) |&gt;\n  dplyr::pull(section, doc_id)\n\nnb_fit &lt;- dfm |&gt;\n  quanteda::dfm_subset(\n    quanteda::docnames(dfm) %in% names(scores)\n  ) |&gt;\n  rlang::as_function(~ {\n    # dfmに格納すると文書の順番が入れ替わるので、labelsの順番をあわせなければならない\n    quanteda.textmodels::textmodel_nb(., labels[quanteda::docnames(.)])\n  })()\n\ndat &lt;- tbl |&gt;\n  dplyr::mutate(section = factor(section, labels = c(\"上\", \"中\", \"下\"))) |&gt;\n  dplyr::filter(doc_id %in% quanteda::docnames(dfm)) |&gt;\n  dplyr::mutate(.pred = predict(nb_fit, dfm)[as.character(doc_id)]) # 予測値の順番をあわせる必要がある\n\nyardstick::conf_mat(dat, section, .pred) # 混同行列\n#&gt;           Truth\n#&gt; Prediction  上  中  下\n#&gt;         上 491 103  60\n#&gt;         中  43 140  11\n#&gt;         下  22  30 229\n\nyardstick::accuracy(dat, section, .pred) # 正解率\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy multiclass     0.762\n\nyardstick::f_meas(dat, section, .pred) # F値\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 f_meas  macro          0.733\n\n精度よく分類することよりも、各カテゴリにおける「スコア」を見るのが目的でナイーブベイズを使っているはずなので、確認してみます。\n\ndat &lt;-\n  coef(nb_fit) |&gt;\n  dplyr::as_tibble(rownames = \"token\") |&gt;\n  rlang::as_function(~ {\n    s &lt;- t(coef(nb_fit)) |&gt; colSums()\n    dplyr::mutate(.,\n      across(where(is.numeric), ~ . / s),\n      var = t(coef(nb_fit)) |&gt; cov() |&gt; diag(),\n      across(where(is.numeric), ~ signif(., 3))\n    )\n  })() |&gt;\n  dplyr::slice_max(var, n = 50)\n\nreactable::reactable(\n  dat,\n  filterable = TRUE,\n  defaultColDef = reactable::colDef(\n    cell = reactablefmtr::data_bars(dat, text_position = \"outside-base\")\n  )\n)\n\n\n\n\n\n\n\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)\n\nsessioninfo::session_info(info = \"packages\")\n#&gt; ═ Session info ═══════════════════════════════════════════════════════════════\n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package             * version  date (UTC) lib source\n#&gt;  abind                 1.4-8    2024-09-12 [1] RSPM (R 4.5.0)\n#&gt;  audubon               0.5.2    2024-04-27 [1] RSPM (R 4.5.0)\n#&gt;  backports             1.5.0    2024-05-23 [1] RSPM\n#&gt;  blob                  1.2.4    2023-03-17 [1] RSPM\n#&gt;  broom                 1.0.10   2025-09-13 [1] RSPM (R 4.5.0)\n#&gt;  cachem                1.1.0    2024-05-16 [1] RSPM\n#&gt;  car                   3.1-3    2024-09-27 [1] RSPM (R 4.5.0)\n#&gt;  carData               3.0-5    2022-01-06 [1] RSPM (R 4.5.0)\n#&gt;  cellranger            1.1.0    2016-07-27 [1] RSPM (R 4.5.0)\n#&gt;  cli                   3.6.5    2025-04-23 [1] RSPM\n#&gt;  clue                  0.3-66   2024-11-13 [1] RSPM (R 4.5.0)\n#&gt;  cluster               2.1.8.1  2025-03-12 [2] CRAN (R 4.5.1)\n#&gt;  codetools             0.2-20   2024-03-31 [2] CRAN (R 4.5.1)\n#&gt;  crosstalk             1.2.2    2025-08-26 [1] RSPM (R 4.5.0)\n#&gt;  curl                  7.0.0    2025-08-19 [1] RSPM\n#&gt;  DBI                 * 1.2.3    2024-06-02 [1] RSPM (R 4.5.0)\n#&gt;  dbplyr                2.5.1    2025-09-10 [1] RSPM\n#&gt;  digest                0.6.37   2024-08-19 [1] RSPM\n#&gt;  dplyr                 1.1.4    2023-11-17 [1] RSPM (R 4.5.0)\n#&gt;  duckdb              * 1.4.0    2025-09-18 [1] RSPM (R 4.5.0)\n#&gt;  evaluate              1.0.5    2025-08-27 [1] RSPM\n#&gt;  factoextra            1.0.7    2020-04-01 [1] RSPM (R 4.5.0)\n#&gt;  farver                2.1.2    2024-05-13 [1] RSPM (R 4.5.0)\n#&gt;  fastmap               1.2.0    2024-05-15 [1] RSPM\n#&gt;  fastmatch             1.1-6    2024-12-23 [1] RSPM (R 4.5.0)\n#&gt;  foreach               1.5.2    2022-02-02 [1] RSPM (R 4.5.0)\n#&gt;  Formula               1.2-5    2023-02-24 [1] RSPM (R 4.5.0)\n#&gt;  generics              0.1.4    2025-05-09 [1] RSPM (R 4.5.0)\n#&gt;  ggplot2             * 4.0.0    2025-09-11 [1] RSPM (R 4.5.0)\n#&gt;  ggpubr                0.6.1    2025-06-27 [1] RSPM (R 4.5.0)\n#&gt;  ggrepel               0.9.6    2024-09-07 [1] RSPM (R 4.5.0)\n#&gt;  ggsignif              0.6.4    2022-10-13 [1] RSPM (R 4.5.0)\n#&gt;  gibasa                1.1.2    2025-02-16 [1] RSPM (R 4.5.0)\n#&gt;  glmnet                4.1-10   2025-07-17 [1] RSPM (R 4.5.0)\n#&gt;  glue                  1.8.0    2024-09-30 [1] RSPM\n#&gt;  gtable                0.3.6    2024-10-25 [1] RSPM (R 4.5.0)\n#&gt;  htmltools             0.5.8.1  2024-04-04 [1] RSPM\n#&gt;  htmlwidgets           1.6.4    2023-12-06 [1] RSPM\n#&gt;  igraph                2.1.4    2025-01-23 [1] RSPM (R 4.5.0)\n#&gt;  iterators             1.0.14   2022-02-05 [1] RSPM (R 4.5.0)\n#&gt;  janeaustenr           1.0.0    2022-08-26 [1] RSPM (R 4.5.0)\n#&gt;  jsonlite              2.0.0    2025-03-27 [1] RSPM\n#&gt;  knitr                 1.50     2025-03-16 [1] RSPM\n#&gt;  labeling              0.4.3    2023-08-29 [1] RSPM (R 4.5.0)\n#&gt;  lattice               0.22-7   2025-04-02 [2] CRAN (R 4.5.1)\n#&gt;  lifecycle             1.0.4    2023-11-07 [1] RSPM\n#&gt;  lobstr                1.1.2    2022-06-22 [1] RSPM (R 4.5.0)\n#&gt;  magrittr              2.0.4    2025-09-12 [1] RSPM\n#&gt;  Matrix                1.7-3    2025-03-11 [2] CRAN (R 4.5.1)\n#&gt;  memoise               2.0.1    2021-11-26 [1] RSPM\n#&gt;  nsyllable             1.0.1    2022-02-28 [1] RSPM (R 4.5.0)\n#&gt;  patchwork             1.3.2    2025-08-25 [1] RSPM (R 4.5.0)\n#&gt;  pillar                1.11.1   2025-09-17 [1] RSPM\n#&gt;  pkgconfig             2.0.3    2019-09-22 [1] RSPM\n#&gt;  plyr                  1.8.9    2023-10-02 [1] RSPM (R 4.5.0)\n#&gt;  prettyunits           1.2.0    2023-09-24 [1] RSPM\n#&gt;  proxyC                0.5.2    2025-04-25 [1] RSPM (R 4.5.0)\n#&gt;  purrr                 1.1.0    2025-07-10 [1] RSPM\n#&gt;  quanteda              4.3.1    2025-07-10 [1] RSPM (R 4.5.0)\n#&gt;  quanteda.textmodels   0.9.10   2025-02-10 [1] RSPM (R 4.5.0)\n#&gt;  quanteda.textstats    0.97.2   2024-09-03 [1] RSPM (R 4.5.0)\n#&gt;  R.cache               0.17.0   2025-05-02 [1] RSPM\n#&gt;  R.methodsS3           1.8.2    2022-06-13 [1] RSPM\n#&gt;  R.oo                  1.27.1   2025-05-02 [1] RSPM\n#&gt;  R.utils               2.13.0   2025-02-24 [1] RSPM\n#&gt;  R6                    2.6.1    2025-02-15 [1] RSPM\n#&gt;  RColorBrewer          1.1-3    2022-04-03 [1] RSPM (R 4.5.0)\n#&gt;  Rcpp                  1.1.0    2025-07-02 [1] RSPM\n#&gt;  RcppParallel          5.1.11-1 2025-08-27 [1] RSPM (R 4.5.0)\n#&gt;  reactable             0.4.4    2023-03-12 [1] RSPM (R 4.5.0)\n#&gt;  reactablefmtr         2.0.0    2022-03-16 [1] RSPM (R 4.5.0)\n#&gt;  reactR                0.6.1    2024-09-14 [1] RSPM (R 4.5.0)\n#&gt;  readxl                1.4.5    2025-03-07 [1] RSPM (R 4.5.0)\n#&gt;  reshape2              1.4.4    2020-04-09 [1] RSPM (R 4.5.0)\n#&gt;  rlang                 1.1.6    2025-04-11 [1] RSPM\n#&gt;  rmarkdown             2.30     2025-09-28 [1] RSPM (R 4.5.0)\n#&gt;  RSpectra              0.16-2   2024-07-18 [1] RSPM (R 4.5.0)\n#&gt;  rstatix               0.7.2    2023-02-01 [1] RSPM (R 4.5.0)\n#&gt;  S7                    0.2.0    2024-11-07 [1] RSPM (R 4.5.0)\n#&gt;  sass                  0.4.10   2025-04-11 [1] RSPM\n#&gt;  scales                1.4.0    2025-04-24 [1] RSPM (R 4.5.0)\n#&gt;  seededlda             1.4.3    2025-09-28 [1] RSPM (R 4.5.0)\n#&gt;  sessioninfo           1.2.3    2025-02-05 [1] RSPM\n#&gt;  shape                 1.4.6.1  2024-02-23 [1] RSPM (R 4.5.0)\n#&gt;  skmeans               0.2-18   2024-11-13 [1] RSPM (R 4.5.0)\n#&gt;  slam                  0.1-55   2024-11-13 [1] RSPM (R 4.5.0)\n#&gt;  SnowballC             0.7.1    2023-04-25 [1] RSPM (R 4.5.0)\n#&gt;  stopwords             2.3      2021-10-28 [1] RSPM (R 4.5.0)\n#&gt;  stringi               1.8.7    2025-03-27 [1] RSPM\n#&gt;  stringr               1.5.2    2025-09-08 [1] RSPM\n#&gt;  styler                1.10.3   2024-04-07 [1] RSPM\n#&gt;  survival              3.8-3    2024-12-17 [2] CRAN (R 4.5.1)\n#&gt;  tibble                3.3.0    2025-06-08 [1] RSPM\n#&gt;  tidyr                 1.3.1    2024-01-24 [1] RSPM (R 4.5.0)\n#&gt;  tidyselect            1.2.1    2024-03-11 [1] RSPM (R 4.5.0)\n#&gt;  tidytext              0.4.3    2025-07-25 [1] RSPM (R 4.5.0)\n#&gt;  tokenizers            0.3.0    2022-12-22 [1] RSPM (R 4.5.0)\n#&gt;  utf8                  1.2.6    2025-06-08 [1] RSPM\n#&gt;  V8                    8.0.0    2025-09-27 [1] RSPM (R 4.5.0)\n#&gt;  vctrs                 0.6.5    2023-12-01 [1] RSPM\n#&gt;  withr                 3.0.2    2024-10-28 [1] RSPM\n#&gt;  xfun                  0.53     2025-08-19 [1] RSPM\n#&gt;  yaml                  2.3.10   2024-07-26 [1] RSPM\n#&gt;  yardstick             1.3.2    2025-01-22 [1] RSPM (R 4.5.0)\n#&gt; \n#&gt;  [1] /usr/local/lib/R/site-library\n#&gt;  [2] /usr/local/lib/R/library\n#&gt;  * ── Packages attached to the search path.\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>文書メニュー</span>"
    ]
  },
  {
    "objectID": "cb-codes-menu.html",
    "href": "cb-codes-menu.html",
    "title": "Appendix F — コーディングメニュー",
    "section": "",
    "text": "F.1 単純集計（A.7.1）\nrules &lt;- list(\n  \"人の死\" = c(\"死後\", \"死病\", \"死期\", \"死因\", \"死骸\", \"生死\", \"自殺\", \"殉死\", \"頓死\", \"変死\", \"亡\", \"死ぬ\", \"亡くなる\", \"殺す\", \"亡くす\", \"死\"),\n  \"恋愛\" = c(\"愛\", \"恋\", \"愛す\", \"愛情\", \"恋人\", \"愛人\", \"恋愛\", \"失恋\", \"恋しい\"),\n  \"友情\" = c(\"友達\", \"友人\", \"旧友\", \"親友\", \"朋友\", \"友\", \"級友\"),\n  \"信用・不信\" = c(\"信用\", \"信じる\", \"信ずる\", \"不信\", \"疑い\", \"疑惑\", \"疑念\", \"猜疑\", \"狐疑\", \"疑問\", \"疑い深い\", \"疑う\", \"疑る\", \"警戒\"),\n  \"病気\" = c(\"医者\", \"病人\", \"病室\", \"病院\", \"病症\", \"病状\", \"持病\", \"死病\", \"主治医\", \"精神病\", \"仮病\", \"病気\", \"看病\", \"大病\", \"病む\", \"病\")\n) |&gt;\n  quanteda::dictionary()\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(original), token, original)) |&gt;\n  dplyr::count(doc_id, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::dfm_lookup(rules)\n\ndfm |&gt;\n  quanteda::convert(to = \"data.frame\") |&gt;\n  dplyr::mutate(`コードなし` = as.numeric(rowSums(dplyr::pick(where(is.numeric))) == 0)) |&gt;\n  tidyr::pivot_longer(cols = !doc_id, names_to = \"code\", values_to = \"count\") |&gt;\n  dplyr::summarise(\n    total = sum(count),\n    prop = total / dplyr::n(),\n    .by = code\n  )\n#&gt; # A tibble: 6 × 3\n#&gt;   code       total   prop\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 人の死       148 0.122 \n#&gt; 2 恋愛          65 0.0536\n#&gt; 3 友情          51 0.0420\n#&gt; 4 信用・不信   123 0.101 \n#&gt; 5 病気         150 0.124 \n#&gt; 6 コードなし   919 0.758",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>コーディングメニュー</span>"
    ]
  },
  {
    "objectID": "cb-codes-menu.html#クロス集計a.7.2",
    "href": "cb-codes-menu.html#クロス集計a.7.2",
    "title": "Appendix F — コーディングメニュー",
    "section": "F.2 クロス集計（A.7.2）",
    "text": "F.2 クロス集計（A.7.2）\n\nF.2.1 クロス表\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(original), token, original)) |&gt;\n  dplyr::count(label, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(label, token, n) |&gt;\n  quanteda::dfm_lookup(rules)\n\ndfm |&gt;\n  quanteda::convert(to = \"data.frame\") |&gt;\n  dplyr::mutate(`コードなし` = as.numeric(rowSums(dplyr::pick(where(is.numeric))) == 0)) |&gt;\n  tidyr::pivot_longer(cols = !doc_id, names_to = \"code\", values_to = \"count\") |&gt;\n  dplyr::left_join(\n    dplyr::distinct(tbl, label, section),\n    by = dplyr::join_by(doc_id == label)\n  ) |&gt;\n  tidyr::uncount(count) |&gt;\n  crosstable::crosstable(section, by = code, total = \"both\") |&gt;\n  crosstable::as_flextable()\n\nlabelvariablecodeTotalコードなし信用・不信人の死病気友情恋愛section[1]上_先生と私6 (3.26%)36 (19.57%)53 (28.80%)51 (27.72%)17 (9.24%)21 (11.41%)184 (33.21%)[2]中_両親と私0 (0%)13 (10.57%)30 (24.39%)76 (61.79%)4 (3.25%)0 (0%)123 (22.20%)[3]下_先生と遺書11 (4.45%)74 (29.96%)65 (26.32%)23 (9.31%)30 (12.15%)44 (17.81%)247 (44.58%)Total17 (3.07%)123 (22.20%)148 (26.71%)150 (27.08%)51 (9.21%)65 (11.73%)554 (100.00%)\n\n\n\n\nF.2.2 ヒートマップ\n横に長すぎてラベルが見づらいです。\n\ndfm |&gt;\n  quanteda::convert(to = \"data.frame\") |&gt;\n  dplyr::mutate(`コードなし` = as.numeric(rowSums(dplyr::pick(where(is.numeric))) == 0)) |&gt;\n  tidyr::pivot_longer(cols = !doc_id, names_to = \"code\", values_to = \"count\") |&gt;\n  dplyr::filter(count &gt; 0) |&gt;\n  ggplot(aes(x = factor(doc_id, levels = unique(tbl$label)), y = code)) +\n  geom_raster(aes(fill = count)) +\n  labs(x = element_blank(), y = element_blank()) +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1))\n#&gt; Warning: `label` cannot be a &lt;ggplot2::element_blank&gt; object.\n#&gt; `label` cannot be a &lt;ggplot2::element_blank&gt; object.\n\n\n\n\n\n\n\n\n\n\nF.2.3 バルーンプロット\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(original), token, original)) |&gt;\n  dplyr::count(section, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(section, token, n) |&gt;\n  quanteda::dfm_lookup(rules)\n\ndat &lt;- dfm |&gt;\n  quanteda::convert(to = \"data.frame\") |&gt;\n  dplyr::mutate(`コードなし` = as.numeric(rowSums(dplyr::pick(where(is.numeric))) == 0)) |&gt;\n  tidyr::pivot_longer(cols = !doc_id, names_to = \"code\", values_to = \"count\")\n\nclusters &lt;- dat |&gt;\n  tidytext::cast_dfm(doc_id, code, count) |&gt;\n  proxyC::dist(margin = 2, method = \"euclidean\") |&gt;\n  as.dist() |&gt;\n  hclust(method = \"ward.D2\")\n\ndat |&gt;\n  ggpubr::ggballoonplot(x = \"doc_id\", y = \"code\", size = \"count\", color = \"gray\", fill = \"#f5f5f5\", show.label = TRUE) +\n  legendry::scale_y_dendro(clust = clusters)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>コーディングメニュー</span>"
    ]
  },
  {
    "objectID": "cb-codes-menu.html#類似度行列a.7.3",
    "href": "cb-codes-menu.html#類似度行列a.7.3",
    "title": "Appendix F — コーディングメニュー",
    "section": "F.3 類似度行列（A.7.3）",
    "text": "F.3 類似度行列（A.7.3）\n\ndfm &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(original), token, original)) |&gt;\n  dplyr::count(label, token) |&gt;\n  dplyr::collect() |&gt;\n  tidytext::cast_dfm(label, token, n) |&gt;\n  quanteda::dfm_lookup(rules) |&gt;\n  quanteda::dfm_weight(scheme = \"boolean\")\n\nquanteda.textstats::textstat_simil(dfm, margin = \"features\", method = \"jaccard\")\n#&gt; textstat_simil object; method = \"jaccard\"\n#&gt;            人の死   恋愛   友情 信用・不信   病気\n#&gt; 人の死      1.000 0.1129 0.1186      0.253 0.4310\n#&gt; 恋愛        0.113 1.0000 0.0652      0.283 0.0476\n#&gt; 友情        0.119 0.0652 1.0000      0.175 0.1053\n#&gt; 信用・不信  0.253 0.2833 0.1746      1.000 0.2817\n#&gt; 病気        0.431 0.0476 0.1053      0.282 1.0000",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>コーディングメニュー</span>"
    ]
  },
  {
    "objectID": "cb-codes-menu.html#その他の分析a.7.4-8",
    "href": "cb-codes-menu.html#その他の分析a.7.4-8",
    "title": "Appendix F — コーディングメニュー",
    "section": "F.4 その他の分析（A.7.4-8）",
    "text": "F.4 その他の分析（A.7.4-8）\n基本的に抽出語メニューのときと同じやり方でグラフをつくることができるはずです。階層的クラスター分析、共起ネットワーク、SOMについては省略しています。\n\nF.4.1 対応分析\n\nlibrary(ca)\n\nquanteda.textmodels::textmodel_ca(dfm, nd = 2, sparse = TRUE) |&gt;\n  plot()\n\n\n\n\n\n\n\n\n\n\nF.4.2 多次元尺度構成法（MDS）\n\nsimil &lt;- dfm |&gt;\n  proxyC::simil(margin = 2, method = \"jaccard\")\n\ndat &lt;- MASS::sammon(1 - simil, k = 2) |&gt;\n  purrr::pluck(\"points\")\n#&gt; Initial stress        : 0.09511\n#&gt; stress after  10 iters: 0.03043, magic = 0.500\n#&gt; stress after  20 iters: 0.03038, magic = 0.500\n\n\ndat &lt;- dat |&gt;\n  dplyr::as_tibble(\n    rownames = \"label\",\n    .name_repair = ~ c(\"Dim1\", \"Dim2\")\n  ) |&gt;\n  dplyr::mutate(\n    clust = (hclust(\n      proxyC::dist(dat, method = \"euclidean\") |&gt; as.dist(),\n      method = \"ward.D2\"\n    ) |&gt; cutree(k = 3))[label]\n  )\n\ndat |&gt;\n  ggplot(aes(x = Dim1, y = Dim2, label = label, col = factor(clust))) +\n  geom_point(alpha = .3, show.legend = FALSE) +\n  ggrepel::geom_label_repel(show.legend = FALSE) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nF.4.3 LSS🍳\n極性をあらわす少数の種語を使いつつ、指定した語と共起する語や文書について1次元の極性を与える手法だそうです。LSXというパッケージとして実装されています。\n本来はk（Truncated SVDにおけるランク）は200～300程度を指定するため、相当の量の文書が必要とされます。提案論文では、おおむね40文程度の長さの文書が5,000～10,000文書くらい必要と書かれています。ここでは分析にかける文書が足りていないので、意味を解釈できる結果は得られていないと思います。\n\nrules &lt;-\n  list(\n    \"人の死\" = c(\"死後\", \"死病\", \"死期\", \"死因\", \"死骸\", \"生死\", \"自殺\", \"殉死\", \"頓死\", \"変死\", \"亡\", \"死ぬ\", \"亡くなる\", \"殺す\", \"亡くす\", \"死\"),\n    \"恋愛\" = c(\"愛\", \"恋\", \"愛す\", \"愛情\", \"恋人\", \"愛人\", \"恋愛\", \"失恋\", \"恋しい\"),\n    \"友情\" = c(\"友達\", \"友人\", \"旧友\", \"親友\", \"朋友\", \"友\", \"級友\"),\n    \"信用・不信\" = c(\"信用\", \"信じる\", \"信ずる\", \"不信\", \"疑い\", \"疑惑\", \"疑念\", \"猜疑\", \"狐疑\", \"疑問\", \"疑い深い\", \"疑う\", \"疑る\", \"警戒\"),\n    \"病気\" = c(\"医者\", \"病人\", \"病室\", \"病院\", \"病症\", \"病状\", \"持病\", \"死病\", \"主治医\", \"精神病\", \"仮病\", \"病気\", \"看病\", \"大病\", \"病む\", \"病\")\n  ) |&gt;\n  quanteda::dictionary()\n\n# 日本語評価極性辞書（用言編） https://www.cl.ecei.tohoku.ac.jp/Open_Resources-Japanese_Sentiment_Polarity_Dictionary.html\npn &lt;-\n  readr::read_tsv(\n    \"https://www.cl.ecei.tohoku.ac.jp/resources/sent_lex/wago.121808.pn\",\n    col_names = c(\"polarity\", \"word\"),\n    show_col_types = FALSE\n  )\n\n# 極性辞書をもとに種語を用意する\nseed &lt;- pn |&gt;\n  dplyr::inner_join(\n    dplyr::tbl(con, \"tokens\") |&gt;\n      dplyr::filter(pos == \"動詞\") |&gt;\n      dplyr::select(token, pos, original) |&gt;\n      dplyr::distinct() |&gt;\n      dplyr::collect(),\n    by = c(\"word\" = \"token\")\n  ) |&gt;\n  dplyr::mutate(\n    polarity = dplyr::if_else(\n      stringr::str_detect(polarity, \"ネガ\"),\n      \"negative\",\n      \"positive\"\n    ),\n    token = dplyr::if_else(is.na(original), word, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::distinct(polarity, token) |&gt;\n  dplyr::reframe(dict = list(token), .by = polarity) |&gt;\n  tibble::deframe()\n\nseed &lt;- seed |&gt;\n  quanteda::dictionary() |&gt;\n  LSX::as.seedwords(upper = 2, lower = 1) # ここではpositiveが2番目, negativeが1番目\n#&gt; Registered S3 methods overwritten by 'LSX':\n#&gt;   method                       from               \n#&gt;   print.coefficients_textmodel quanteda.textmodels\n#&gt;   print.statistics_textmodel   quanteda.textmodels\n#&gt;   print.summary.textmodel      quanteda.textmodels\n\ntoks &lt;-\n  dplyr::tbl(con, \"tokens\") |&gt;\n  dplyr::filter(\n    pos %in% c(\n      \"名詞\", \"名詞C\",\n      \"地名\", \"人名\", \"組織名\", \"固有名詞\",\n      \"動詞\", \"未知語\", \"タグ\"\n    )\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(original), token, original),\n    token = paste(token, pos, sep = \"/\")\n  ) |&gt;\n  dplyr::select(label, token) |&gt;\n  dplyr::collect() |&gt;\n  dplyr::reframe(dict = list(token), .by = label) |&gt;\n  tibble::deframe() |&gt;\n  quanteda::as.tokens()\n\nterm &lt;-\n  LSX::char_context(\n    toks,\n    pattern = rules$`信用・不信`,\n    window = 10,\n    valuetype = \"regex\",\n    case_insensitive = FALSE,\n    min_count = 2,\n    p = 0.05\n  ) |&gt;\n  toupper()\n\n\nlss &lt;-\n  LSX::textmodel_lss(\n    quanteda::dfm(toks),\n    seeds = seed,\n    terms = term,\n    k = 20,\n    include_data = TRUE,\n    group_data = TRUE\n  )\n\n単語の極性です。\n\nLSX::textplot_terms(lss)\n\n\n\n\n\n\n\n\n文書の極性です。ここでは文書の数が少ないのでこのようにプロットしていますが、実際にはもっと大量の文書を分析にかけるはずなので、文書を横軸にとってpolarityの曲線を描く可視化例がパッケージのvignetteで紹介されています。\n\ntibble::tibble(\n  docs = factor(unique(tbl$label), levels = unique(tbl$label)),\n  polarity = predict(lss)[as.character(docs)],\n  section = tbl$section[match(docs, tbl$label)]\n) |&gt;\n  dplyr::filter(!is.na(polarity)) |&gt;\n  ggplot(aes(x = docs, y = polarity, fill = section)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nduckdb::dbDisconnect(con)\nduckdb::duckdb_shutdown(drv)\n\nsessioninfo::session_info(info = \"packages\")\n#&gt; ═ Session info ═══════════════════════════════════════════════════════════════\n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package             * version  date (UTC) lib source\n#&gt;  abind                 1.4-8    2024-09-12 [1] RSPM (R 4.5.0)\n#&gt;  askpass               1.2.1    2024-10-04 [1] RSPM\n#&gt;  audubon               0.5.2    2024-04-27 [1] RSPM (R 4.5.0)\n#&gt;  backports             1.5.0    2024-05-23 [1] RSPM\n#&gt;  bit                   4.6.0    2025-03-06 [1] RSPM (R 4.5.0)\n#&gt;  bit64                 4.6.0-1  2025-01-16 [1] RSPM (R 4.5.0)\n#&gt;  blob                  1.2.4    2023-03-17 [1] RSPM\n#&gt;  broom                 1.0.10   2025-09-13 [1] RSPM (R 4.5.0)\n#&gt;  ca                  * 0.71.1   2020-01-24 [1] RSPM (R 4.5.0)\n#&gt;  cachem                1.1.0    2024-05-16 [1] RSPM\n#&gt;  car                   3.1-3    2024-09-27 [1] RSPM (R 4.5.0)\n#&gt;  carData               3.0-5    2022-01-06 [1] RSPM (R 4.5.0)\n#&gt;  cellranger            1.1.0    2016-07-27 [1] RSPM (R 4.5.0)\n#&gt;  checkmate             2.3.3    2025-08-18 [1] RSPM (R 4.5.0)\n#&gt;  cli                   3.6.5    2025-04-23 [1] RSPM\n#&gt;  codetools             0.2-20   2024-03-31 [2] CRAN (R 4.5.1)\n#&gt;  crayon                1.5.3    2024-06-20 [1] RSPM\n#&gt;  crosstable            0.8.2    2025-09-07 [1] RSPM (R 4.5.0)\n#&gt;  curl                  7.0.0    2025-08-19 [1] RSPM\n#&gt;  data.table            1.17.8   2025-07-10 [1] RSPM (R 4.5.0)\n#&gt;  DBI                 * 1.2.3    2024-06-02 [1] RSPM (R 4.5.0)\n#&gt;  dbplyr                2.5.1    2025-09-10 [1] RSPM\n#&gt;  digest                0.6.37   2024-08-19 [1] RSPM\n#&gt;  dplyr                 1.1.4    2023-11-17 [1] RSPM (R 4.5.0)\n#&gt;  duckdb              * 1.4.0    2025-09-18 [1] RSPM (R 4.5.0)\n#&gt;  evaluate              1.0.5    2025-08-27 [1] RSPM\n#&gt;  farver                2.1.2    2024-05-13 [1] RSPM (R 4.5.0)\n#&gt;  fastmap               1.2.0    2024-05-15 [1] RSPM\n#&gt;  fastmatch             1.1-6    2024-12-23 [1] RSPM (R 4.5.0)\n#&gt;  flextable             0.9.10   2025-08-24 [1] RSPM (R 4.5.0)\n#&gt;  fontBitstreamVera     0.1.1    2017-02-01 [1] RSPM (R 4.5.0)\n#&gt;  fontLiberation        0.1.0    2016-10-15 [1] RSPM (R 4.5.0)\n#&gt;  fontquiver            0.2.1    2017-02-01 [1] RSPM (R 4.5.0)\n#&gt;  forcats               1.0.1    2025-09-25 [1] RSPM (R 4.5.0)\n#&gt;  foreach               1.5.2    2022-02-02 [1] RSPM (R 4.5.0)\n#&gt;  Formula               1.2-5    2023-02-24 [1] RSPM (R 4.5.0)\n#&gt;  gdtools               0.4.3    2025-08-26 [1] RSPM (R 4.5.0)\n#&gt;  generics              0.1.4    2025-05-09 [1] RSPM (R 4.5.0)\n#&gt;  ggplot2             * 4.0.0    2025-09-11 [1] RSPM (R 4.5.0)\n#&gt;  ggpubr                0.6.1    2025-06-27 [1] RSPM (R 4.5.0)\n#&gt;  ggrepel               0.9.6    2024-09-07 [1] RSPM (R 4.5.0)\n#&gt;  ggsignif              0.6.4    2022-10-13 [1] RSPM (R 4.5.0)\n#&gt;  gibasa                1.1.2    2025-02-16 [1] RSPM (R 4.5.0)\n#&gt;  glmnet                4.1-10   2025-07-17 [1] RSPM (R 4.5.0)\n#&gt;  glue                  1.8.0    2024-09-30 [1] RSPM\n#&gt;  gtable                0.3.6    2024-10-25 [1] RSPM (R 4.5.0)\n#&gt;  hms                   1.1.3    2023-03-21 [1] RSPM (R 4.5.0)\n#&gt;  htmltools             0.5.8.1  2024-04-04 [1] RSPM\n#&gt;  htmlwidgets           1.6.4    2023-12-06 [1] RSPM\n#&gt;  iterators             1.0.14   2022-02-05 [1] RSPM (R 4.5.0)\n#&gt;  janeaustenr           1.0.0    2022-08-26 [1] RSPM (R 4.5.0)\n#&gt;  jsonlite              2.0.0    2025-03-27 [1] RSPM\n#&gt;  knitr                 1.50     2025-03-16 [1] RSPM\n#&gt;  labeling              0.4.3    2023-08-29 [1] RSPM (R 4.5.0)\n#&gt;  lattice               0.22-7   2025-04-02 [2] CRAN (R 4.5.1)\n#&gt;  legendry              0.2.4    2025-09-14 [1] RSPM (R 4.5.0)\n#&gt;  lifecycle             1.0.4    2023-11-07 [1] RSPM\n#&gt;  locfit                1.5-9.12 2025-03-05 [1] RSPM (R 4.5.0)\n#&gt;  LSX                   1.5.0    2025-09-12 [1] RSPM (R 4.5.0)\n#&gt;  magrittr              2.0.4    2025-09-12 [1] RSPM\n#&gt;  MASS                  7.3-65   2025-02-28 [2] CRAN (R 4.5.1)\n#&gt;  Matrix                1.7-3    2025-03-11 [2] CRAN (R 4.5.1)\n#&gt;  memoise               2.0.1    2021-11-26 [1] RSPM\n#&gt;  nsyllable             1.0.1    2022-02-28 [1] RSPM (R 4.5.0)\n#&gt;  officer               0.7.0    2025-09-03 [1] RSPM (R 4.5.0)\n#&gt;  openssl               2.3.4    2025-09-30 [1] RSPM (R 4.5.0)\n#&gt;  pillar                1.11.1   2025-09-17 [1] RSPM\n#&gt;  pkgconfig             2.0.3    2019-09-22 [1] RSPM\n#&gt;  proxyC                0.5.2    2025-04-25 [1] RSPM (R 4.5.0)\n#&gt;  purrr                 1.1.0    2025-07-10 [1] RSPM\n#&gt;  quanteda              4.3.1    2025-07-10 [1] RSPM (R 4.5.0)\n#&gt;  quanteda.textmodels   0.9.10   2025-02-10 [1] RSPM (R 4.5.0)\n#&gt;  quanteda.textstats    0.97.2   2024-09-03 [1] RSPM (R 4.5.0)\n#&gt;  R.cache               0.17.0   2025-05-02 [1] RSPM\n#&gt;  R.methodsS3           1.8.2    2022-06-13 [1] RSPM\n#&gt;  R.oo                  1.27.1   2025-05-02 [1] RSPM\n#&gt;  R.utils               2.13.0   2025-02-24 [1] RSPM\n#&gt;  R6                    2.6.1    2025-02-15 [1] RSPM\n#&gt;  ragg                  1.5.0    2025-09-02 [1] RSPM\n#&gt;  RColorBrewer          1.1-3    2022-04-03 [1] RSPM (R 4.5.0)\n#&gt;  Rcpp                  1.1.0    2025-07-02 [1] RSPM\n#&gt;  RcppParallel          5.1.11-1 2025-08-27 [1] RSPM (R 4.5.0)\n#&gt;  readr                 2.1.5    2024-01-10 [1] RSPM (R 4.5.0)\n#&gt;  readxl                1.4.5    2025-03-07 [1] RSPM (R 4.5.0)\n#&gt;  rlang                 1.1.6    2025-04-11 [1] RSPM\n#&gt;  rmarkdown             2.30     2025-09-28 [1] RSPM (R 4.5.0)\n#&gt;  RSpectra              0.16-2   2024-07-18 [1] RSPM (R 4.5.0)\n#&gt;  rstatix               0.7.2    2023-02-01 [1] RSPM (R 4.5.0)\n#&gt;  S7                    0.2.0    2024-11-07 [1] RSPM (R 4.5.0)\n#&gt;  scales                1.4.0    2025-04-24 [1] RSPM (R 4.5.0)\n#&gt;  sessioninfo           1.2.3    2025-02-05 [1] RSPM\n#&gt;  shape                 1.4.6.1  2024-02-23 [1] RSPM (R 4.5.0)\n#&gt;  SnowballC             0.7.1    2023-04-25 [1] RSPM (R 4.5.0)\n#&gt;  stopwords             2.3      2021-10-28 [1] RSPM (R 4.5.0)\n#&gt;  stringi               1.8.7    2025-03-27 [1] RSPM\n#&gt;  stringr               1.5.2    2025-09-08 [1] RSPM\n#&gt;  styler                1.10.3   2024-04-07 [1] RSPM\n#&gt;  survival              3.8-3    2024-12-17 [2] CRAN (R 4.5.1)\n#&gt;  systemfonts           1.3.1    2025-10-01 [1] RSPM (R 4.5.0)\n#&gt;  textshaping           1.0.3    2025-09-02 [1] RSPM\n#&gt;  tibble                3.3.0    2025-06-08 [1] RSPM\n#&gt;  tidyr                 1.3.1    2024-01-24 [1] RSPM (R 4.5.0)\n#&gt;  tidyselect            1.2.1    2024-03-11 [1] RSPM (R 4.5.0)\n#&gt;  tidytext              0.4.3    2025-07-25 [1] RSPM (R 4.5.0)\n#&gt;  tokenizers            0.3.0    2022-12-22 [1] RSPM (R 4.5.0)\n#&gt;  tzdb                  0.5.0    2025-03-15 [1] RSPM (R 4.5.0)\n#&gt;  utf8                  1.2.6    2025-06-08 [1] RSPM\n#&gt;  uuid                  1.2-1    2024-07-29 [1] RSPM (R 4.5.0)\n#&gt;  V8                    8.0.0    2025-09-27 [1] RSPM (R 4.5.0)\n#&gt;  vctrs                 0.6.5    2023-12-01 [1] RSPM\n#&gt;  vroom                 1.6.6    2025-09-19 [1] RSPM (R 4.5.0)\n#&gt;  withr                 3.0.2    2024-10-28 [1] RSPM\n#&gt;  xfun                  0.53     2025-08-19 [1] RSPM\n#&gt;  xml2                  1.4.0    2025-08-20 [1] RSPM\n#&gt;  yaml                  2.3.10   2024-07-26 [1] RSPM\n#&gt;  zip                   2.3.3    2025-05-13 [1] RSPM\n#&gt; \n#&gt;  [1] /usr/local/lib/R/site-library\n#&gt;  [2] /usr/local/lib/R/library\n#&gt;  * ── Packages attached to the search path.\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>コーディングメニュー</span>"
    ]
  }
]