[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RとMeCabによる日本語テキストマイニングの前処理",
    "section": "",
    "text": "はじめに",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "index.html#この資料について",
    "href": "index.html#この資料について",
    "title": "RとMeCabによる日本語テキストマイニングの前処理",
    "section": "この資料について",
    "text": "この資料について\n\nこの資料でやりたいこと\ngibasaやその他のRパッケージを使って、RMeCabでできるようなテキストマイニングの前処理をより見通しよくおこなうやり方を紹介します。\n\n\n想定する知識など\nR言語の基本的な使い方の説明はしません。tidyverseなどの使い方については、他の資料を参照してください。参考までに、tidyverseなどの使い方についての紹介は次の資料がおすすめです。\n\n私たちのR\n\nまた、以降の説明ではRでの日本語テキストの前処理のやり方のみにフォーカスしているため、具体的なテキストデータの分析のやり方には一切踏み込んでいません。Rでおこなうようなテキストデータの分析の方法については、いずれも英語の資料ですが、次が参考になると思います。\n\nText Mining with R\nSupervised Machine Learning for Text Analysis in R",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "index.html#rでテキストマイニングするということ",
    "href": "index.html#rでテキストマイニングするということ",
    "title": "RとMeCabによる日本語テキストマイニングの前処理",
    "section": "Rでテキストマイニングするということ",
    "text": "Rでテキストマイニングするということ\n\nテキストを分析して何がしたいのか\nテキストマイニングに関する入門的な本だと、「テキストマイニングとは何か」みたいな話から入るような気がします。ここでは必ずしも入門的な内容をめざしてはいませんが、しかし、すこし考えてみましょう。テキストマイニングとはなんでしょうか。\n自然言語処理というのは、まあいろいろと思想はあるでしょうが、総じて「テキストを機械的に処理してごにょごにょする」技術のことだと思います。自然言語処理界隈の論文などを眺めていると、その範囲はかなり広くて、文書要約から文書生成といったタスクまで含まれるようです。\nそのなかでもテキストマイニングというと、「テキストから特徴量をつくって何かを分析する」みたいな部分にフォーカスしてくるのではないでしょうか。\n素人考えですが、テキストマイニングとはしたがってデータ分析のことです。そのため、前提としてテキストを分析して何がしたいのか（＝何ができるのか）を見通しよくしておくと、嬉しいことが多い気がします。\n\n\nテキストマイニングでめざすこと・できること\nCRISP-DM (Cross-Industry Standard Process for Data Mining) は、IBMを中心としたコンソーシアムが提案したデータマイニングのための標準プロセスです。\nこれはデータ分析をビジネスに活かすことを念頭においてつくられた「課題ドリブン」なプロセスであるため、場合によってはそのまま採用できないかもしれませんが、こうした標準プロセスを押さえておくことは、分析プロセスを設計するうえで有用だと思います。\nCRISP-DMは以下の6つの段階（phases）を行ったり来たりすることで進められていきます。\n\nBusiness Understanding\nData Understanding\nData Preparation\nModeling\nEvaluation\nDeployment\n\nCRISP-DMはデータ分析を通じて達成したいことから分析をスタートしていく、ある意味でトップダウン的なプロセスです。しかし、データからの知見の発掘はそんなにトップダウン一直線にはうまくいかないものです。いわばボトムアップ的にも、段階を「行ったり来たり」しながら分析を進めるためには、データ分析でとれるカードをなんとなく把握しておく必要があります。\nこれも素人考えですが、私たちがデータ分析でとれるカードというのは、だいたい次の３つくらいのものです。\n\nモデルをつくって何かの回帰をする\nモデルをつくって何かの分類をする\nグループに分けて違いを評価する\n\nそのために、これらの落としどころに持ち込むためのテキストの特徴量をどうにかしてつくること（前処理）が、私たちが実際におこなうテキストマイニングの大きな部分を占めるように思います。\nそして、それらの特徴量は、テキストについて何かを数えた頻度または比率とそれらを変換したものだと思っておくとすっきりします。数を数える「何か」というのは、たとえば語だったり品詞だったり、それらのNgramだったり、その他のタグ付けされた情報だったりします。\n\n\nテキストマイニングの流れ\nテキストマイニングの大まかな流れは、イメージ的には、次のような感じになります。\n\n分析したいテキストをいっぱい集める\n\n\n分析して何がしたいか考える\nそのためにつくるべき特徴量を考える\n\n\n特徴量をつくる\n\n\n正規化などの文字列処理\nトークナイズ・ステミング・レメタイズ\n集計\n特徴量の変換や補完\n\n\n分析する\n\n\n特徴量をつかってデータ分析する\n得られた結果を評価する\n\n\n（必要に応じて）得られた知見を活かす\n\nこの資料では、この流れのなかでも、2にとくにフォーカスして、テキストの前処理のやり方を説明します。",
    "crumbs": [
      "はじめに"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  gibasaの基本的な使い方",
    "section": "",
    "text": "1.1 テキストデータ\nここでは、audubonパッケージに含まれているaudubon::polanoというデータを例にgibasaの基本的な使い方を説明していきます。このデータは、青空文庫で公開されている、宮沢賢治の「ポラーノの広場」という小説を、改行ごとにひとつの要素としてベクトルにしたものです。\nこのデータを、次のようなかたちのデータフレーム（tibble）にします。\ndat_txt &lt;-\n  tibble::tibble(\n    doc_id = seq_along(audubon::polano) |&gt; as.character(),\n    text = audubon::polano\n  ) |&gt;\n  dplyr::mutate(text = audubon::strj_normalize(text))\n\nstr(dat_txt)\n#&gt; tibble [899 × 2] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: chr [1:899] \"1\" \"2\" \"3\" \"4\" ...\n#&gt;  $ text  : chr [1:899] \"ポラーノの広場\" \"宮沢賢治\" \"前十七等官レオーノ・キュースト誌\" \"宮沢賢治訳述\" ...\nこのかたちのデータフレームは、Text Interchange Formats（TIF）という仕様を念頭においている形式です（ちなみに、このかたちのデータフレームはreadtextパッケージを使うと簡単に得ることができますが、readtextクラスのオブジェクトはdplyrと相性が悪いようなので、使う場合はdplyr::tibbleなどでtibbleにしてしまうことをおすすめします）。\nText Interchange Formats（TIF）は、2017年にrOpenSci Text Workshopで整備された、テキスト分析用のRパッケージのデザインパターンのようなものです。\nTIFでは、コーパス（corpus）、文書単語行列（dtm）、トークン（token）という3種類のオブジェクトの形式が定義されており、異なるパッケージ間で同様の形式を扱うようにすることで、複数のパッケージを通じて便利にテキスト分析を進められるようになっています。\n上のdat_txtは、文書の集合であるコーパスをデータフレームのかたちで保持したものです。この形式のデータフレームは、次のように、tidytextやtokenizersの関数にそのまま渡すことができます。なお、これらの形式のオブジェクトは、TIFの枠組みのなかではトークンと呼ばれます。\ndat_txt |&gt;\n  tidytext::unnest_tokens(token, text) |&gt;\n  head(4)\n#&gt; # A tibble: 4 × 2\n#&gt;   doc_id token   \n#&gt;   &lt;chr&gt;  &lt;chr&gt;   \n#&gt; 1 1      ポラーノ\n#&gt; 2 1      の      \n#&gt; 3 1      広場    \n#&gt; 4 2      宮沢\n\ndat_txt |&gt;\n  tokenizers::tokenize_words() |&gt;\n  head(4)\n#&gt; $`1`\n#&gt; [1] \"ポラーノ\" \"の\"       \"広場\"    \n#&gt; \n#&gt; $`2`\n#&gt; [1] \"宮沢\" \"賢治\"\n#&gt; \n#&gt; $`3`\n#&gt;  [1] \"前\"     \"十七\"   \"等\"     \"官\"     \"レ\"     \"オー\"   \"ノ\"     \"キュー\"\n#&gt;  [9] \"スト\"   \"誌\"    \n#&gt; \n#&gt; $`4`\n#&gt; [1] \"宮沢\" \"賢治\" \"訳述\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>gibasaの基本的な使い方</span>"
    ]
  },
  {
    "objectID": "intro.html#gibasaの使い方",
    "href": "intro.html#gibasaの使い方",
    "title": "1  gibasaの基本的な使い方",
    "section": "1.2 gibasaの使い方",
    "text": "1.2 gibasaの使い方\n\n1.2.1 tokenize\n前節で見たように、tokenizers::tokenize_wordsやこれを利用しているtidytext::unnest_tokensは、日本語のテキストであっても機械的にトークンのかたちに整形する（分かち書きする）ことができます。\ntokenizersパッケージの分かち書きは、内部的には、ICUのBoundary Analysisによるものです。この単語境界判定は、たとえば新聞記事のような、比較的整った文体の文章ではおおむね期待通り分かち書きがおこなわれ、また、日本語と英語などが混ざっている文章であってもとくに気にすることなく、高速に分かち書きできるという強みがあります。\nしかし、手元にある辞書に収録されている語の通りに分かち書きしたい場合や、品詞情報などがほしい場合には、やはりMeCabのような形態素解析器による分かち書きが便利なこともあります。\ngibasaは、そのようなケースにおいて、tidytext::unnest_tokensの代わりに使用できる機能を提供するために開発しているパッケージです。この機能はgibasa::tokenizeという関数として提供していて、次のように使うことができます。\n\ndat &lt;- gibasa::tokenize(dat_txt, text, doc_id)\nstr(dat)\n#&gt; tibble [26,849 × 5] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id     : Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ token_id   : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ...\n#&gt;  $ token      : chr [1:26849] \"ポラーノ\" \"の\" \"広場\" \"宮沢\" ...\n#&gt;  $ feature    : chr [1:26849] \"名詞,一般,*,*,*,*,*\" \"助詞,連体化,*,*,*,*,の,ノ,ノ\" \"名詞,一般,*,*,*,*,広場,ヒロバ,ヒロバ\" \"名詞,固有名詞,人名,姓,*,*,宮沢,ミヤザワ,ミヤザワ\" ...\n\n\n\n1.2.2 prettify\ngibasa::tokenizeの戻り値のデータフレームは、それぞれのトークンについて、MeCabから返される素性情報のすべてを含んでいるfeatureという列を持っています。\nMeCabから返される素性情報は、使用している辞書によって異なります。たとえば、IPA辞書やUniDic（2.1.2, aka unidic-lite）の素性は、次のような情報を持っています。\n\ngibasa::get_dict_features(\"ipa\")\n#&gt; [1] \"POS1\"        \"POS2\"        \"POS3\"        \"POS4\"        \"X5StageUse1\"\n#&gt; [6] \"X5StageUse2\" \"Original\"    \"Yomi1\"       \"Yomi2\"\ngibasa::get_dict_features(\"unidic26\")\n#&gt;  [1] \"POS1\"      \"POS2\"      \"POS3\"      \"POS4\"      \"cType\"     \"cForm\"    \n#&gt;  [7] \"lForm\"     \"lemma\"     \"orth\"      \"pron\"      \"orthBase\"  \"pronBase\" \n#&gt; [13] \"goshu\"     \"iType\"     \"iForm\"     \"fType\"     \"fForm\"     \"kana\"     \n#&gt; [19] \"kanaBase\"  \"form\"      \"formBase\"  \"iConType\"  \"fConType\"  \"aType\"    \n#&gt; [25] \"aConType\"  \"aModeType\"\n\nこうした素性情報をデータフレームの列にパースするには、gibasa::prettifyという関数を利用できます。\nデフォルトではすべての素性についてパースしますが、col_select引数に残したい列名を指定することにより、特定の素性情報だけをパースすることもできます。このかたちのデータフレームは、解析するテキストの文章量によっては、数十万から数百万くらいの行からなることもよくあります。そのような規模のデータフレームについて、いちいちすべての素性をパースしていると、それだけでメモリを余計に消費してしまいます。メモリの消費を抑えるためにも、なるべく後で必要な素性だけをこまめに指定することをおすすめします。\n\nstr(gibasa::prettify(dat))\n#&gt; tibble [26,849 × 13] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id     : Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ token_id   : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ...\n#&gt;  $ token      : chr [1:26849] \"ポラーノ\" \"の\" \"広場\" \"宮沢\" ...\n#&gt;  $ POS1       : chr [1:26849] \"名詞\" \"助詞\" \"名詞\" \"名詞\" ...\n#&gt;  $ POS2       : chr [1:26849] \"一般\" \"連体化\" \"一般\" \"固有名詞\" ...\n#&gt;  $ POS3       : chr [1:26849] NA NA NA \"人名\" ...\n#&gt;  $ POS4       : chr [1:26849] NA NA NA \"姓\" ...\n#&gt;  $ X5StageUse1: chr [1:26849] NA NA NA NA ...\n#&gt;  $ X5StageUse2: chr [1:26849] NA NA NA NA ...\n#&gt;  $ Original   : chr [1:26849] NA \"の\" \"広場\" \"宮沢\" ...\n#&gt;  $ Yomi1      : chr [1:26849] NA \"ノ\" \"ヒロバ\" \"ミヤザワ\" ...\n#&gt;  $ Yomi2      : chr [1:26849] NA \"ノ\" \"ヒロバ\" \"ミヤザワ\" ...\nstr(gibasa::prettify(dat, col_select = c(1, 2)))\n#&gt; tibble [26,849 × 6] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id     : Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ token_id   : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ...\n#&gt;  $ token      : chr [1:26849] \"ポラーノ\" \"の\" \"広場\" \"宮沢\" ...\n#&gt;  $ POS1       : chr [1:26849] \"名詞\" \"助詞\" \"名詞\" \"名詞\" ...\n#&gt;  $ POS2       : chr [1:26849] \"一般\" \"連体化\" \"一般\" \"固有名詞\" ...\nstr(gibasa::prettify(dat, col_select = c(\"POS1\", \"Original\")))\n#&gt; tibble [26,849 × 6] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id     : Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ token_id   : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ...\n#&gt;  $ token      : chr [1:26849] \"ポラーノ\" \"の\" \"広場\" \"宮沢\" ...\n#&gt;  $ POS1       : chr [1:26849] \"名詞\" \"助詞\" \"名詞\" \"名詞\" ...\n#&gt;  $ Original   : chr [1:26849] NA \"の\" \"広場\" \"宮沢\" ...\n\n\n\n1.2.3 pack\ngibasa::packという関数を使うと、トークンの形式のデータフレームから、各トークンを半角スペースで区切ったコーパスの形式のデータフレームにすることができます。\n\ndat_corpus &lt;- dat |&gt;\n  gibasa::pack()\n\nstr(dat_corpus)\n#&gt; tibble [899 × 2] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ text  : chr [1:899] \"ポラーノ の 広場\" \"宮沢 賢治\" \"前 十 七 等 官 レオーノ・キュースト 誌\" \"宮沢 賢治 訳述\" ...\n\nこのかたちのデータフレームはTIFに準拠していたため、他のパッケージと組み合わせて使うのに便利なことがあります。たとえば、このかたちから、次のようにtidytext::unnest_tokensと組み合わせて、もう一度トークンの形式のデータフレームに戻すことができます。\n\ndat_corpus |&gt;\n  tidytext::unnest_tokens(token, text, token = \\(x) {\n    strsplit(x, \" +\")\n  }) |&gt;\n  head(4)\n#&gt; # A tibble: 4 × 2\n#&gt;   doc_id token   \n#&gt;   &lt;fct&gt;  &lt;chr&gt;   \n#&gt; 1 1      ポラーノ\n#&gt; 2 1      の      \n#&gt; 3 1      広場    \n#&gt; 4 2      宮沢\n\nあるいは、次のようにquantedaと組み合わせて使うこともできます。\n\ndat_corpus |&gt;\n  quanteda::corpus() |&gt;\n  quanteda::tokens(what = \"fastestword\", remove_punct = FALSE)\n#&gt; Tokens consisting of 899 documents.\n#&gt; 1 :\n#&gt; [1] \"ポラーノ\" \"の\"       \"広場\"    \n#&gt; \n#&gt; 2 :\n#&gt; [1] \"宮沢\" \"賢治\"\n#&gt; \n#&gt; 3 :\n#&gt; [1] \"前\"                   \"十\"                   \"七\"                  \n#&gt; [4] \"等\"                   \"官\"                   \"レオーノ・キュースト\"\n#&gt; [7] \"誌\"                  \n#&gt; \n#&gt; 4 :\n#&gt; [1] \"宮沢\" \"賢治\" \"訳述\"\n#&gt; \n#&gt; 5 :\n#&gt;  [1] \"その\"     \"ころ\"     \"わたくし\" \"は\"       \"、\"       \"モリーオ\"\n#&gt;  [7] \"市\"       \"の\"       \"博物\"     \"局\"       \"に\"       \"勤め\"    \n#&gt; [ ... and 5 more ]\n#&gt; \n#&gt; 6 :\n#&gt;  [1] \"十\"   \"八\"   \"等\"   \"官\"   \"でし\" \"た\"   \"から\" \"役所\" \"の\"   \"なか\"\n#&gt; [11] \"でも\" \"、\"  \n#&gt; [ ... and 219 more ]\n#&gt; \n#&gt; [ reached max_ndoc ... 893 more documents ]\n\n\n\n1.2.4 lazy_dtなどと組み合わせて使う場合\ngibasa::prettifyはデータフレームにしか使えないため、data.tableなどと組み合わせて使う場合にはtidyr::separateを代わりに使ってください。\n\ndat_toks &lt;- dat |&gt;\n  dtplyr::lazy_dt() |&gt;\n  tidyr::separate(feature,\n    into = gibasa::get_dict_features(),\n    sep = \",\", extra = \"merge\", fill = \"right\"\n  ) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(Original == \"*\", token, Original),\n    token = stringr::str_c(token, POS1, POS2, sep = \"/\")\n  ) |&gt;\n  dplyr::select(doc_id, sentence_id, token_id, token) |&gt;\n  dplyr::as_tibble() |&gt;\n  dplyr::mutate(across(where(is.character), ~ dplyr::na_if(., \"*\")))\n\nstr(dat_toks)\n#&gt; tibble [26,849 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id     : Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ...\n#&gt;  $ token_id   : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ...\n#&gt;  $ token      : chr [1:26849] \"ポラーノ/名詞/一般\" \"の/助詞/連体化\" \"広場/名詞/一般\" \"宮沢/名詞/固有名詞\" ...",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>gibasaの基本的な使い方</span>"
    ]
  },
  {
    "objectID": "dtm.html",
    "href": "dtm.html",
    "title": "2  トークンの集計と文書単語行列への整形",
    "section": "",
    "text": "2.1 トークンの集計",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>トークンの集計と文書単語行列への整形</span>"
    ]
  },
  {
    "objectID": "dtm.html#トークンの集計",
    "href": "dtm.html#トークンの集計",
    "title": "2  トークンの集計と文書単語行列への整形",
    "section": "",
    "text": "2.1.1 品詞などにもとづくしぼりこみ\nトークンを簡単に集計するには、dplyrの関数群を利用するのが便利です。\nたとえば、集計に先立って特定のトークンを素性情報にもとづいて選択するにはdplyr::filterを使います。\n\ndat |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |&gt;\n  dplyr::filter(POS1 %in% c(\"名詞\", \"動詞\", \"形容詞\")) |&gt;\n  dplyr::slice_head(n = 30L) |&gt;\n  reactable::reactable(compact = TRUE)\n\n\n\n\n\n一方で、以下で紹介するようなトークンの再結合を後からやりたい場合には、この方法は適切ではありません。dplyr::filterを使うとデータフレーム中のトークンを抜き取ってしまうため、この操作をした後では、実際の文書のなかでは隣り合っていないトークンどうしが隣接しているように扱われてしまいます。\n品詞などの情報にもとづいてトークンを取捨選択しつつも、トークンの位置関係はとりあえず保持したいという場合には、gibasa::mute_tokensを使います。この関数は、条件にマッチしたトークンをNA_character_に置き換えます（reactableによる出力のなかでは空白として表示されています）。\n\ndat |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |&gt;\n  gibasa::mute_tokens(!POS1 %in% c(\"名詞\", \"動詞\", \"形容詞\")) |&gt;\n  dplyr::slice_head(n = 30L) |&gt;\n  reactable::reactable(compact = TRUE)\n\n\n\n\n\n\n\n2.1.2 品詞などにもとづくトークンの再結合\nトークンを集計する目的によっては、形態素解析された結果の単語では単位として短すぎることがあります。\nたとえば、IPA辞書では「小田急線」は「小田急（名詞・固有名詞）+線（名詞・接尾）」として解析され、「小田急線」という単語としては解析されません。このように、必ずしも直感的な解析結果がえられないことは、UniDicを利用している場合により頻繁に発生します。実際、UniDicでは「水族館」も「水族（名詞・普通名詞）+館（接尾辞・名詞的）」として解析されるなど、IPA辞書よりもかなり細かな単位に解析されます。\n\n# IPA辞書による解析の例\ngibasa::tokenize(c(\n  \"佐藤さんはそのとき小田急線で江の島水族館に向かっていた\",\n  \"秒速5センチメートルは新海誠が監督した映画作品\",\n  \"辛そうで辛くない少し辛いラー油の辛さ\"\n)) |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"POS2\", \"POS3\")) |&gt;\n  reactable::reactable(compact = TRUE)\n\n\n\n\n\n分析の関心によっては、こうした細かくなりすぎたトークンをまとめあげて、もっと長い単位の単語として扱えると便利かもしれません。\ngibasa::collapse_tokensを使うと、渡された条件にマッチする一連のトークンをまとめあげて、新しいトークンにすることができます。\n\ngibasa::tokenize(c(\n  \"佐藤さんはそのとき小田急線で江の島水族館に向かっていた\",\n  \"秒速5センチメートルは新海誠が監督した映画作品\",\n  \"辛そうで辛くない少し辛いラー油の辛さ\"\n)) |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"POS2\", \"POS3\")) |&gt;\n  gibasa::collapse_tokens(\n    (POS1 %in% c(\"名詞\", \"接頭詞\") &\n      !stringr::str_detect(token, \"^[あ-ン]+$\")) |\n      (POS1 %in% c(\"名詞\", \"形容詞\") &\n        POS2 %in% c(\"自立\", \"接尾\", \"数接続\"))\n  ) |&gt;\n  reactable::reactable(compact = TRUE)\n\n\n\n\n\nこの機能は強力ですが、条件を書くには、利用している辞書の品詞体系について理解している必要があります。また、機械的に処理しているにすぎないため、一部のトークンは、かえって意図しないかたちにまとめあげられてしまう場合があります。あるいは、機械学習の特徴量をつくるのが目的であるケースなどでは、単純にNgramを利用したほうが便利かもしれません。\n\n\n2.1.3 原形の集計\ndplyr::countでトークンを文書ごとに集計します。ここでは、IPA辞書の見出し語がある語については「原形（Original）」を、見出し語がない語（未知語）については表層形を数えています。\nMeCabは、未知語であっても品詞の推定をおこないますが、未知語の場合には「読み（Yomi1, Yomi2）」のような一部の素性については情報を返しません。このような未知語の素性については、prettifyした結果のなかでは、NA_character_になっていることに注意してください。\n\ndat_count &lt;- dat |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |&gt;\n  dplyr::filter(POS1 %in% c(\"名詞\", \"動詞\", \"形容詞\")) |&gt;\n  dplyr::mutate(\n    doc_id = forcats::fct_drop(doc_id),\n    token = dplyr::if_else(is.na(Original), token, Original)\n  ) |&gt;\n  dplyr::count(doc_id, token)\n\nstr(dat_count)\n#&gt; tibble [9,723 × 3] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: Factor w/ 876 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 2 3 3 3 3 3 3 ...\n#&gt;  $ token : chr [1:9723] \"ポラーノ\" \"広場\" \"宮沢\" \"賢治\" ...\n#&gt;  $ n     : int [1:9723] 1 1 1 1 1 1 1 1 1 1 ...",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>トークンの集計と文書単語行列への整形</span>"
    ]
  },
  {
    "objectID": "dtm.html#文書単語行列への整形",
    "href": "dtm.html#文書単語行列への整形",
    "title": "2  トークンの集計と文書単語行列への整形",
    "section": "2.2 文書単語行列への整形",
    "text": "2.2 文書単語行列への整形\nこうして集計した縦持ちの頻度表を横持ちにすると、いわゆる文書単語行列になります。\n\ndtm &lt;- dat_count |&gt;\n  tidyr::pivot_wider(\n    id_cols = doc_id,\n    names_from = token,\n    values_from = n,\n    values_fill = 0\n  )\n\ndim(dtm)\n#&gt; [1]  876 2173\n\nただし、このようにtidyr::pivot_widerで単純に横持ちにすることは、非常に大量の列を持つ巨大なデータフレームを作成することになるため、おすすめしません。文書単語行列を作成するには、tidytext::cast_sparseやtidytext::cast_dfmなどを使って、疎行列のオブジェクトにしましょう。\n\ndtm &lt;- dat_count |&gt;\n  tidytext::cast_sparse(doc_id, token, n)\n\ndim(dtm)\n#&gt; [1]  876 2172",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>トークンの集計と文書単語行列への整形</span>"
    ]
  },
  {
    "objectID": "ngram.html",
    "href": "ngram.html",
    "title": "3  N-gram",
    "section": "",
    "text": "3.1 dplyrを使ってNgramを数える方法\ndplyrを使って簡単にやる場合、次のようにすると2-gramを集計できます。\nbigram &lt;- gibasa::ngram_tokenizer(2)\n\ndat_ngram &lt;- dat |&gt;\n  gibasa::prettify(col_select = \"Original\") |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(Original), token, Original)\n  ) |&gt;\n  dplyr::reframe(token = bigram(token, sep = \"-\"), .by = doc_id) |&gt;\n  dplyr::count(doc_id, token)\n\nstr(dat_ngram)\n#&gt; tibble [24,398 × 3] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 3 3 3 3 3 3 4 ...\n#&gt;  $ token : chr [1:24398] \"の-広場\" \"ポラーノ-の\" \"宮沢-賢治\" \"レオーノ・キュースト-誌\" ...\n#&gt;  $ n     : int [1:24398] 1 1 1 1 1 1 1 1 1 1 ...\nなお、RMeCabでできるような「名詞-名詞」の2-gramだけを抽出したいといったケースでは、2-gramをつくる前に品詞でフィルタしてしまうと元の文書内におけるトークンの隣接関係を破壊してしまい、正しい2-gramを抽出することができません。そのようなことをしたい場合には、あらかじめ品詞のNgramもつくったうえで、後から品詞のNgramでフィルタします。\ndat_ngram &lt;- dat |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |&gt;\n  dplyr::mutate(\n    token = dplyr::if_else(is.na(Original), token, Original)\n  ) |&gt;\n  dplyr::reframe(\n    token = bigram(token, sep = \"-\"),\n    pos = bigram(POS1, sep = \"-\"), # 品詞のNgramをつくる\n    .by = doc_id\n  ) |&gt;\n  dplyr::filter(pos %in% c(\"名詞-名詞\")) |&gt; # 品詞のNgramでフィルタする\n  dplyr::count(doc_id, token)\n\nstr(dat_ngram)\n#&gt; tibble [890 × 3] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 2 3 3 3 3 3 4 4 5 5 ...\n#&gt;  $ token : chr [1:890] \"宮沢-賢治\" \"レオーノ・キュースト-誌\" \"七-等\" \"十-七\" ...\n#&gt;  $ n     : int [1:890] 1 1 1 1 1 1 1 1 1 1 ...",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>N-gram</span>"
    ]
  },
  {
    "objectID": "ngram.html#quantedaにngramを持ちこむ方法",
    "href": "ngram.html#quantedaにngramを持ちこむ方法",
    "title": "3  N-gram",
    "section": "3.2 quantedaにNgramを持ちこむ方法",
    "text": "3.2 quantedaにNgramを持ちこむ方法\ngibasa::packを使ってNgramの分かち書きをつくることもできます。この場合、次のようにquantedaの枠組みの中でNgramをトークンとして数えることで集計することができます。\n\ndat_ngram &lt;- dat |&gt;\n  gibasa::prettify(col_select = \"Original\") |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(Original), token, Original)) |&gt;\n  gibasa::pack(n = 2)\n\nstr(dat_ngram)\n#&gt; tibble [899 × 2] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ text  : chr [1:899] \"ポラーノ-の の-広場\" \"宮沢-賢治\" \"前-十 十-七 七-等 等-官 官-レオーノ・キュースト レオーノ・キュースト-誌\" \"宮沢-賢治 賢治-訳述\" ...\n\ndat_ngram |&gt;\n  quanteda::corpus() |&gt;\n  quanteda::tokens(what = \"fastestword\") |&gt;\n  quanteda::dfm()\n#&gt; Document-feature matrix of: 899 documents, 10,890 features (99.75% sparse) and 0 docvars.\n#&gt;     features\n#&gt; docs ポラーノ-の の-広場 宮沢-賢治 前-十 十-七 七-等 等-官\n#&gt;    1           1       1         0     0     0     0     0\n#&gt;    2           0       0         1     0     0     0     0\n#&gt;    3           0       0         0     1     1     1     1\n#&gt;    4           0       0         1     0     0     0     0\n#&gt;    5           0       0         0     0     0     0     0\n#&gt;    6           0       0         0     0     0     0     1\n#&gt;     features\n#&gt; docs 官-レオーノ・キュースト レオーノ・キュースト-誌 賢治-訳述\n#&gt;    1                       0                       0         0\n#&gt;    2                       0                       0         0\n#&gt;    3                       1                       1         0\n#&gt;    4                       0                       0         1\n#&gt;    5                       0                       0         0\n#&gt;    6                       0                       0         0\n#&gt; [ reached max_ndoc ... 893 more documents, reached max_nfeat ... 10,880 more features ]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>N-gram</span>"
    ]
  },
  {
    "objectID": "ngram.html#quantedaでngramを数える方法",
    "href": "ngram.html#quantedaでngramを数える方法",
    "title": "3  N-gram",
    "section": "3.3 quantedaでNgramを数える方法",
    "text": "3.3 quantedaでNgramを数える方法\nまた、quantedaの枠組みの中でNgramをつくりながら数えて集計することもできます。\n\ndat_ngram &lt;- dat |&gt;\n  gibasa::prettify(col_select = \"Original\") |&gt;\n  dplyr::mutate(token = dplyr::if_else(is.na(Original), token, Original)) |&gt;\n  gibasa::pack()\n\nstr(dat_ngram)\n#&gt; tibble [899 × 2] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ doc_id: Factor w/ 899 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ text  : chr [1:899] \"ポラーノ の 広場\" \"宮沢 賢治\" \"前 十 七 等 官 レオーノ・キュースト 誌\" \"宮沢 賢治 訳述\" ...\n\ndat_ngram |&gt;\n  quanteda::corpus() |&gt;\n  quanteda::tokens(what = \"fastestword\") |&gt;\n  quanteda::tokens_ngrams(n = 2) |&gt;\n  quanteda::dfm()\n#&gt; Document-feature matrix of: 899 documents, 10,890 features (99.75% sparse) and 0 docvars.\n#&gt;     features\n#&gt; docs ポラーノ_の の_広場 宮沢_賢治 前_十 十_七 七_等 等_官\n#&gt;    1           1       1         0     0     0     0     0\n#&gt;    2           0       0         1     0     0     0     0\n#&gt;    3           0       0         0     1     1     1     1\n#&gt;    4           0       0         1     0     0     0     0\n#&gt;    5           0       0         0     0     0     0     0\n#&gt;    6           0       0         0     0     0     0     1\n#&gt;     features\n#&gt; docs 官_レオーノ・キュースト レオーノ・キュースト_誌 賢治_訳述\n#&gt;    1                       0                       0         0\n#&gt;    2                       0                       0         0\n#&gt;    3                       1                       1         0\n#&gt;    4                       0                       0         1\n#&gt;    5                       0                       0         0\n#&gt;    6                       0                       0         0\n#&gt; [ reached max_ndoc ... 893 more documents, reached max_nfeat ... 10,880 more features ]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>N-gram</span>"
    ]
  },
  {
    "objectID": "weighting.html",
    "href": "weighting.html",
    "title": "4  単語頻度の重みづけ",
    "section": "",
    "text": "4.1 tidytextによる重みづけ\ntidytext::bind_tf_idfを使うと単語頻度からTF-IDFを算出することができます。\ndat_count |&gt;\n  tidytext::bind_tf_idf(token, doc_id, n) |&gt;\n  dplyr::slice_max(tf_idf, n = 5L)\n#&gt; # A tibble: 5 × 6\n#&gt;   doc_id token        n    tf   idf tf_idf\n#&gt;   &lt;fct&gt;  &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 139    去年         1     1  6.78   6.78\n#&gt; 2 880    ざあい       1     1  6.78   6.78\n#&gt; 3 255    あわてる     1     1  6.08   6.08\n#&gt; 4 713    こちら       1     1  6.08   6.08\n#&gt; 5 288    こいつ       1     1  5.39   5.39\ntidytextにおけるTFとIDFは、RMeCabにおけるそれとは採用している計算式が異なるため、計算結果が異なります。TFはRMeCabでは生の索引語頻度（tfの場合）ですが、tidytextでは文書内での相対頻度になります。また、IDFはRMeCabでは対数の底が2であるのに対して、tidytextでは底がexp(1)であるなどの違いがあります。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>単語頻度の重みづけ</span>"
    ]
  },
  {
    "objectID": "weighting.html#gibasaによる重みづけ",
    "href": "weighting.html#gibasaによる重みづけ",
    "title": "4  単語頻度の重みづけ",
    "section": "4.2 gibasaによる重みづけ",
    "text": "4.2 gibasaによる重みづけ\ngibasaはRMeCabにおける単語頻度の重みづけをtidytext::bind_tf_idfと同様のスタイルでおこなうことができる関数gibasa::bind_tf_idf2を提供しています。\nRMeCabは以下の単語頻度の重みづけをサポートしています。\n\n局所的重み（TF）\n\ntf（索引語頻度）\ntf2（対数化索引語頻度）\ntf3（２進重み）\n\n大域的重み（IDF）\n\nidf（文書頻度の逆数）\nidf2（大域的IDF）\nidf3（確率的IDF）\nidf4（エントロピー）\n\n正規化\n\nnorm（コサイン正規化）\n\n\ngibasaはこれらの重みづけを再実装しています。ただし、tf=\"tf\"はgibasaでは相対頻度になるため、RMeCabのweight=\"tf*idf\"に相当する出力を得るには、たとえば次のように計算します。\n\ndat_count |&gt;\n  gibasa::bind_tf_idf2(token, doc_id, n) |&gt;\n  dplyr::mutate(\n    tf_idf = n * idf\n  ) |&gt;\n  dplyr::slice_max(tf_idf, n = 5L)\n#&gt; # A tibble: 6 × 6\n#&gt;   doc_id token     n     tf   idf tf_idf\n#&gt;   &lt;fct&gt;  &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 825    勉強      6 0.0638  8.77   52.6\n#&gt; 2 826    力        6 0.0667  8.77   52.6\n#&gt; 3 822    勉強      5 0.0549  8.77   43.9\n#&gt; 4 715    の       11 0.103   3.74   41.2\n#&gt; 5 113    鞭        4 0.211   9.77   39.1\n#&gt; 6 826    得る      4 0.0444  9.77   39.1\n\nなお、注意点として、RMeCabの単語を数える機能は、品詞情報（POS1）を確認しながら単語を数えているようなので、ここでのように原形だけを見て数えた結果とは必ずしも一致しません。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>単語頻度の重みづけ</span>"
    ]
  },
  {
    "objectID": "weighting.html#udpipeによる重みづけ",
    "href": "weighting.html#udpipeによる重みづけ",
    "title": "4  単語頻度の重みづけ",
    "section": "4.3 udpipeによる重みづけ",
    "text": "4.3 udpipeによる重みづけ\nudpipeを使っても単語頻度とTF-IDFを算出できます。また、udpipe::document_term_frequencies_statisticsでは、TF、IDFとTF-IDFにくわえて、Okapi BM25を計算することができます。\nudpipe::document_term_frequencies_statisticsには、パラメータとしてkとbを渡すことができます。デフォルト値はそれぞれk=1.2、b=0.5です。kの値を大きくすると、単語の出現数の増加に対してBM25の値もより大きくなりやすくなります。 k=1.2というのは、Elasticsearchでもデフォルト値として採用されている値です。WikipediaやElasticsearchの技術記事によると、kは[1.2, 2.0]、b=.75とした場合に、多くのケースでよい結果が得られるとされています。\ndplyrを使っていればあまり意識する必要はないと思いますが、udpipeのこのあたりの関数の戻り値はdata.tableである点に注意してください。\n\nsuppressPackageStartupMessages(require(dplyr))\n\ndat |&gt;\n  gibasa::prettify(col_select = c(\"POS1\", \"Original\")) |&gt;\n  dplyr::filter(POS1 %in% c(\"名詞\", \"動詞\", \"形容詞\")) |&gt;\n  dplyr::mutate(\n    doc_id = forcats::fct_drop(doc_id),\n    token = dplyr::if_else(is.na(Original), token, Original)\n  ) |&gt;\n  udpipe::document_term_frequencies(document = \"doc_id\", term = \"token\") |&gt;\n  udpipe::document_term_frequencies_statistics(b = .75) |&gt;\n  dplyr::slice_max(tf_bm25, n = 5L)\n#&gt;     doc_id   term  freq        tf      idf    tf_idf  tf_bm25     bm25\n#&gt;     &lt;fctr&gt; &lt;char&gt; &lt;int&gt;     &lt;num&gt;    &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n#&gt;  1:    381   呑む     2 0.5000000 3.639872 1.8199359 1.699130 6.184616\n#&gt;  2:    398   決闘     2 0.5000000 4.829456 2.4147280 1.699130 8.205875\n#&gt;  3:    864   呑む     2 0.5000000 3.639872 1.8199359 1.699130 6.184616\n#&gt;  4:    859   呑む     3 0.3333333 3.639872 1.2132906 1.670247 6.079487\n#&gt;  5:     60     ん     2 0.4000000 1.900169 0.7600675 1.652923 3.140834\n#&gt;  6:     62     ん     2 0.4000000 1.900169 0.7600675 1.652923 3.140834\n#&gt;  7:    233     ん     2 0.4000000 1.900169 0.7600675 1.652923 3.140834\n#&gt;  8:    260   飛ぶ     2 0.4000000 5.165928 2.0663713 1.652923 8.538884\n#&gt;  9:    428   する     2 0.4000000 1.533619 0.6134476 1.652923 2.534955\n#&gt; 10:    521     君     2 0.4000000 4.578142 1.8312566 1.652923 7.567318\n#&gt; 11:    742     ん     2 0.4000000 1.900169 0.7600675 1.652923 3.140834",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>単語頻度の重みづけ</span>"
    ]
  },
  {
    "objectID": "weighting.html#tidyloによる重みづけ",
    "href": "weighting.html#tidyloによる重みづけ",
    "title": "4  単語頻度の重みづけ",
    "section": "4.4 tidyloによる重みづけ",
    "text": "4.4 tidyloによる重みづけ\nTF-IDFによる単語頻度の重みづけのモチベーションは、索引語のなかでも特定の文書だけに多く出現していて、ほかの文書ではそれほど出現しないような「注目に値する語」を調べることにあります。\nこうしたことを実現するための値として、tidyloパッケージでは「重み付きログオッズ（weighted log odds）」を計算することができます。\n\ndat_count |&gt;\n  tidylo::bind_log_odds(set = doc_id, feature = token, n = n) |&gt;\n  dplyr::filter(is.finite(log_odds_weighted)) |&gt;\n  dplyr::slice_max(log_odds_weighted, n = 5L)\n#&gt; # A tibble: 5 × 4\n#&gt;   doc_id token        n log_odds_weighted\n#&gt;   &lt;fct&gt;  &lt;chr&gt;    &lt;int&gt;             &lt;dbl&gt;\n#&gt; 1 536    する         1             117. \n#&gt; 2 430    する         1             105. \n#&gt; 3 824    わたくし     1             102. \n#&gt; 4 577    いる         1              94.5\n#&gt; 5 465    する         1              94.0\n\nここで用いているデータは小説を改行ごとに一つの文書と見なしていたため、中には次のような極端に短い文書が含まれています。こうした文書では、直観的にはそれほどレアには思われない単語についてもオッズが極端に高くなってしまっているように見えます。\n\ndat_txt |&gt;\n  dplyr::filter(doc_id %in% c(430, 536, 577, 824)) |&gt;\n  dplyr::pull(text)\n#&gt; [1] \"「承知しました。」\"                              \n#&gt; [2] \"「起訴するぞ。」\"                                \n#&gt; [3] \"「きっと遠くでございますわ。もし生きていれば。」\"\n#&gt; [4] \"わたくしは思わずはねあがりました。\"\n\nweighted log oddsについてはこの資料などを参照してください。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>単語頻度の重みづけ</span>"
    ]
  },
  {
    "objectID": "collocation.html",
    "href": "collocation.html",
    "title": "5  コロケーション",
    "section": "",
    "text": "5.1 文書内での共起\n共起関係を数える機能はgibasaには実装されていません。文書内での共起を簡単に数えるには、たとえば次のようにします。\ndat_fcm &lt;- dat_count |&gt;\n  tidytext::cast_dfm(doc_id, token, n) |&gt;\n  quanteda::fcm()\n\ndat_fcm\n#&gt; Feature co-occurrence matrix of: 2,172 by 2,172 features.\n#&gt;                       features\n#&gt; features               ポラーノ 広場 宮沢 賢治 レオーノ・キュースト 七 十 官 等\n#&gt;   ポラーノ                    5   52    0    0                    0  0  2  0  0\n#&gt;   広場                        0    5    0    0                    0  0  2  0  0\n#&gt;   宮沢                        0    0    0    2                    0  0  0  0  0\n#&gt;   賢治                        0    0    0    0                    0  0  0  0  0\n#&gt;   レオーノ・キュースト        0    0    0    0                    0  1  3  3  3\n#&gt;   七                          0    0    0    0                    0  0  9  1  1\n#&gt;   十                          0    0    0    0                    0  0  8  6  6\n#&gt;   官                          0    0    0    0                    0  0  0  0  5\n#&gt;   等                          0    0    0    0                    0  0  0  0  0\n#&gt;   誌                          0    0    0    0                    0  0  0  0  0\n#&gt;                       features\n#&gt; features               誌\n#&gt;   ポラーノ              0\n#&gt;   広場                  0\n#&gt;   宮沢                  0\n#&gt;   賢治                  0\n#&gt;   レオーノ・キュースト  1\n#&gt;   七                    1\n#&gt;   十                    1\n#&gt;   官                    1\n#&gt;   等                    1\n#&gt;   誌                    0\n#&gt; [ reached max_feat ... 2,162 more features, reached max_nfeat ... 2,162 more features ]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>コロケーション</span>"
    ]
  },
  {
    "objectID": "collocation.html#任意のウィンドウ内での共起",
    "href": "collocation.html#任意のウィンドウ内での共起",
    "title": "5  コロケーション",
    "section": "5.2 任意のウィンドウ内での共起",
    "text": "5.2 任意のウィンドウ内での共起\n\n5.2.1 共起の集計\nRMeCab::collocateのような任意のウィンドウの中での共起を集計するには、次のようにする必要があります。ここではwindowは前後5個のトークンを見るようにします。\n\ndat_corpus &lt;- dat |&gt;\n  gibasa::pack()\n\ndat_fcm &lt;- dat_corpus |&gt;\n  quanteda::corpus() |&gt;\n  quanteda::tokens(what = \"fastestword\") |&gt;\n  quanteda::fcm(context = \"window\", window = 5)\n\nこうすると、nodeについて共起しているtermとその頻度を確認できます。以下では、「わたくし」というnodeと共起しているtermで頻度が上位20までであるものを表示しています。\n\ndat_fcm &lt;- dat_fcm |&gt;\n  tidytext::tidy() |&gt;\n  dplyr::rename(node = document, term = term) |&gt;\n  dplyr::filter(node == \"わたくし\") |&gt;\n  dplyr::slice_max(count, n = 20)\n\ndat_fcm\n#&gt; # A tibble: 20 × 3\n#&gt;    node     term  count\n#&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;\n#&gt;  1 わたくし は      205\n#&gt;  2 わたくし 。      122\n#&gt;  3 わたくし た      110\n#&gt;  4 わたくし て       99\n#&gt;  5 わたくし 、       91\n#&gt;  6 わたくし まし     90\n#&gt;  7 わたくし を       62\n#&gt;  8 わたくし に       61\n#&gt;  9 わたくし が       51\n#&gt; 10 わたくし し       37\n#&gt; 11 わたくし も       35\n#&gt; 12 わたくし で       33\n#&gt; 13 わたくし ども     28\n#&gt; 14 わたくし と       23\n#&gt; 15 わたくし 」       23\n#&gt; 16 わたくし です     22\n#&gt; 17 わたくし へ       17\n#&gt; 18 わたくし い       15\n#&gt; 19 わたくし 「       15\n#&gt; 20 わたくし から     14\n\n\n\n5.2.2 T値やMI値の算出\nT値やMI値は、たとえば次のようにして計算できます。\nT値については「1.65」を越える場合、その共起が偶然ではないと考える大まかな目安となるそうです。また、MI値については「1.58」を越える場合に共起関係の大まかな目安となります（いずれの値についても「2」などを目安とする場合もあります）。\n\nntok &lt;- dat_corpus |&gt;\n  quanteda::corpus() |&gt;\n  quanteda::tokens(what = \"fastestword\") |&gt;\n  quanteda::ntoken() |&gt;\n  sum()\n\ntotal &lt;- dat_corpus |&gt;\n  quanteda::corpus() |&gt;\n  quanteda::tokens(what = \"fastestword\") |&gt;\n  quanteda::tokens_select(c(\"わたくし\", dat_fcm$term)) |&gt;\n  quanteda::dfm() |&gt;\n  quanteda::colSums()\n\ndat_fcm |&gt;\n  dplyr::select(-node) |&gt;\n  dplyr::mutate(\n    expect = total[term] / ntok * total[\"わたくし\"] * 5 * 2, ## 5はwindowのサイズ\n    t = (count - expect) / sqrt(count),\n    mi = log2(count / expect)\n  )\n#&gt; # A tibble: 20 × 5\n#&gt;    term  count expect       t      mi\n#&gt;    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 は      205  88.1    8.17   1.22  \n#&gt;  2 。      122 161.    -3.53  -0.400 \n#&gt;  3 た      110 108.     0.190  0.0264\n#&gt;  4 て       99 106.    -0.694 -0.0972\n#&gt;  5 、       91 102.    -1.13  -0.162 \n#&gt;  6 まし     90  64.1    2.73   0.489 \n#&gt;  7 を       62  67.4   -0.689 -0.121 \n#&gt;  8 に       61  68.8   -1.00  -0.174 \n#&gt;  9 が       51  51.9   -0.126 -0.0252\n#&gt; 10 し       37  24.1    2.11   0.616 \n#&gt; 11 も       35  31.4    0.615  0.158 \n#&gt; 12 で       33  34.7   -0.290 -0.0710\n#&gt; 13 ども     28   3.11   4.70   3.17  \n#&gt; 14 と       23  28.7   -1.18  -0.317 \n#&gt; 15 」       23  54.8   -6.63  -1.25  \n#&gt; 16 です     22  15.4    1.40   0.512 \n#&gt; 17 へ       17  16.5    0.114  0.0403\n#&gt; 18 い       15  17.4   -0.628 -0.217 \n#&gt; 19 「       15  56.2  -10.6   -1.91  \n#&gt; 20 から     14  20.4   -1.72  -0.546\n\n注意点として、quantedaは全角スペースなどをトークンとして数えないようなので、ここでの総語数（ntok）は、RMeCabの計算で使われる総語数よりも少なくなることがあります。RMeCabでの計算結果と概ね一致させたい場合は、総語数としてgibasa::tokenizeの戻り値の行数を使ってください。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>コロケーション</span>"
    ]
  },
  {
    "objectID": "sessioninfo.html",
    "href": "sessioninfo.html",
    "title": "6  セッション情報",
    "section": "",
    "text": "6.1 更新情報",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>セッション情報</span>"
    ]
  },
  {
    "objectID": "sessioninfo.html#更新情報",
    "href": "sessioninfo.html#更新情報",
    "title": "6  セッション情報",
    "section": "",
    "text": "2024-03-10\n\nサイトの設定を更新しました。内容に変更はありません\n\n2024-03-01\n\nbookdownを利用したサイトからQuarto Booksを利用したサイトに置き換えました。内容に変更はありません\n\n2024-02-28\n\n「Chapter 4 単語頻度の重みづけ」の「gibasaによる重みづけ」のコードに誤りがあったため、修正しました\n\n2024-02-17\n\n「Chapter 3 N-gram」にNgramを品詞でフィルタする場合の説明を追加しました\n「Chapter 4 単語頻度の重みづけ」の「コサイン正規化」についての説明を修正しました。gibasa v1.1.0からnorm=TRUE時の挙動をRMeCabと同じになるように変更したため、「（挙動の）細かな点が異なる」としていた説明を削除しました\n\n2023-12-13\n\n見た目を調整しました。内容に変更はありません\n\n2023-12-12\n\n「想定する知識など」に参考となる他の資料へのリンクを追加しました\nコードブロックの表示のされ方を調整しました\n\n2023-12-03\n\n「Chapter 4 単語頻度の重みづけ」の内容を更新しました\n「Chapter 7 Appendix」に「MeCabの辞書をビルドするには」という節を追加しました\n\n2023-08-02\n\n「tidytextによる重みづけ」についての記述に誤りがあったため、修正しました\n「Posit Cloud（旧・RStudio Cloud）でのgibasaの使い方」の内容を更新しました",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>セッション情報</span>"
    ]
  },
  {
    "objectID": "sessioninfo.html#セッション情報",
    "href": "sessioninfo.html#セッション情報",
    "title": "6  セッション情報",
    "section": "6.2 セッション情報",
    "text": "6.2 セッション情報\n\nsessioninfo::session_info()\n#&gt; ─ Session info ───────────────────────────────────────────────────────────────\n#&gt;  setting  value\n#&gt;  version  R version 4.4.0 (2024-04-24)\n#&gt;  os       Ubuntu 22.04.4 LTS\n#&gt;  system   x86_64, linux-gnu\n#&gt;  ui       X11\n#&gt;  language (EN)\n#&gt;  collate  ja_JP.UTF-8\n#&gt;  ctype    ja_JP.UTF-8\n#&gt;  tz       Asia/Tokyo\n#&gt;  date     2024-06-07\n#&gt;  pandoc   2.9.2.1 @ /usr/bin/ (via rmarkdown)\n#&gt; \n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package      * version    date (UTC) lib source\n#&gt;  audubon      * 0.5.2      2024-04-27 [1] https://paithiov909.r-universe.dev (R 4.4.0)\n#&gt;  cachem         1.1.0      2024-05-16 [1] CRAN (R 4.4.0)\n#&gt;  cli            3.6.2      2023-12-11 [1] RSPM (R 4.4.0)\n#&gt;  curl           5.2.1      2024-03-01 [1] RSPM (R 4.4.0)\n#&gt;  data.table     1.15.4     2024-03-30 [1] RSPM (R 4.4.0)\n#&gt;  digest         0.6.35     2024-03-11 [1] RSPM (R 4.4.0)\n#&gt;  dplyr        * 1.1.4      2023-11-17 [1] RSPM (R 4.4.0)\n#&gt;  dtplyr       * 1.3.1      2023-03-22 [1] RSPM (R 4.4.0)\n#&gt;  evaluate       0.23       2023-11-01 [1] RSPM (R 4.4.0)\n#&gt;  fansi          1.0.6      2023-12-08 [1] RSPM (R 4.4.0)\n#&gt;  fastmap        1.2.0      2024-05-15 [1] RSPM (R 4.4.0)\n#&gt;  fastmatch      1.1-4      2023-08-18 [1] RSPM (R 4.4.0)\n#&gt;  generics       0.1.3      2022-07-05 [1] RSPM (R 4.4.0)\n#&gt;  gibasa       * 1.1.0.9004 2024-04-25 [1] https://paithiov909.r-universe.dev (R 4.3.3)\n#&gt;  glue           1.7.0      2024-01-09 [1] RSPM (R 4.4.0)\n#&gt;  htmltools      0.5.8.1    2024-04-04 [1] RSPM (R 4.4.0)\n#&gt;  htmlwidgets    1.6.4      2023-12-06 [1] RSPM (R 4.4.0)\n#&gt;  janeaustenr    1.0.0      2022-08-26 [1] RSPM (R 4.4.0)\n#&gt;  jsonlite       1.8.8      2023-12-04 [1] RSPM (R 4.4.0)\n#&gt;  knitr          1.47       2024-05-29 [1] CRAN (R 4.4.0)\n#&gt;  lattice        0.22-5     2023-10-24 [4] CRAN (R 4.3.1)\n#&gt;  lifecycle      1.0.4      2023-11-07 [1] RSPM (R 4.4.0)\n#&gt;  magrittr       2.0.3      2022-03-30 [1] RSPM (R 4.4.0)\n#&gt;  Matrix         1.6-5      2024-01-11 [4] CRAN (R 4.3.3)\n#&gt;  memoise        2.0.1      2021-11-26 [1] RSPM (R 4.4.0)\n#&gt;  pillar         1.9.0      2023-03-22 [1] RSPM (R 4.4.0)\n#&gt;  pkgconfig      2.0.3      2019-09-22 [1] RSPM (R 4.4.0)\n#&gt;  purrr          1.0.2      2023-08-10 [1] RSPM (R 4.4.0)\n#&gt;  quanteda     * 4.0.2      2024-04-24 [1] CRAN (R 4.4.0)\n#&gt;  R.cache        0.16.0     2022-07-21 [1] RSPM (R 4.4.0)\n#&gt;  R.methodsS3    1.8.2      2022-06-13 [1] RSPM (R 4.4.0)\n#&gt;  R.oo           1.26.0     2024-01-24 [1] RSPM (R 4.4.0)\n#&gt;  R.utils        2.12.3     2023-11-18 [1] RSPM (R 4.4.0)\n#&gt;  R6             2.5.1      2021-08-19 [1] RSPM (R 4.4.0)\n#&gt;  Rcpp           1.0.12     2024-01-09 [1] RSPM (R 4.4.0)\n#&gt;  RcppParallel   5.1.7      2023-02-27 [1] RSPM (R 4.4.0)\n#&gt;  reactable    * 0.4.4      2023-03-12 [1] RSPM (R 4.4.0)\n#&gt;  rlang          1.1.4      2024-06-04 [1] RSPM (R 4.4.0)\n#&gt;  rmarkdown      2.27       2024-05-17 [1] CRAN (R 4.4.0)\n#&gt;  sessioninfo    1.2.2      2021-12-06 [1] RSPM (R 4.4.0)\n#&gt;  SnowballC      0.7.1      2023-04-25 [1] RSPM (R 4.4.0)\n#&gt;  stopwords      2.3        2021-10-28 [1] RSPM (R 4.4.0)\n#&gt;  stringi        1.8.4      2024-05-06 [1] CRAN (R 4.4.0)\n#&gt;  styler         1.10.3     2024-04-07 [1] RSPM (R 4.4.0)\n#&gt;  tibble         3.2.1      2023-03-20 [1] RSPM (R 4.4.0)\n#&gt;  tidylo       * 0.2.0      2022-03-22 [1] RSPM (R 4.4.0)\n#&gt;  tidyr        * 1.3.1      2024-01-24 [1] RSPM (R 4.4.0)\n#&gt;  tidyselect     1.2.1      2024-03-11 [1] RSPM (R 4.4.0)\n#&gt;  tidytext     * 0.4.2      2024-04-10 [1] RSPM (R 4.4.0)\n#&gt;  tokenizers     0.3.0      2022-12-22 [1] RSPM (R 4.4.0)\n#&gt;  udpipe       * 0.8.11     2023-01-06 [1] RSPM (R 4.4.0)\n#&gt;  utf8           1.2.4      2023-10-22 [1] RSPM (R 4.4.0)\n#&gt;  V8             4.4.2      2024-02-15 [1] RSPM (R 4.4.0)\n#&gt;  vctrs          0.6.5      2023-12-01 [1] RSPM (R 4.4.0)\n#&gt;  xfun           0.44       2024-05-15 [1] RSPM (R 4.4.0)\n#&gt;  yaml           2.3.8      2023-12-11 [1] RSPM (R 4.4.0)\n#&gt; \n#&gt;  [1] /home/paithiov909/R/x86_64-pc-linux-gnu-library/4.4\n#&gt;  [2] /usr/local/lib/R/site-library\n#&gt;  [3] /usr/lib/R/site-library\n#&gt;  [4] /usr/lib/R/library\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>セッション情報</span>"
    ]
  },
  {
    "objectID": "misc.html",
    "href": "misc.html",
    "title": "Appendix A — gibasa・MeCabの使い方",
    "section": "",
    "text": "A.1 Posit Cloud（旧・RStudio Cloud）でのgibasaの使い方\nRによるデータ分析を手軽に試したい場合には、Posit Cloud（旧・RStudio Cloud）のようなクラウド環境が便利かもしれません。\n一方で、Posit Cloudはユーザー権限しかない環境のため、gibasaを使えるようにするまでにはややコツが要ります。とはいえ、gibasaはRMeCabとは異なり、MeCabのバイナリはなくても使える（辞書とmecabrcがあればよい）ので、RMeCabを使う場合ほど複雑なことをする必要はないはずです。\nここでは、Posit Cloudでgibasaを利用できるようにするための手順を簡単に説明します（RMeCabもあわせて試したいという場合には、MeCabのバイナリを自分でビルドする必要があります。その場合はこの記事などを参考にしてください）。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>gibasa・MeCabの使い方</span>"
    ]
  },
  {
    "objectID": "misc.html#posit-cloud旧rstudio-cloudでのgibasaの使い方",
    "href": "misc.html#posit-cloud旧rstudio-cloudでのgibasaの使い方",
    "title": "Appendix A — gibasa・MeCabの使い方",
    "section": "",
    "text": "A.1.1 辞書（ipadic, unidic-lite）の配置\nMeCabの辞書は、Terminalタブからpipでインストールできます。ここでは、IPA辞書（ipadic）とunidic-liteをインストールします。\npython3 -m pip install ipadic unidic-lite\npython3 -c \"import ipadic; print('dicdir=' + ipadic.DICDIR);\" &gt; ~/.mecabrc\n\n\nA.1.2 gibasaのインストール\ngibasaをインストールします。\ninstall.packages(\"gibasa\")\n\n\nA.1.3 試すには\nうまくいっていると、辞書を指定しない場合はIPA辞書が使われます。unidic-liteはsys_dic引数にフルパスを指定することで使用できます。\ngibasa::tokenize(\"こんにちは\")\nunidic_lite &lt;- path.expand(\"~/.local/lib/python3.8/site-packages/unidic_lite/dicdir\")\ngibasa::tokenize(\"こんにちは\", sys_dic = unidic_lite) |&gt;\n  gibasa::prettify(into = gibasa::get_dict_features(\"unidic26\"))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>gibasa・MeCabの使い方</span>"
    ]
  },
  {
    "objectID": "misc.html#mecabの辞書をビルドするには",
    "href": "misc.html#mecabの辞書をビルドするには",
    "title": "Appendix A — gibasa・MeCabの使い方",
    "section": "A.2 MeCabの辞書をビルドするには",
    "text": "A.2 MeCabの辞書をビルドするには\nv1.0.1から、gibasaを使ってMeCabのシステム辞書やユーザー辞書をビルドできるようになりました。以下では、gibasaを使ってMeCabの辞書をビルドする方法を紹介します。\nMeCabの辞書は、各行が次のようなデータからなる「ヘッダーなしCSVファイル」を用意して、それらをもとに生成します。 ...の部分は見出し語の品詞情報で、ビルドしたい辞書によって異なります。IPA辞書の場合、品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音をこの通りの順番で記述します。\n表層形,左文脈ID,右文脈ID,コスト,...\n左文脈IDと右文脈IDは、品詞情報が正確に書かれている場合、空にしておくと自動で補完されます。 しかし、品詞情報を正確に書くには、おそらく当の左文脈IDと右文脈IDを含む出力を確認する必要があるため、ふつうに確認した値で埋めてしまったほうが確実です。\nここでは例として、「月ノ美兎」（ANYCOLOR社が運営する「にじさんじ」所属のVTuberの名前）という語彙を含む文をIPA辞書を使いつつ狙いどおりに解析してみましょう。\n\nA.2.1 必要な品詞情報を確認する\ngibasa::posDebugRcppは、与えられた文字列について、MeCabの-aオプションに相当する解析結果（解析結果になりえるすべての形態素の組み合わせ）を出力する関数です。 ここでの最適解（is_best == \"01\"）である結果について確認すると、「月ノ美兎」という語彙は次のように複数の形態素に分割されてしまっていることがわかります。\n\ngibasa::posDebugRcpp(\"月ノ美兎は箱の中\") |&gt;\n  dplyr::filter(is_best == \"01\")\n#&gt;    doc_id pos_id surface                                 feature stat lcAttr\n#&gt; 1       1      0                         BOS/EOS,*,*,*,*,*,*,*,*   02      0\n#&gt; 2       1     38      月          名詞,一般,*,*,*,*,月,ツキ,ツキ   00   1285\n#&gt; 3       1      4      ノ              記号,一般,*,*,*,*,ノ,ノ,ノ   00      5\n#&gt; 4       1     44      美  名詞,固有名詞,人名,名,*,*,美,ヨシ,ヨシ   00   1291\n#&gt; 5       1     38      兎      名詞,一般,*,*,*,*,兎,ウサギ,ウサギ   00   1285\n#&gt; 6       1     16      は            助詞,係助詞,*,*,*,*,は,ハ,ワ   00    261\n#&gt; 7       1     38      箱          名詞,一般,*,*,*,*,箱,ハコ,ハコ   00   1285\n#&gt; 8       1     24      の            助詞,連体化,*,*,*,*,の,ノ,ノ   00    368\n#&gt; 9       1     66      中 名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ   00   1313\n#&gt; 10      1      0                         BOS/EOS,*,*,*,*,*,*,*,*   03      0\n#&gt;    rcAttr     alpha      beta is_best prob wcost  cost\n#&gt; 1       0      0.00 -22144.50      01    0     0     0\n#&gt; 2    1285  -6190.50 -15954.00      01    1  8537  8254\n#&gt; 3       5  -8874.75 -13269.75      01    1  4929 11833\n#&gt; 4    1291 -13974.00  -8170.50      01    1  7885 18632\n#&gt; 5    1285 -18080.25  -4064.25      01    1  5290 24107\n#&gt; 6     261 -18095.25  -4049.25      01    1  3865 24127\n#&gt; 7    1285 -22729.50    585.00      01    1  6142 30306\n#&gt; 8     368 -23010.00    865.50      01    1  4816 30680\n#&gt; 9    1313 -24007.50   1863.00      01    1  6528 32010\n#&gt; 10      0 -22144.50      0.00      01    1     0 29526\n\nこのような語については、こちらのビネットで説明しているように、制約付き解析を使って次のように強制的に抽出することもできます。\n\ngibasa::posDebugRcpp(\"月ノ\\t*\\n美兎\\t*\\nは箱の中\", partial = TRUE)\n#&gt;   doc_id pos_id surface                                 feature stat lcAttr\n#&gt; 1      1      0                         BOS/EOS,*,*,*,*,*,*,*,*   02      0\n#&gt; 2      1     38    月ノ                     名詞,一般,*,*,*,*,*   01   1285\n#&gt; 3      1     38    美兎                     名詞,一般,*,*,*,*,*   01   1285\n#&gt; 4      1     16      は            助詞,係助詞,*,*,*,*,は,ハ,ワ   00    261\n#&gt; 5      1     38      箱          名詞,一般,*,*,*,*,箱,ハコ,ハコ   00   1285\n#&gt; 6      1     24      の            助詞,連体化,*,*,*,*,の,ノ,ノ   00    368\n#&gt; 7      1     66      中 名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ   00   1313\n#&gt; 8      1      0                         BOS/EOS,*,*,*,*,*,*,*,*   03      0\n#&gt;   rcAttr     alpha      beta is_best prob wcost  cost\n#&gt; 1      0      0.00 -19563.75      01    0     0     0\n#&gt; 2   1285  -6883.50 -12680.25      01    1  9461  9178\n#&gt; 3   1285 -15499.50  -4064.25      01    1 11426 20666\n#&gt; 4    261 -15514.50  -4049.25      01    1  3865 20686\n#&gt; 5   1285 -20148.75    585.00      01    1  6142 26865\n#&gt; 6    368 -20429.25    865.50      01    1  4816 27239\n#&gt; 7   1313 -21426.75   1863.00      01    1  6528 28569\n#&gt; 8      0 -19563.75      0.00      01    1     0 26085\n\n一方で、IPA辞書には、たとえば「早見」のような名詞,固有名詞,人名,姓,...という品詞と、「沙織」のような名詞,固有名詞,人名,名,...という品詞があります。 このような解析結果としてより望ましい品詞を確認するには、正しく解析させたい語（ここでは「月ノ美兎」）と同じような使われ方をする語（たとえば「早見沙織」）を実際に解析してみて、その結果を確認するとよいでしょう。\n\ngibasa::posDebugRcpp(\"早見沙織のラジオ番組\") |&gt;\n  dplyr::filter(is_best == \"01\")\n#&gt;   doc_id pos_id surface                                      feature stat\n#&gt; 1      1      0                              BOS/EOS,*,*,*,*,*,*,*,*   02\n#&gt; 2      1     43    早見 名詞,固有名詞,人名,姓,*,*,早見,ハヤミ,ハヤミ   00\n#&gt; 3      1     44    沙織 名詞,固有名詞,人名,名,*,*,沙織,サオリ,サオリ   00\n#&gt; 4      1     24      の                 助詞,連体化,*,*,*,*,の,ノ,ノ   00\n#&gt; 5      1     38  ラジオ       名詞,一般,*,*,*,*,ラジオ,ラジオ,ラジオ   00\n#&gt; 6      1     38    番組     名詞,一般,*,*,*,*,番組,バングミ,バングミ   00\n#&gt; 7      1      0                              BOS/EOS,*,*,*,*,*,*,*,*   03\n#&gt;   lcAttr rcAttr     alpha      beta is_best prob wcost  cost\n#&gt; 1      0      0      0.00 -10104.75      01    0     0     0\n#&gt; 2   1290   1290  -4362.75  -5742.00      01    1  7472  5817\n#&gt; 3   1291   1291  -5452.50  -4652.25      01    1  8462  7270\n#&gt; 4    368    368  -6658.50  -3446.25      01    1  4816  8878\n#&gt; 5   1285   1285  -7886.25  -2218.50      01    1  3942 10515\n#&gt; 6   1285   1285 -10534.50    429.75      01    1  3469 14046\n#&gt; 7      0      0 -10104.75      0.00      01    1     0 13473\n\nこの結果は狙いどおりのものであるため、「月ノ美兎」を正しく解析するために用意するCSVファイルは、仮に次のように作成しておくことができそうです。\n\nwriteLines(\n  c(\n    \"月ノ,1290,1290,7472,名詞,固有名詞,人名,姓,*,*,月ノ,ツキノ,ツキノ\",\n    \"美兎,1291,1291,8462,名詞,固有名詞,人名,名,*,*,美兎,ミト,ミト\"\n  ),\n  con = (csv_file &lt;- tempfile(fileext = \".csv\"))\n)\n\n\n\nA.2.2 ユーザー辞書のビルド\n試しに、ユーザー辞書をビルドしてみましょう。gibasaでユーザー辞書をビルドするには、gibasa::build_user_dicを使います。 ユーザー辞書をビルドするにはシステム辞書が必要なため、あらかじめシステム辞書（ここではIPA辞書）が適切に配置されていることを確認しておいてください。\n次のようにしてユーザー辞書をビルドできます。\n\ngibasa::build_user_dic(\n  dic_dir = stringr::str_sub(gibasa::dictionary_info()$file_path, end = -8),\n  file = (user_dic &lt;- tempfile(fileext = \".dic\")),\n  csv_file = csv_file,\n  encoding = \"utf8\"\n)\n#&gt; reading /tmp/RtmpMnG64K/file3812779ae1e4.csv ... 2\n#&gt; \n#&gt; done!\n\nなお、gibasaによる辞書のビルド時の注意点として、「MeCab: 単語の追加方法」で案内されている「コストの自動推定機能」はgibasaからは利用できません。 追加したい見出し語の生起コストは空にせず、必ず適当な値で埋めるようにしてください。\nさて、ビルドしたユーザー辞書を使ってみましょう。\n\ngibasa::dictionary_info(user_dic = user_dic)\n#&gt;                              file_path charset lsize rsize   size type version\n#&gt; 1    /var/lib/mecab/dic/debian/sys.dic   UTF-8  1316  1316 392127    0     102\n#&gt; 2 /tmp/RtmpMnG64K/file381272a02343.dic    utf8  1316  1316      2    1     102\ngibasa::tokenize(\"月ノ美兎は箱の中\", user_dic = user_dic)\n#&gt; # A tibble: 6 × 5\n#&gt;   doc_id sentence_id token_id token feature                                     \n#&gt;   &lt;fct&gt;        &lt;int&gt;    &lt;int&gt; &lt;chr&gt; &lt;chr&gt;                                       \n#&gt; 1 1                1        1 月ノ  名詞,固有名詞,人名,姓,*,*,月ノ,ツキノ,ツキノ\n#&gt; 2 1                1        2 美兎  名詞,固有名詞,人名,名,*,*,美兎,ミト,ミト    \n#&gt; 3 1                1        3 は    助詞,係助詞,*,*,*,*,は,ハ,ワ                \n#&gt; 4 1                1        4 箱    名詞,一般,*,*,*,*,箱,ハコ,ハコ              \n#&gt; 5 1                1        5 の    助詞,連体化,*,*,*,*,の,ノ,ノ                \n#&gt; 6 1                1        6 中    名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ\n\n狙いどおりに解析できているようです。\n\n\nA.2.3 生起コストを調整する\nここまでに紹介したようなやり方で辞書を整備することで、おおむね狙いどおりの解析結果を得られるようになると思われますが、 追加した語のかたちによっては、生起コストをより小さな値に調整しないと、一部の文において正しく切り出されない場合があるかもしれません。\nたとえば、こちらの記事で紹介されているように、 仮に高等学校,1285,1285,5078,名詞,一般,*,*,*,*,高等学校,コウトウガッコウ,コートーガッコーという見出し語を追加したとしても、 与える文によっては高等学校が狙いどおりに切り出されません。\n\nwriteLines(\n  c(\n    \"高等学校,1285,1285,5078,名詞,一般,*,*,*,*,高等学校,コウトウガッコウ,コートーガッコー\"\n  ),\n  con = (csv_file &lt;- tempfile(fileext = \".csv\"))\n)\ngibasa::build_user_dic(\n  dic_dir = stringr::str_sub(gibasa::dictionary_info()$file_path, end = -8),\n  file = (user_dic &lt;- tempfile(fileext = \".dic\")),\n  csv_file = csv_file,\n  encoding = \"utf8\"\n)\n#&gt; reading /tmp/RtmpMnG64K/file3812775f2351.csv ... 1\n#&gt; \n#&gt; done!\ngibasa::tokenize(\n  c(\n    \"九州高等学校ゴルフ選手権\",\n    \"地元の高等学校に進学した\",\n    \"帝京高等学校のエースとして活躍\",\n    \"開成高等学校117人が現役合格\",\n    \"マンガを高等学校の授業で使う\"\n  ),\n  user_dic = user_dic\n) |&gt;\n  gibasa::pack()\n#&gt; # A tibble: 5 × 2\n#&gt;   doc_id text                                \n#&gt;   &lt;fct&gt;  &lt;chr&gt;                               \n#&gt; 1 1      九州 高等 学校 ゴルフ 選手権        \n#&gt; 2 2      地元 の 高等 学校 に 進学 し た     \n#&gt; 3 3      帝京 高等 学校 の エース として 活躍\n#&gt; 4 4      開成 高等 学校 117 人 が 現役 合格  \n#&gt; 5 5      マンガ を 高等 学校 の 授業 で 使う\n\nこの例のように、複数の既存の見出し語のほうが優先されてしまう場合には、追加する見出し語の生起コストを小さくすることによって、 狙いどおりの解析結果を得ることができます。\n先ほどの記事のなかで紹介されているのと同じやり方で、適切な生起コストをgibasaを使って求めるには、たとえば次のような関数を用意します（やや複雑なのでバグがあるかもしれません）。\n\ncalc_adjusted_cost &lt;- \\(sentences, target_word, sys_dic = \"\", user_dic = \"\") {\n  sentences_mod &lt;-\n    stringi::stri_replace_all_regex(\n      sentences,\n      pattern = paste0(\"(?&lt;target&gt;(\", target_word, \"))\"),\n      replacement = \"\\n${target}\\t*\\n\",\n      vectorize_all = FALSE\n    )\n  calc_cumcost &lt;- \\(x) {\n    ret &lt;-\n      gibasa::posDebugRcpp(x, sys_dic = sys_dic, user_dic = user_dic, partial = TRUE) |&gt;\n      dplyr::mutate(\n        lcAttr = dplyr::lead(lcAttr, default = 0),\n        cost = purrr::map2_dbl(\n          rcAttr, lcAttr,\n          ~ gibasa::get_transition_cost(.x, .y, sys_dic = sys_dic, user_dic = user_dic)\n        ),\n        wcost = cumsum(wcost),\n        cost = cumsum(cost),\n        ## 1行目のBOS/EOS-&gt;BOS/EOS間の連接コストを足しすぎてしまうので、引く\n        total_cost = wcost + cost - gibasa::get_transition_cost(0, 0, sys_dic = sys_dic, user_dic = user_dic),\n        .by = doc_id\n      ) |&gt;\n      dplyr::slice_tail(n = 1, by = doc_id) |&gt;\n      dplyr::pull(\"total_cost\")\n    ret\n  }\n  cost1 &lt;- calc_cumcost(sentences)\n  cost2 &lt;- calc_cumcost(sentences_mod)\n\n  gibasa::posDebugRcpp(sentences_mod, sys_dic = sys_dic, user_dic = user_dic, partial = TRUE) |&gt;\n    dplyr::filter(surface %in% target_word) |&gt;\n    dplyr::reframe(\n      stat = stat,\n      surface = surface,\n      pos_id = pos_id,\n      feature = feature,\n      lcAttr = lcAttr,\n      rcAttr = rcAttr,\n      current_cost = wcost,\n      adjusted_cost = wcost + (cost1[doc_id] - cost2[doc_id] - 1)\n    ) |&gt;\n    dplyr::slice_min(adjusted_cost, n = 1, by = surface)\n}\n\nこの関数を使って、実際に適切な生起コストを計算してみます。\n\nadjusted_cost &lt;-\n  calc_adjusted_cost(\n    c(\n      \"九州高等学校ゴルフ選手権\",\n      \"地元の高等学校に進学した\",\n      \"帝京高等学校のエースとして活躍\",\n      \"開成高等学校117人が現役合格\",\n      \"マンガを高等学校の授業で使う\"\n    ),\n    target_word = \"高等学校\",\n    user_dic = user_dic\n  )\n\nこの生起コストを使って改めてユーザー辞書をビルドし、結果を確認してみましょう。\n\nadjusted_cost |&gt;\n  tidyr::unite(\n    csv_body,\n    surface, lcAttr, rcAttr, adjusted_cost, feature,\n    sep = \",\"\n  ) |&gt;\n  dplyr::pull(\"csv_body\") |&gt;\n  writeLines(con = (csv_file &lt;- tempfile(fileext = \".csv\")))\n\ngibasa::build_user_dic(\n  dic_dir = stringr::str_sub(gibasa::dictionary_info()$file_path, end = -8),\n  file = (user_dic &lt;- tempfile(fileext = \".dic\")),\n  csv_file = csv_file,\n  encoding = \"utf8\"\n)\n#&gt; reading /tmp/RtmpMnG64K/file38122ef5d94a.csv ... 1\n#&gt; \n#&gt; done!\n\ngibasa::tokenize(\n  c(\n    \"九州高等学校ゴルフ選手権\",\n    \"地元の高等学校に進学した\",\n    \"帝京高等学校のエースとして活躍\",\n    \"開成高等学校117人が現役合格\",\n    \"マンガを高等学校の授業で使う\"\n  ),\n  user_dic = user_dic\n) |&gt;\n  gibasa::pack()\n#&gt; # A tibble: 5 × 2\n#&gt;   doc_id text                               \n#&gt;   &lt;fct&gt;  &lt;chr&gt;                              \n#&gt; 1 1      九州 高等学校 ゴルフ 選手権        \n#&gt; 2 2      地元 の 高等学校 に 進学 し た     \n#&gt; 3 3      帝京 高等学校 の エース として 活躍\n#&gt; 4 4      開成 高等学校 117 人 が 現役 合格  \n#&gt; 5 5      マンガ を 高等学校 の 授業 で 使う\n\n今度はうまくいっていそうです。\n\n\nA.2.4 システム辞書のビルド\nユーザー辞書ではなく、システム辞書をビルドすることもできます。 ただ、ふつうに入手できるIPA辞書のソースの文字コードはEUC-JPであり、UTF-8のCSVファイルと混在させることができないため、扱いに注意が必要です。\nまた、UniDicの2.3.xについては、同梱されているファイルに問題があるようで、そのままではビルドできないという話があるようです（参考）。 UniDicはIPA辞書に比べるとビルドするのにそれなりにメモリが必要なことからも、とくに事情がないかぎりはビルド済みのバイナリ辞書をダウンロードしてきて使ったほうがよいでしょう。\n\nipadic_temp &lt;- tempfile(fileext = \".tar.gz\")\ndownload.file(\"https://github.com/shogo82148/mecab/releases/download/v0.996.9/mecab-ipadic-2.7.0-20070801.tar.gz\", destfile = ipadic_temp)\nuntar(ipadic_temp, exdir = tempdir())\n\ngibasa::build_sys_dic(\n  dic_dir = file.path(tempdir(), \"mecab-ipadic-2.7.0-20070801\"),\n  out_dir = tempdir(),\n  encoding = \"euc-jp\"\n)\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/unk.def ... 40\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Adverb.csv ... 3032\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Conjunction.csv ... 171\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Suffix.csv ... 1393\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Noun.adverbal.csv ... 795\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Noun.others.csv ... 151\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Noun.org.csv ... 16668\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Verb.csv ... 130750\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Noun.place.csv ... 72999\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Noun.csv ... 60477\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Adnominal.csv ... 135\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Noun.number.csv ... 42\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Noun.verbal.csv ... 12146\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Filler.csv ... 19\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Others.csv ... 2\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Noun.adjv.csv ... 3328\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Interjection.csv ... 252\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Postp-col.csv ... 91\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Noun.nai.csv ... 42\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Prefix.csv ... 221\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Noun.name.csv ... 34202\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Symbol.csv ... 208\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Adj.csv ... 27210\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Noun.demonst.csv ... 120\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Noun.proper.csv ... 27327\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Postp.csv ... 146\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/Auxil.csv ... 199\n#&gt; reading /tmp/RtmpMnG64K/mecab-ipadic-2.7.0-20070801/matrix.def ... 1316x1316\n#&gt; \n#&gt; done!\n\n# `dicrc`ファイルをビルドした辞書のあるディレクトリにコピーする\nfile.copy(file.path(tempdir(), \"mecab-ipadic-2.7.0-20070801/dicrc\"), tempdir())\n#&gt; [1] TRUE\n\n# ここでは`mecabrc`ファイルが適切な位置に配置されていないという想定で、\n# `mecabrc`ファイルを偽装している。\nwithr::with_envvar(\n  c(\n    \"MECABRC\" = if (.Platform$OS.type == \"windows\") {\n      \"nul\"\n    } else {\n      \"/dev/null\"\n    }\n  ),\n  gibasa::tokenize(\"月ノ美兎は箱の中\", sys_dic = tempdir())\n)\n#&gt; # A tibble: 8 × 5\n#&gt;   doc_id sentence_id token_id token feature                                \n#&gt;   &lt;fct&gt;        &lt;int&gt;    &lt;int&gt; &lt;chr&gt; &lt;chr&gt;                                  \n#&gt; 1 1                1        1 月    名詞,一般,*,*,*,*,月,ツキ,ツキ         \n#&gt; 2 1                1        2 ノ    記号,一般,*,*,*,*,ノ,ノ,ノ             \n#&gt; 3 1                1        3 美    名詞,固有名詞,人名,名,*,*,美,ヨシ,ヨシ \n#&gt; 4 1                1        4 兎    名詞,一般,*,*,*,*,兎,ウサギ,ウサギ     \n#&gt; 5 1                1        5 は    助詞,係助詞,*,*,*,*,は,ハ,ワ           \n#&gt; 6 1                1        6 箱    名詞,一般,*,*,*,*,箱,ハコ,ハコ         \n#&gt; 7 1                1        7 の    助詞,連体化,*,*,*,*,の,ノ,ノ           \n#&gt; 8 1                1        8 中    名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>gibasa・MeCabの使い方</span>"
    ]
  }
]