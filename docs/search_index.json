[["index.html", "RとMeCabによる日本語テキストマイニングの前処理 はじめに 0.1 この資料について 0.2 Rでテキストマイニングするということ", " RとMeCabによる日本語テキストマイニングの前処理 Akiru Kato 2023-08-01 はじめに 0.1 この資料について 0.1.1 この資料でやりたいこと gibasaやその他のRパッケージを使って、RMeCabでできるようなテキストマイニングの前処理をより見通しよくおこなうやり方を紹介します。 0.1.2 想定する知識など R言語の基本的な使い方の説明はしません。tidyverseなどの使い方については、他の資料を参照してください。 0.2 Rでテキストマイニングするということ 0.2.1 テキストを分析して何がしたいのか テキストマイニングに関する入門的な本だと、「テキストマイニングとは何か」みたいな話から入るような気がします。ここでは必ずしも入門的な内容をめざしてはいませんが、しかし、すこし考えてみましょう。テキストマイニングとはなんでしょうか。 自然言語処理というのは、まあいろいろと思想はあるでしょうが、総じて「テキストを機械的に処理してごにょごにょする」技術のことだと思います。自然言語処理界隈の論文などを眺めていると、その範囲はかなり広くて、文書要約から文書生成といったタスクまで含まれるようです。 そのなかでもテキストマイニングというと、「テキストから特徴量をつくって何かを分析する」みたいな部分にフォーカスしてくるのではないでしょうか。 素人考えですが、テキストマイニングとはしたがってデータ分析のことです。そのため、前提としてテキストを分析して何がしたいのか（＝何ができるのか）を見通しよくしておくと、嬉しいことが多い気がします。 0.2.2 テキストマイニングでめざすこと・できること CRISP-DM (Cross-Industry Standard Process for Data Mining) は、IBMを中心としたコンソーシアムが提案したデータマイニングのための標準プロセスです。 これはデータ分析をビジネスに活かすことを念頭においてつくられた「課題ドリブン」なプロセスであるため、場合によってはそのまま採用できないかもしれませんが、こうした標準プロセスを押さえておくことは、分析プロセスを設計するうえで有用だと思います。 CRISP-DMは以下の6つの段階（phases）を行ったり来たりすることで進められていきます。 Business Understanding Data Understanding Data Preparation Modeling Evaluation Deployment CRISP-DMはデータ分析を通じて達成したいことから分析をスタートしていく、ある意味でトップダウン的なプロセスです。しかし、データからの知見の発掘はそんなにトップダウン一直線にはうまくいかないものです。いわばボトムアップ的にも、段階を「行ったり来たり」しながら分析を進めるためには、データ分析でとれるカードをなんとなく把握しておく必要があります。 これも素人考えですが、私たちがデータ分析でとれるカードというのは、だいたい次の３つくらいのものです。 モデルをつくって何かの回帰をする モデルをつくって何かの分類をする グループに分けて違いを評価する そのために、これらの落としどころに持ち込むためのテキストの特徴量をどうにかしてつくること（前処理）が、私たちが実際におこなうテキストマイニングの大きな部分を占めるように思います。 そして、それらの特徴量は、テキストについて何かを数えた頻度または比率とそれらを変換したものだと思っておくとすっきりします。数を数える「何か」というのは、たとえば語だったり品詞だったり、それらのNgramだったり、その他のタグ付けされた情報だったりします。 0.2.3 テキストマイニングの流れ テキストマイニングの大まかな流れは、イメージ的には、次のような感じになります。 分析したいテキストをいっぱい集める 分析して何がしたいか考える そのためにつくるべき特徴量を考える 特徴量をつくる 正規化などの文字列処理 トークナイズ・ステミング・レメタイズ 集計 特徴量の変換や補完 分析する 特徴量をつかってデータ分析する 得られた結果を評価する （必要に応じて）得られた知見を活かす この資料では、この流れのなかでも、2にとくにフォーカスして、テキストの前処理のやり方を説明します。 "],["intro.html", "Chapter 1 gibasaの基本的な使い方 1.1 テキストデータ 1.2 gibasaの使い方", " Chapter 1 gibasaの基本的な使い方 1.1 テキストデータ ここでは、audubonパッケージに含まれているaudubon::polanoというデータを例にgibasaの基本的な使い方を説明していきます。このデータは、青空文庫で公開されている、宮沢賢治の「ポラーノの広場」という小説を、改行ごとにひとつの要素としてベクトルにしたものです。 このデータを、次のようなかたちのデータフレーム（tibble）にします。 dat_txt &lt;- tibble::tibble( doc_id = seq_along(audubon::polano) |&gt; as.character(), text = audubon::polano ) |&gt; dplyr::mutate(text = audubon::strj_normalize(text)) str(dat_txt) ## tibble [899 × 2] (S3: tbl_df/tbl/data.frame) ## $ doc_id: chr [1:899] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ text : chr [1:899] &quot;ポラーノの広場&quot; &quot;宮沢賢治&quot; &quot;前十七等官レオーノ・キュースト誌&quot; &quot;宮沢賢治訳述&quot; ... このかたちのデータフレームは、Text Interchange Formats（TIF）という仕様を念頭においている形式です（ちなみに、このかたちのデータフレームはreadtextパッケージを使うと簡単に得ることができますが、readtextクラスのオブジェクトはdplyrと相性が悪いようなので、使う場合はdplyr::tibbleなどでtibbleにしてしまうことをおすすめします）。 Text Interchange Formats（TIF）は、2017年にrOpenSci Text Workshopで整備された、テキスト分析用のRパッケージのデザインパターンのようなものです。 TIFでは、コーパス（corpus）、文書単語行列（dtm）、トークン（token）という3種類のオブジェクトの形式が定義されており、異なるパッケージ間で同様の形式を扱うようにすることで、複数のパッケージを通じて便利にテキスト分析を進められるようになっています。 上のdat_txtは、文書の集合であるコーパスをデータフレームのかたちで保持したものです。この形式のデータフレームは、次のように、tidytextやtokenizersの関数にそのまま渡すことができます。なお、これらの形式のオブジェクトは、TIFの枠組みのなかではトークンと呼ばれます。 dat_txt |&gt; tidytext::unnest_tokens(token, text) |&gt; head(4) ## # A tibble: 4 × 2 ## doc_id token ## &lt;chr&gt; &lt;chr&gt; ## 1 1 ポラーノ ## 2 1 の ## 3 1 広場 ## 4 2 宮沢 dat_txt |&gt; tokenizers::tokenize_words() |&gt; head(4) ## $`1` ## [1] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; ## ## $`2` ## [1] &quot;宮沢&quot; &quot;賢治&quot; ## ## $`3` ## [1] &quot;前&quot; &quot;十七&quot; &quot;等&quot; &quot;官&quot; ## [5] &quot;レ&quot; &quot;オー&quot; &quot;ノ&quot; &quot;キュー&quot; ## [9] &quot;スト&quot; &quot;誌&quot; ## ## $`4` ## [1] &quot;宮沢&quot; &quot;賢治&quot; &quot;訳述&quot; 1.2 gibasaの使い方 1.2.1 tokenize 前節で見たように、tokenizers::tokenize_wordsやこれを利用しているtidytext::unnest_tokensは、日本語のテキストであっても機械的にトークンのかたちに整形する（分かち書きする）ことができます。 tokenizersパッケージの分かち書きは、内部的には、ICUのBoundary Analysisによるものです。この単語境界判定は、たとえば新聞記事のような、比較的整った文体の文章ではおおむね期待通り分かち書きがおこなわれ、また、日本語と英語などが混ざっている文章であってもとくに気にすることなく、高速に分かち書きできるという強みがあります。 しかし、手元にある辞書に収録されている語の通りに分かち書きしたい場合や、品詞情報などがほしい場合には、やはりMeCabのような形態素解析器による分かち書きが便利なこともあります。 gibasaは、そのようなケースにおいて、tidytext::unnest_tokensの代わりに使用できる機能を提供するために開発しているパッケージです。この機能はgibasa::tokenizeという関数として提供していて、次のように使うことができます。 dat &lt;- gibasa::tokenize(dat_txt, text, doc_id) str(dat) ## tibble [26,849 × 5] (S3: tbl_df/tbl/data.frame) ## $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... ## $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... ## $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... ## $ token : chr [1:26849] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... ## $ feature : chr [1:26849] &quot;名詞,一般,*,*,*,*,*&quot; &quot;助詞,連体化,*,*,*,*,の,ノ,ノ&quot; &quot;名詞,一般,*,*,*,*,広場,ヒロバ,ヒロバ&quot; &quot;名詞,固有名詞,人名,姓,*,*,宮沢,ミヤザワ,ミヤザワ&quot; ... 1.2.2 prettify gibasa::tokenizeの戻り値のデータフレームは、それぞれのトークンについて、MeCabから返される素性情報のすべてを含んでいるfeatureという列を持っています。 MeCabから返される素性情報は、使用している辞書によって異なります。たとえば、IPA辞書やUniDic（2.1.2, aka unidic-lite）の素性は、次のような情報を持っています。 gibasa::get_dict_features(&quot;ipa&quot;) ## [1] &quot;POS1&quot; &quot;POS2&quot; ## [3] &quot;POS3&quot; &quot;POS4&quot; ## [5] &quot;X5StageUse1&quot; &quot;X5StageUse2&quot; ## [7] &quot;Original&quot; &quot;Yomi1&quot; ## [9] &quot;Yomi2&quot; gibasa::get_dict_features(&quot;unidic26&quot;) ## [1] &quot;POS1&quot; &quot;POS2&quot; &quot;POS3&quot; ## [4] &quot;POS4&quot; &quot;cType&quot; &quot;cForm&quot; ## [7] &quot;lForm&quot; &quot;lemma&quot; &quot;orth&quot; ## [10] &quot;pron&quot; &quot;orthBase&quot; &quot;pronBase&quot; ## [13] &quot;goshu&quot; &quot;iType&quot; &quot;iForm&quot; ## [16] &quot;fType&quot; &quot;fForm&quot; &quot;kana&quot; ## [19] &quot;kanaBase&quot; &quot;form&quot; &quot;formBase&quot; ## [22] &quot;iConType&quot; &quot;fConType&quot; &quot;aType&quot; ## [25] &quot;aConType&quot; &quot;aModeType&quot; こうした素性情報をデータフレームの列にパースするには、gibasa::prettifyという関数を利用できます。 デフォルトではすべての素性についてパースしますが、col_select引数に残したい列名を指定することにより、特定の素性情報だけをパースすることもできます。このかたちのデータフレームは、解析するテキストの文章量によっては、数十万から数百万くらいの行からなることもよくあります。そのような規模のデータフレームについて、いちいちすべての素性をパースしていると、それだけでメモリを余計に消費してしまいます。メモリの消費を抑えるためにも、なるべく後で必要な素性だけをこまめに指定することをおすすめします。 str(gibasa::prettify(dat)) ## tibble [26,849 × 13] (S3: tbl_df/tbl/data.frame) ## $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... ## $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... ## $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... ## $ token : chr [1:26849] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... ## $ POS1 : chr [1:26849] &quot;名詞&quot; &quot;助詞&quot; &quot;名詞&quot; &quot;名詞&quot; ... ## $ POS2 : chr [1:26849] &quot;一般&quot; &quot;連体化&quot; &quot;一般&quot; &quot;固有名詞&quot; ... ## $ POS3 : chr [1:26849] NA NA NA &quot;人名&quot; ... ## $ POS4 : chr [1:26849] NA NA NA &quot;姓&quot; ... ## $ X5StageUse1: chr [1:26849] NA NA NA NA ... ## $ X5StageUse2: chr [1:26849] NA NA NA NA ... ## $ Original : chr [1:26849] NA &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... ## $ Yomi1 : chr [1:26849] NA &quot;ノ&quot; &quot;ヒロバ&quot; &quot;ミヤザワ&quot; ... ## $ Yomi2 : chr [1:26849] NA &quot;ノ&quot; &quot;ヒロバ&quot; &quot;ミヤザワ&quot; ... str(gibasa::prettify(dat, col_select = c(1, 2))) ## tibble [26,849 × 6] (S3: tbl_df/tbl/data.frame) ## $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... ## $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... ## $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... ## $ token : chr [1:26849] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... ## $ POS1 : chr [1:26849] &quot;名詞&quot; &quot;助詞&quot; &quot;名詞&quot; &quot;名詞&quot; ... ## $ POS2 : chr [1:26849] &quot;一般&quot; &quot;連体化&quot; &quot;一般&quot; &quot;固有名詞&quot; ... str(gibasa::prettify(dat, col_select = c(&quot;POS1&quot;, &quot;Original&quot;))) ## tibble [26,849 × 6] (S3: tbl_df/tbl/data.frame) ## $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... ## $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... ## $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... ## $ token : chr [1:26849] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... ## $ POS1 : chr [1:26849] &quot;名詞&quot; &quot;助詞&quot; &quot;名詞&quot; &quot;名詞&quot; ... ## $ Original : chr [1:26849] NA &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... 1.2.3 pack gibasa::packという関数を使うと、トークンの形式のデータフレームから、各トークンを半角スペースで区切ったコーパスの形式のデータフレームにすることができます。 dat_corpus &lt;- dat |&gt; gibasa::pack() str(dat_corpus) ## tibble [899 × 2] (S3: tbl_df/tbl/data.frame) ## $ doc_id: Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ text : chr [1:899] &quot;ポラーノ の 広場&quot; &quot;宮沢 賢治&quot; &quot;前 十 七 等 官 レオーノ・キュースト 誌&quot; &quot;宮沢 賢治 訳述&quot; ... このかたちのデータフレームはTIFに準拠していたため、他のパッケージと組み合わせて使うのに便利なことがあります。たとえば、このかたちから、次のようにtidytext::unnest_tokensと組み合わせて、もう一度トークンの形式のデータフレームに戻すことができます。 dat_corpus |&gt; tidytext::unnest_tokens(token, text, token = \\(x) { strsplit(x, &quot; +&quot;) }) |&gt; head(4) ## # A tibble: 4 × 2 ## doc_id token ## &lt;fct&gt; &lt;chr&gt; ## 1 1 ポラーノ ## 2 1 の ## 3 1 広場 ## 4 2 宮沢 あるいは、次のようにquantedaと組み合わせて使うこともできます。 dat_corpus |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;, remove_punct = FALSE) ## Tokens consisting of 899 documents. ## 1 : ## [1] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; ## ## 2 : ## [1] &quot;宮沢&quot; &quot;賢治&quot; ## ## 3 : ## [1] &quot;前&quot; ## [2] &quot;十&quot; ## [3] &quot;七&quot; ## [4] &quot;等&quot; ## [5] &quot;官&quot; ## [6] &quot;レオーノ・キュースト&quot; ## [7] &quot;誌&quot; ## ## 4 : ## [1] &quot;宮沢&quot; &quot;賢治&quot; &quot;訳述&quot; ## ## 5 : ## [1] &quot;その&quot; &quot;ころ&quot; &quot;わたくし&quot; ## [4] &quot;は&quot; &quot;、&quot; &quot;モリーオ&quot; ## [7] &quot;市&quot; &quot;の&quot; &quot;博物&quot; ## [10] &quot;局&quot; &quot;に&quot; &quot;勤め&quot; ## [ ... and 5 more ] ## ## 6 : ## [1] &quot;十&quot; &quot;八&quot; &quot;等&quot; &quot;官&quot; &quot;でし&quot; ## [6] &quot;た&quot; &quot;から&quot; &quot;役所&quot; &quot;の&quot; &quot;なか&quot; ## [11] &quot;でも&quot; &quot;、&quot; ## [ ... and 219 more ] ## ## [ reached max_ndoc ... 893 more documents ] 1.2.4 lazy_dtなどと組み合わせて使う場合 gibasa::prettifyはデータフレームにしか使えないため、data.tableなどと組み合わせて使う場合にはtidyr::separateを代わりに使ってください。 dat_toks &lt;- dat |&gt; dtplyr::lazy_dt() |&gt; tidyr::separate(feature, into = gibasa::get_dict_features(), sep = &quot;,&quot;, extra = &quot;merge&quot;, fill = &quot;right&quot;) |&gt; dplyr::mutate( token = dplyr::if_else(Original == &quot;*&quot;, token, Original), token = stringr::str_c(token, POS1, POS2, sep = &quot;/&quot;) ) |&gt; dplyr::select(doc_id, sentence_id, token_id, token) |&gt; dplyr::as_tibble() |&gt; dplyr::mutate(across(where(is.character), ~ dplyr::na_if(., &quot;*&quot;))) str(dat_toks) ## tibble [26,849 × 4] (S3: tbl_df/tbl/data.frame) ## $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... ## $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... ## $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... ## $ token : chr [1:26849] &quot;ポラーノ/名詞/一般&quot; &quot;の/助詞/連体化&quot; &quot;広場/名詞/一般&quot; &quot;宮沢/名詞/固有名詞&quot; ... "],["dtm.html", "Chapter 2 トークンの集計と文書単語行列への整形 2.1 トークンの集計 2.2 文書単語行列への整形", " Chapter 2 トークンの集計と文書単語行列への整形 2.1 トークンの集計 2.1.1 品詞などにもとづくしぼりこみ トークンを簡単に集計するには、dplyrの関数群を利用するのが便利です。 たとえば、集計に先立って特定のトークンを素性情報にもとづいて選択するにはdplyr::filterを使います。 dat |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;Original&quot;)) |&gt; dplyr::filter(POS1 %in% c(&quot;名詞&quot;, &quot;動詞&quot;, &quot;形容詞&quot;)) |&gt; dplyr::slice_head(n = 30L) |&gt; reactable::reactable(compact = TRUE) 一方で、以下で紹介するようなトークンの再結合を後からやりたい場合には、この方法は適切ではありません。dplyr::filterを使うとデータフレーム中のトークンを抜き取ってしまうため、この操作をした後では、実際の文書のなかでは隣り合っていないトークンどうしが隣接しているように扱われてしまいます。 品詞などの情報にもとづいてトークンを取捨選択しつつも、トークンの位置関係はとりあえず保持したいという場合には、gibasa::mute_tokensを使います。この関数は、条件にマッチしたトークンをNA_character_に置き換えます（reactableによる出力のなかでは空白として表示されています）。 dat |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;Original&quot;)) |&gt; gibasa::mute_tokens(!POS1 %in% c(&quot;名詞&quot;, &quot;動詞&quot;, &quot;形容詞&quot;)) |&gt; dplyr::slice_head(n = 30L) |&gt; reactable::reactable(compact = TRUE) 2.1.2 品詞などにもとづくトークンの再結合 トークンを集計する目的によっては、形態素解析された結果の単語では単位として短すぎることがあります。 たとえば、IPA辞書では「小田急線」は「小田急（名詞・固有名詞）+線（名詞・接尾）」として解析され、「小田急線」という単語としては解析されません。このように、必ずしも直感的な解析結果がえられないことは、UniDicを利用している場合により頻繁に発生します。実際、UniDicでは「水族館」も「水族（名詞・普通名詞）+館（接尾辞・名詞的）」として解析されるなど、IPA辞書よりもかなり細かな単位に解析されます。 # IPA辞書による解析の例 gibasa::tokenize(c( &quot;佐藤さんはそのとき小田急線で江の島水族館に向かっていた&quot;, &quot;秒速5センチメートルは新海誠が監督した映画作品&quot;, &quot;辛そうで辛くない少し辛いラー油の辛さ&quot; )) |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;POS2&quot;, &quot;POS3&quot;)) |&gt; reactable::reactable(compact = TRUE) 分析の関心によっては、こうした細かくなりすぎたトークンをまとめあげて、もっと長い単位の単語として扱えると便利かもしれません。 gibasa::collapse_tokensを使うと、渡された条件にマッチする一連のトークンをまとめあげて、新しいトークンにすることができます。 gibasa::tokenize(c( &quot;佐藤さんはそのとき小田急線で江の島水族館に向かっていた&quot;, &quot;秒速5センチメートルは新海誠が監督した映画作品&quot;, &quot;辛そうで辛くない少し辛いラー油の辛さ&quot; )) |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;POS2&quot;, &quot;POS3&quot;)) |&gt; gibasa::collapse_tokens( (POS1 %in% c(&quot;名詞&quot;, &quot;接頭詞&quot;) &amp; !stringr::str_detect(token, &quot;^[あ-ン]+$&quot;)) | (POS1 %in% c(&quot;名詞&quot;, &quot;形容詞&quot;) &amp; POS2 %in% c(&quot;自立&quot;, &quot;接尾&quot;, &quot;数接続&quot;)) ) |&gt; reactable::reactable(compact = TRUE) この機能は強力ですが、条件を書くには、利用している辞書の品詞体系について理解している必要があります。また、機械的に処理しているにすぎないため、一部のトークンは、かえって意図しないかたちにまとめあげられてしまう場合があります。あるいは、機械学習の特徴量をつくるのが目的であるケースなどでは、単純にNgramを利用したほうが便利かもしれません。 2.1.3 原形の集計 dplyr::countでトークンを文書ごとに集計します。ここでは、IPA辞書の見出し語がある語については「原形（Original）」を、見出し語がない語（未知語）については表層形を数えています。 MeCabは、未知語であっても品詞の推定をおこないますが、未知語の場合には「読み（Yomi1, Yomi2）」のような一部の素性については情報を返しません。このような未知語の素性については、prettifyした結果のなかでは、NA_character_になっていることに注意してください。 dat_count &lt;- dat |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;Original&quot;)) |&gt; dplyr::filter(POS1 %in% c(&quot;名詞&quot;, &quot;動詞&quot;, &quot;形容詞&quot;)) |&gt; dplyr::mutate( doc_id = forcats::fct_drop(doc_id), token = dplyr::if_else(is.na(Original), token, Original) ) |&gt; dplyr::count(doc_id, token) str(dat_count) ## tibble [9,697 × 3] (S3: tbl_df/tbl/data.frame) ## $ doc_id: Factor w/ 876 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 2 2 3 3 3 3 3 3 ... ## $ token : chr [1:9697] &quot;ポラーノ&quot; &quot;広場&quot; &quot;宮沢&quot; &quot;賢治&quot; ... ## $ n : int [1:9697] 1 1 1 1 1 1 1 1 1 1 ... 2.2 文書単語行列への整形 こうして集計した縦持ちの頻度表を横持ちにすると、いわゆる文書単語行列になります。 dtm &lt;- dat_count |&gt; tidyr::pivot_wider( id_cols = doc_id, names_from = token, values_from = n, values_fill = 0 ) dim(dtm) ## [1] 876 2168 ただし、このようにtidyr::pivot_widerで単純に横持ちにすることは、非常に大量の列を持つ巨大なデータフレームを作成することになるため、おすすめしません。文書単語行列を作成するには、tidytext::cast_sparseやtidytext::cast_dfmなどを使って、疎行列のオブジェクトにしましょう。 dtm &lt;- dat_count |&gt; tidytext::cast_sparse(doc_id, token, n) dim(dtm) ## [1] 876 2167 "],["ngram.html", "Chapter 3 N-gram 3.1 dplyrを使ってNgramを数える方法 3.2 quantedaにNgramを持ちこむ方法 3.3 quantedaでNgramを数える方法", " Chapter 3 N-gram 3.1 dplyrを使ってNgramを数える方法 dplyrを使って簡単にやる場合、次のようにすると2-gramを集計できます。 bigram &lt;- audubon::ngram_tokenizer(2) dat_ngram &lt;- dat |&gt; gibasa::prettify(col_select = &quot;Original&quot;) |&gt; dplyr::mutate( token = dplyr::if_else(is.na(Original), token, Original) ) |&gt; dplyr::reframe(token = bigram(token, sep = &quot;-&quot;), .by = doc_id) |&gt; dplyr::count(doc_id, token) str(dat_ngram) ## tibble [24,398 × 3] (S3: tbl_df/tbl/data.frame) ## $ doc_id: Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 2 3 3 3 3 3 3 4 ... ## $ token : chr [1:24398] &quot;の-広場&quot; &quot;ポラーノ-の&quot; &quot;宮沢-賢治&quot; &quot;レオーノ・キュースト-誌&quot; ... ## $ n : int [1:24398] 1 1 1 1 1 1 1 1 1 1 ... 3.2 quantedaにNgramを持ちこむ方法 gibasa::packを使ってNgramの分かち書きをつくることもできます。この場合、次のようにquantedaの枠組みの中でNgramをトークンとして数えることで集計することができます。 dat_ngram &lt;- dat |&gt; gibasa::prettify(col_select = &quot;Original&quot;) |&gt; dplyr::mutate(token = dplyr::if_else(is.na(Original), token, Original)) |&gt; gibasa::pack(n = 2) str(dat_ngram) ## tibble [899 × 2] (S3: tbl_df/tbl/data.frame) ## $ doc_id: Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ text : chr [1:899] &quot;ポラーノ-の の-広場&quot; &quot;宮沢-賢治&quot; &quot;前-十 十-七 七-等 等-官 官-レオーノ・キュースト レオーノ・キュースト-誌&quot; &quot;宮沢-賢治 賢治-訳述&quot; ... dat_ngram |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::dfm() ## Document-feature matrix of: 899 documents, 10,890 features (99.75% sparse) and 0 docvars. ## features ## docs ポラーノ-の の-広場 宮沢-賢治 前-十 ## 1 1 1 0 0 ## 2 0 0 1 0 ## 3 0 0 0 1 ## 4 0 0 1 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## features ## docs 十-七 七-等 等-官 ## 1 0 0 0 ## 2 0 0 0 ## 3 1 1 1 ## 4 0 0 0 ## 5 0 0 0 ## 6 0 0 1 ## features ## docs 官-レオーノ・キュースト ## 1 0 ## 2 0 ## 3 1 ## 4 0 ## 5 0 ## 6 0 ## features ## docs レオーノ・キュースト-誌 賢治-訳述 ## 1 0 0 ## 2 0 0 ## 3 1 0 ## 4 0 1 ## 5 0 0 ## 6 0 0 ## [ reached max_ndoc ... 893 more documents, reached max_nfeat ... 10,880 more features ] 3.3 quantedaでNgramを数える方法 また、quantedaの枠組みの中でNgramをつくりながら数えて集計することもできます。 dat_ngram &lt;- dat |&gt; gibasa::prettify(col_select = &quot;Original&quot;) |&gt; dplyr::mutate(token = dplyr::if_else(is.na(Original), token, Original)) |&gt; gibasa::pack() str(dat_ngram) ## tibble [899 × 2] (S3: tbl_df/tbl/data.frame) ## $ doc_id: Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ text : chr [1:899] &quot;ポラーノ の 広場&quot; &quot;宮沢 賢治&quot; &quot;前 十 七 等 官 レオーノ・キュースト 誌&quot; &quot;宮沢 賢治 訳述&quot; ... dat_ngram |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::tokens_ngrams(n = 2) |&gt; quanteda::dfm() ## Document-feature matrix of: 899 documents, 10,890 features (99.75% sparse) and 0 docvars. ## features ## docs ポラーノ_の の_広場 宮沢_賢治 前_十 ## 1 1 1 0 0 ## 2 0 0 1 0 ## 3 0 0 0 1 ## 4 0 0 1 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## features ## docs 十_七 七_等 等_官 ## 1 0 0 0 ## 2 0 0 0 ## 3 1 1 1 ## 4 0 0 0 ## 5 0 0 0 ## 6 0 0 1 ## features ## docs 官_レオーノ・キュースト ## 1 0 ## 2 0 ## 3 1 ## 4 0 ## 5 0 ## 6 0 ## features ## docs レオーノ・キュースト_誌 賢治_訳述 ## 1 0 0 ## 2 0 0 ## 3 1 0 ## 4 0 1 ## 5 0 0 ## 6 0 0 ## [ reached max_ndoc ... 893 more documents, reached max_nfeat ... 10,880 more features ] "],["weighting.html", "Chapter 4 単語頻度の重みづけ 4.1 tidytextによる重みづけ 4.2 gibasaによる重みづけ 4.3 udpipeによる重みづけ 4.4 tidyloによる重みづけ", " Chapter 4 単語頻度の重みづけ 4.1 tidytextによる重みづけ tidytext::bind_tf_idfを使うと単語頻度からTF-IDFを算出することができます。 dat_count |&gt; tidytext::bind_tf_idf(token, doc_id, n) |&gt; dplyr::slice_max(tf_idf, n = 5L) ## # A tibble: 5 × 6 ## doc_id token n tf idf tf_idf ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 139 去年 1 1 6.78 6.78 ## 2 880 ざあい 1 1 6.78 6.78 ## 3 255 あわてる 1 1 6.08 6.08 ## 4 713 こちら 1 1 6.08 6.08 ## 5 288 こいつ 1 1 5.39 5.39 tidytextにおけるTFとIDFは、RMeCabにおけるそれとは採用している計算式が異なるため、計算結果が異なります。TFはRMeCabでは生の索引語頻度（tfの場合）ですが、tidytextでは文書内での相対頻度になります。また、IDFはRMeCabでは対数の底が2であるのに対して、tidytextでは底がexp(1)であるなどの違いがあります。 4.2 gibasaによる重みづけ gibasaはRMeCabにおける単語頻度の重みづけをtidytext::bind_tf_idfと同様のスタイルでおこなうことができる関数gibasa::bind_tf_idf2を提供しています。 bind_tf_idf2はおおむねRMeCabの単語頻度の重みづけの挙動を再現していますが、細かな点が異なります。たとえば、gibasaは「コサイン正規化」がIDFの計算時のみにおこなわれるため、norm=TRUEにしてもRMeCabの計算結果とは一致しません。 RMeCabは以下の単語頻度の重みづけをサポートしています。 局所的重み（TF） tf（索引語頻度） tf2（対数化索引語頻度） tf3（２進重み） 大域的重み（IDF） idf（文書頻度の逆数） idf2（大域的IDF） idf3（確率的IDF） idf4（エントロピー） 正規化 norm（コサイン正規化） gibasaはこれらの重みづけを再実装しています。ただし、tf=\"tf\"はgibasaでは相対頻度になるため、RMeCabのweight=\"tf*idf\"に相当する出力を得るには、たとえば次のように計算します。 dat_count |&gt; gibasa::bind_tf_idf2(token, doc_id, n) |&gt; dplyr::mutate( tf_idf = n / idf ) |&gt; dplyr::slice_max(tf_idf, n = 5L) ## # A tibble: 8 × 6 ## doc_id token n tf idf tf_idf ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 715 の 11 0.103 3.74 2.94 ## 2 732 いる 8 0.0842 3.10 2.58 ## 3 715 わたくし 8 0.0748 3.32 2.41 ## 4 825 いる 6 0.0638 3.10 1.94 ## 5 615 する 6 0.0408 3.21 1.87 ## 6 715 する 6 0.0561 3.21 1.87 ## 7 822 する 6 0.0659 3.21 1.87 ## 8 825 する 6 0.0638 3.21 1.87 なお、注意点として、RMeCabの単語を数える機能は、品詞情報（POS1とPOS2まで）を確認しながら単語を数えているようなので、ここでのように原形だけを見て数えた結果とは必ずしも一致しません。 4.3 udpipeによる重みづけ udpipeを使っても単語頻度とTF-IDFを算出できます。また、udpipe::document_term_frequencies_statisticsでは、TF、IDFとTF-IDFにくわえて、Okapi BM25を計算することができます。 dplyrを使っていればあまり意識する必要はないと思いますが、udpipeのこのあたりの関数の戻り値はdata.tableである点に注意してください。 suppressPackageStartupMessages(require(dplyr)) dat |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;Original&quot;)) |&gt; dplyr::filter(POS1 %in% c(&quot;名詞&quot;, &quot;動詞&quot;, &quot;形容詞&quot;)) |&gt; dplyr::mutate( doc_id = forcats::fct_drop(doc_id), token = dplyr::if_else(is.na(Original), token, Original) ) |&gt; udpipe::document_term_frequencies(document = &quot;doc_id&quot;, term = &quot;token&quot;) |&gt; udpipe::document_term_frequencies_statistics(k = 2.0) |&gt; dplyr::slice_max(bm25, n = 5L) ## doc_id term freq tf idf ## 1: 698 誤解 2 0.3333333 6.775366 ## 2: 139 去年 1 1.0000000 6.775366 ## 3: 880 ざあい 1 1.0000000 6.775366 ## 4: 87 お父さん 1 0.5000000 6.775366 ## 5: 89 兄さん 1 0.5000000 6.775366 ## 6: 536 起訴 1 0.5000000 6.775366 ## 7: 790 休む 1 0.5000000 6.775366 ## tf_idf tf_bm25 bm25 ## 1: 2.258455 1.860062 12.60260 ## 2: 6.775366 1.850767 12.53962 ## 3: 6.775366 1.850767 12.53962 ## 4: 3.387683 1.722257 11.66892 ## 5: 3.387683 1.722257 11.66892 ## 6: 3.387683 1.722257 11.66892 ## 7: 3.387683 1.722257 11.66892 4.4 tidyloによる重みづけ TF-IDFによる単語頻度の重みづけのモチベーションは、索引語のなかでも特定の文書だけに多く出現していて、ほかの文書ではそれほど出現しないような「レアな単語」を調べることにあります。 こうしたことを実現するための値として、tidyloパッケージでは「重み付きログオッズ（weighted log odds）」を計算することができます。 dat_count |&gt; tidylo::bind_log_odds(set = doc_id, feature = token, n = n) |&gt; dplyr::filter(!is.infinite(log_odds_weighted)) |&gt; dplyr::slice_max(log_odds_weighted, n = 5L) ## # A tibble: 5 × 4 ## doc_id token n log_odds_weighted ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 536 する 1 117. ## 2 430 する 1 105. ## 3 824 わたくし 1 102. ## 4 577 いる 1 94.5 ## 5 465 する 1 94.0 ここで用いているデータは小説を改行ごとに一つの文書と見なしていたため、中には次のような極端に短い文書が含まれています。こうした文書では、直観的にはそれほどレアには思われない単語についてもオッズが極端に高くなってしまっているように見えます。 dat_txt |&gt; dplyr::filter(doc_id %in% c(430, 536, 577, 824)) |&gt; dplyr::pull(text) ## [1] &quot;「承知しました。」&quot; ## [2] &quot;「起訴するぞ。」&quot; ## [3] &quot;「きっと遠くでございますわ。もし生きていれば。」&quot; ## [4] &quot;わたくしは思わずはねあがりました。&quot; weighted log oddsについてはこの資料などを参照してください。 "],["collocation.html", "Chapter 5 コロケーション 5.1 文書内での共起 5.2 任意のウィンドウ内での共起", " Chapter 5 コロケーション 5.1 文書内での共起 共起関係を数える機能はgibasaには実装されていません。文書内での共起を簡単に数えるには、たとえば次のようにします。 dat_fcm &lt;- dat_count |&gt; tidytext::cast_dfm(doc_id, token, n) |&gt; quanteda::fcm() dat_fcm ## Feature co-occurrence matrix of: 2,167 by 2,167 features. ## features ## features ポラーノ 広場 宮沢 ## ポラーノ 5 52 0 ## 広場 0 5 0 ## 宮沢 0 0 0 ## 賢治 0 0 0 ## レオーノ・キュースト 0 0 0 ## 七 0 0 0 ## 十 0 0 0 ## 官 0 0 0 ## 等 0 0 0 ## 誌 0 0 0 ## features ## features 賢治 ## ポラーノ 0 ## 広場 0 ## 宮沢 2 ## 賢治 0 ## レオーノ・キュースト 0 ## 七 0 ## 十 0 ## 官 0 ## 等 0 ## 誌 0 ## features ## features レオーノ・キュースト ## ポラーノ 0 ## 広場 0 ## 宮沢 0 ## 賢治 0 ## レオーノ・キュースト 0 ## 七 0 ## 十 0 ## 官 0 ## 等 0 ## 誌 0 ## features ## features 七 十 官 等 誌 ## ポラーノ 0 2 0 0 0 ## 広場 0 2 0 0 0 ## 宮沢 0 0 0 0 0 ## 賢治 0 0 0 0 0 ## レオーノ・キュースト 1 3 3 3 1 ## 七 0 9 1 1 1 ## 十 0 8 6 6 1 ## 官 0 0 0 5 1 ## 等 0 0 0 0 1 ## 誌 0 0 0 0 0 ## [ reached max_feat ... 2,157 more features, reached max_nfeat ... 2,157 more features ] 5.2 任意のウィンドウ内での共起 5.2.1 共起の集計 RMeCab::collocateのような任意のウィンドウの中での共起を集計するには、次のようにする必要があります。ここではwindowは前後5個のトークンを見るようにします。 dat_corpus &lt;- dat |&gt; gibasa::pack() dat_fcm &lt;- dat_corpus |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::fcm(context = &quot;window&quot;, window = 5) こうすると、nodeについて共起しているtermとその頻度を確認できます。以下では、「わたくし」というnodeと共起しているtermで頻度が上位20までであるものを表示しています。 dat_fcm &lt;- dat_fcm |&gt; tidytext::tidy() |&gt; dplyr::rename(node = document, term = term) |&gt; dplyr::filter(node == &quot;わたくし&quot;) |&gt; dplyr::slice_max(count, n = 20) dat_fcm ## # A tibble: 20 × 3 ## node term count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 わたくし は 205 ## 2 わたくし 。 122 ## 3 わたくし た 110 ## 4 わたくし て 99 ## 5 わたくし 、 91 ## 6 わたくし まし 90 ## 7 わたくし を 62 ## 8 わたくし に 61 ## 9 わたくし が 51 ## 10 わたくし し 37 ## 11 わたくし も 35 ## 12 わたくし で 33 ## 13 わたくし ども 28 ## 14 わたくし と 23 ## 15 わたくし 」 23 ## 16 わたくし です 22 ## 17 わたくし へ 17 ## 18 わたくし い 15 ## 19 わたくし 「 15 ## 20 わたくし から 14 5.2.2 T値やMI値の算出 T値やMI値は、たとえば次のようにして計算できます。 T値については「1.65」を越える場合、その共起が偶然ではないと考える大まかな目安となるそうです。また、MI値については「1.58」を越える場合に共起関係の大まかな目安となります（いずれの値についても「2」などを目安とする場合もあります）。 ntok &lt;- dat_corpus |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::ntoken() |&gt; sum() total &lt;- dat_corpus |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::tokens_select(c(&quot;わたくし&quot;, dat_fcm$term)) |&gt; quanteda::dfm() |&gt; quanteda::colSums() dat_fcm |&gt; dplyr::select(-node) |&gt; dplyr::mutate( expect = total[term] / ntok * total[&quot;わたくし&quot;] * 5 * 2, ## 5はwindowのサイズ t = (count - expect) / sqrt(count), mi = log2(count / expect) ) ## # A tibble: 20 × 5 ## term count expect t mi ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 は 205 88.1 8.17 1.22 ## 2 。 122 161. -3.53 -0.400 ## 3 た 110 108. 0.190 0.0264 ## 4 て 99 106. -0.694 -0.0972 ## 5 、 91 102. -1.13 -0.162 ## 6 まし 90 64.1 2.73 0.489 ## 7 を 62 67.4 -0.689 -0.121 ## 8 に 61 68.8 -1.00 -0.174 ## 9 が 51 51.9 -0.126 -0.0252 ## 10 し 37 24.1 2.11 0.616 ## 11 も 35 31.4 0.615 0.158 ## 12 で 33 34.7 -0.290 -0.0710 ## 13 ども 28 3.11 4.70 3.17 ## 14 と 23 28.7 -1.18 -0.317 ## 15 」 23 54.8 -6.63 -1.25 ## 16 です 22 15.4 1.40 0.512 ## 17 へ 17 16.5 0.114 0.0403 ## 18 い 15 17.4 -0.628 -0.217 ## 19 「 15 56.2 -10.6 -1.91 ## 20 から 14 20.4 -1.72 -0.546 注意点として、quantedaは全角スペースなどをトークンとして数えないようなので、ここでの総語数（ntok）は、RMeCabの計算で使われる総語数よりも少なくなることがあります。RMeCabでの計算結果と概ね一致させたい場合は、総語数としてgibasa::tokenizeの戻り値の行数を使ってください。 "],["sessioninfo.html", "Chapter 6 セッション情報 6.1 更新情報 6.2 セッション情報", " Chapter 6 セッション情報 6.1 更新情報 \\[2023-08-02\\] 「tidytextによる重みづけ」についての記述に誤りがあったため、修正しました 「Posit Cloud（旧・RStudio Cloud）でのgibasaの使い方」の内容を更新しました 6.2 セッション情報 sessioninfo::session_info() ## ─ Session info ────────────────────────── ## setting value ## version R version 4.3.1 (2023-06-16) ## os Ubuntu 22.04.2 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2023-08-01 ## pandoc 3.1.1 @ /usr/local/bin/ (via rmarkdown) ## ## ─ Packages ────────────────────────────── ## package * version date (UTC) lib source ## audubon 0.5.1 2023-05-02 [1] RSPM ## bit 4.0.5 2022-11-15 [1] RSPM (R 4.3.0) ## bit64 4.0.5 2020-08-30 [1] RSPM (R 4.3.0) ## bookdown 0.34 2023-05-09 [1] RSPM ## bslib 0.5.0 2023-06-09 [1] RSPM (R 4.3.0) ## cachem 1.0.8 2023-05-01 [1] RSPM (R 4.3.0) ## cli 3.6.1 2023-03-23 [1] RSPM (R 4.3.0) ## crayon 1.5.2 2022-09-29 [1] RSPM (R 4.3.0) ## curl 5.0.1 2023-06-07 [1] RSPM (R 4.3.0) ## data.table 1.14.8 2023-02-17 [1] RSPM (R 4.3.0) ## digest 0.6.31 2022-12-11 [1] RSPM (R 4.3.0) ## dplyr * 1.1.2 2023-04-20 [1] RSPM (R 4.3.0) ## dtplyr 1.3.1 2023-03-22 [1] RSPM (R 4.3.0) ## ellipsis 0.3.2 2021-04-29 [1] RSPM (R 4.3.0) ## evaluate 0.21 2023-05-05 [1] RSPM (R 4.3.0) ## fansi 1.0.4 2023-01-22 [1] RSPM (R 4.3.0) ## fastmap 1.1.1 2023-02-24 [1] RSPM (R 4.3.0) ## fastmatch 1.1-3 2021-07-23 [1] RSPM ## forcats 1.0.0 2023-01-29 [1] RSPM (R 4.3.0) ## generics 0.1.3 2022-07-05 [1] RSPM (R 4.3.0) ## gibasa 0.9.5 2023-07-09 [1] RSPM ## glue 1.6.2 2022-02-24 [1] RSPM (R 4.3.0) ## hms 1.1.3 2023-03-21 [1] RSPM (R 4.3.0) ## htmltools 0.5.5 2023-03-23 [1] RSPM (R 4.3.0) ## htmlwidgets 1.6.2 2023-03-17 [1] RSPM (R 4.3.0) ## janeaustenr 1.0.0 2022-08-26 [1] RSPM ## jquerylib 0.1.4 2021-04-26 [1] RSPM (R 4.3.0) ## jsonlite 1.8.7 2023-06-29 [1] RSPM ## knitr 1.43 2023-05-25 [1] RSPM (R 4.3.0) ## lattice 0.21-8 2023-04-05 [2] CRAN (R 4.3.1) ## lifecycle 1.0.3 2022-10-07 [1] RSPM (R 4.3.0) ## magrittr 2.0.3 2022-03-30 [1] RSPM (R 4.3.0) ## Matrix 1.5-4.1 2023-05-18 [2] CRAN (R 4.3.1) ## memoise 2.0.1 2021-11-26 [1] RSPM (R 4.3.0) ## pillar 1.9.0 2023-03-22 [1] RSPM (R 4.3.0) ## pkgconfig 2.0.3 2019-09-22 [1] RSPM (R 4.3.0) ## purrr 1.0.1 2023-01-10 [1] RSPM (R 4.3.0) ## quanteda 3.3.1 2023-05-18 [1] RSPM ## R6 2.5.1 2021-08-19 [1] RSPM (R 4.3.0) ## Rcpp 1.0.10 2023-01-22 [1] RSPM (R 4.3.0) ## RcppParallel 5.1.7 2023-02-27 [1] RSPM ## reactable 0.4.4 2023-03-12 [1] RSPM ## reactR 0.4.4 2021-02-22 [1] RSPM ## readr 2.1.4 2023-02-10 [1] RSPM (R 4.3.0) ## rlang 1.1.1 2023-04-28 [1] RSPM ## rmarkdown 2.22 2023-06-01 [1] RSPM ## rstudioapi 0.14 2022-08-22 [1] RSPM (R 4.3.0) ## sass 0.4.6 2023-05-03 [1] RSPM (R 4.3.0) ## sessioninfo 1.2.2 2021-12-06 [1] RSPM ## SnowballC 0.7.1 2023-04-25 [1] RSPM ## stopwords 2.3 2021-10-28 [1] RSPM ## stringi 1.7.12 2023-01-11 [1] RSPM (R 4.3.0) ## stringr 1.5.0 2022-12-02 [1] RSPM (R 4.3.0) ## tibble 3.2.1 2023-03-20 [1] RSPM (R 4.3.0) ## tidylo 0.2.0 2022-03-22 [1] RSPM ## tidyr 1.3.0 2023-01-24 [1] RSPM (R 4.3.0) ## tidyselect 1.2.0 2022-10-10 [1] RSPM (R 4.3.0) ## tidytext 0.4.1 2023-01-07 [1] RSPM ## tokenizers 0.3.0 2022-12-22 [1] RSPM ## tzdb 0.4.0 2023-05-12 [1] RSPM (R 4.3.0) ## udpipe 0.8.11 2023-01-06 [1] RSPM ## utf8 1.2.3 2023-01-31 [1] RSPM (R 4.3.0) ## V8 4.3.3 2023-07-18 [1] RSPM ## vctrs 0.6.2 2023-04-19 [1] RSPM (R 4.3.0) ## vroom 1.6.3 2023-04-28 [1] RSPM (R 4.3.0) ## withr 2.5.0 2022-03-03 [1] RSPM (R 4.3.0) ## xfun 0.39 2023-04-20 [1] RSPM (R 4.3.0) ## yaml 2.3.7 2023-01-23 [1] RSPM (R 4.3.0) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library ## ## ───────────────────────────────────────── "],["misc.html", "Chapter 7 Appendix 7.1 Posit Cloud（旧・RStudio Cloud）でのgibasaの使い方", " Chapter 7 Appendix 7.1 Posit Cloud（旧・RStudio Cloud）でのgibasaの使い方 Rによるデータ分析を手軽に試したい場合には、Posit Cloud（旧・RStudio Cloud）のようなクラウド環境が便利かもしれません。 一方で、Posit Cloudはユーザー権限しかない環境のため、gibasaを使えるようにするまでにはややコツが要ります。とはいえ、gibasaはRMeCabとは異なり、MeCabのバイナリはなくても使える（辞書とmecabrcがあればよい）ので、RMeCabを使う場合ほど複雑なことをする必要はないはずです。 ここでは、Posit Cloudでgibasaを利用できるようにするための手順を簡単に説明します（RMeCabもあわせて試したいという場合には、MeCabのバイナリを自分でビルドする必要があります。その場合はこの記事などを参考にしてください）。 7.1.1 辞書（ipadic, unidic-lite）の配置 MeCabの辞書は、Terminalタブからpipでインストールできます。ここでは、IPA辞書（ipadic）とunidic-liteをインストールします。 python3 -m pip install ipadic unidic-lite python3 -c &quot;import ipadic; print(&#39;dicdir=&#39; + ipadic.DICDIR);&quot; &gt; ~/.mecabrc 7.1.2 gibasaのインストール gibasaをインストールします。 install.packages(&quot;gibasa&quot;) 7.1.3 試すには うまくいっていると、辞書を指定しない場合はIPA辞書が使われます。unidic-liteはsys_dic引数にフルパスを指定することで使用できます。 gibasa::tokenize(&quot;こんにちは&quot;) unidic_lite &lt;- path.expand(&quot;~/.local/lib/python3.8/site-packages/unidic_lite/dicdir&quot;) gibasa::tokenize(&quot;こんにちは&quot;, sys_dic = unidic_lite) |&gt; gibasa::prettify(into = gibasa::get_dict_features(&quot;unidic26&quot;)) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
