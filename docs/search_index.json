[["index.html", "RとMeCabによる日本語テキストマイニングの前処理 はじめに 0.1 この資料について 0.2 Rでテキストマイニングするということ", " RとMeCabによる日本語テキストマイニングの前処理 Akiru Kato 2023-12-13 はじめに 0.1 この資料について 0.1.1 この資料でやりたいこと gibasaやその他のRパッケージを使って、RMeCabでできるようなテキストマイニングの前処理をより見通しよくおこなうやり方を紹介します。 0.1.2 想定する知識など R言語の基本的な使い方の説明はしません。tidyverseなどの使い方については、他の資料を参照してください。参考までに、tidyverseなどの使い方についての紹介は次の資料がおすすめです。 私たちのR また、以降の説明ではRでの日本語テキストの前処理のやり方のみにフォーカスしているため、具体的なテキストデータの分析のやり方には一切踏み込んでいません。Rでおこなうようなテキストデータの分析の方法については、いずれも英語の資料ですが、次が参考になると思います。 Text Mining with R Supervised Machine Learning for Text Analysis in R 0.2 Rでテキストマイニングするということ 0.2.1 テキストを分析して何がしたいのか テキストマイニングに関する入門的な本だと、「テキストマイニングとは何か」みたいな話から入るような気がします。ここでは必ずしも入門的な内容をめざしてはいませんが、しかし、すこし考えてみましょう。テキストマイニングとはなんでしょうか。 自然言語処理というのは、まあいろいろと思想はあるでしょうが、総じて「テキストを機械的に処理してごにょごにょする」技術のことだと思います。自然言語処理界隈の論文などを眺めていると、その範囲はかなり広くて、文書要約から文書生成といったタスクまで含まれるようです。 そのなかでもテキストマイニングというと、「テキストから特徴量をつくって何かを分析する」みたいな部分にフォーカスしてくるのではないでしょうか。 素人考えですが、テキストマイニングとはしたがってデータ分析のことです。そのため、前提としてテキストを分析して何がしたいのか（＝何ができるのか）を見通しよくしておくと、嬉しいことが多い気がします。 0.2.2 テキストマイニングでめざすこと・できること CRISP-DM (Cross-Industry Standard Process for Data Mining) は、IBMを中心としたコンソーシアムが提案したデータマイニングのための標準プロセスです。 これはデータ分析をビジネスに活かすことを念頭においてつくられた「課題ドリブン」なプロセスであるため、場合によってはそのまま採用できないかもしれませんが、こうした標準プロセスを押さえておくことは、分析プロセスを設計するうえで有用だと思います。 CRISP-DMは以下の6つの段階（phases）を行ったり来たりすることで進められていきます。 Business Understanding Data Understanding Data Preparation Modeling Evaluation Deployment CRISP-DMはデータ分析を通じて達成したいことから分析をスタートしていく、ある意味でトップダウン的なプロセスです。しかし、データからの知見の発掘はそんなにトップダウン一直線にはうまくいかないものです。いわばボトムアップ的にも、段階を「行ったり来たり」しながら分析を進めるためには、データ分析でとれるカードをなんとなく把握しておく必要があります。 これも素人考えですが、私たちがデータ分析でとれるカードというのは、だいたい次の３つくらいのものです。 モデルをつくって何かの回帰をする モデルをつくって何かの分類をする グループに分けて違いを評価する そのために、これらの落としどころに持ち込むためのテキストの特徴量をどうにかしてつくること（前処理）が、私たちが実際におこなうテキストマイニングの大きな部分を占めるように思います。 そして、それらの特徴量は、テキストについて何かを数えた頻度または比率とそれらを変換したものだと思っておくとすっきりします。数を数える「何か」というのは、たとえば語だったり品詞だったり、それらのNgramだったり、その他のタグ付けされた情報だったりします。 0.2.3 テキストマイニングの流れ テキストマイニングの大まかな流れは、イメージ的には、次のような感じになります。 分析したいテキストをいっぱい集める 分析して何がしたいか考える そのためにつくるべき特徴量を考える 特徴量をつくる 正規化などの文字列処理 トークナイズ・ステミング・レメタイズ 集計 特徴量の変換や補完 分析する 特徴量をつかってデータ分析する 得られた結果を評価する （必要に応じて）得られた知見を活かす この資料では、この流れのなかでも、2にとくにフォーカスして、テキストの前処理のやり方を説明します。 "],["intro.html", "Chapter 1 gibasaの基本的な使い方 1.1 テキストデータ 1.2 gibasaの使い方", " Chapter 1 gibasaの基本的な使い方 1.1 テキストデータ ここでは、audubonパッケージに含まれているaudubon::polanoというデータを例にgibasaの基本的な使い方を説明していきます。このデータは、青空文庫で公開されている、宮沢賢治の「ポラーノの広場」という小説を、改行ごとにひとつの要素としてベクトルにしたものです。 このデータを、次のようなかたちのデータフレーム（tibble）にします。 dat_txt &lt;- tibble::tibble( doc_id = seq_along(audubon::polano) |&gt; as.character(), text = audubon::polano ) |&gt; dplyr::mutate(text = audubon::strj_normalize(text)) str(dat_txt) #&gt; tibble [899 × 2] (S3: tbl_df/tbl/data.frame) #&gt; $ doc_id: chr [1:899] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... #&gt; $ text : chr [1:899] &quot;ポラーノの広場&quot; &quot;宮沢賢治&quot; &quot;前十七等官レオーノ・キュースト誌&quot; &quot;宮沢賢治訳述&quot; ... このかたちのデータフレームは、Text Interchange Formats（TIF）という仕様を念頭においている形式です（ちなみに、このかたちのデータフレームはreadtextパッケージを使うと簡単に得ることができますが、readtextクラスのオブジェクトはdplyrと相性が悪いようなので、使う場合はdplyr::tibbleなどでtibbleにしてしまうことをおすすめします）。 Text Interchange Formats（TIF）は、2017年にrOpenSci Text Workshopで整備された、テキスト分析用のRパッケージのデザインパターンのようなものです。 TIFでは、コーパス（corpus）、文書単語行列（dtm）、トークン（token）という3種類のオブジェクトの形式が定義されており、異なるパッケージ間で同様の形式を扱うようにすることで、複数のパッケージを通じて便利にテキスト分析を進められるようになっています。 上のdat_txtは、文書の集合であるコーパスをデータフレームのかたちで保持したものです。この形式のデータフレームは、次のように、tidytextやtokenizersの関数にそのまま渡すことができます。なお、これらの形式のオブジェクトは、TIFの枠組みのなかではトークンと呼ばれます。 dat_txt |&gt; tidytext::unnest_tokens(token, text) |&gt; head(4) #&gt; # A tibble: 4 × 2 #&gt; doc_id token #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 ポラーノ #&gt; 2 1 の #&gt; 3 1 広場 #&gt; 4 2 宮沢 dat_txt |&gt; tokenizers::tokenize_words() |&gt; head(4) #&gt; $`1` #&gt; [1] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; #&gt; #&gt; $`2` #&gt; [1] &quot;宮沢&quot; &quot;賢治&quot; #&gt; #&gt; $`3` #&gt; [1] &quot;前&quot; &quot;十七&quot; &quot;等&quot; &quot;官&quot; &quot;レ&quot; &quot;オー&quot; &quot;ノ&quot; &quot;キュー&quot; &quot;スト&quot; &quot;誌&quot; #&gt; #&gt; $`4` #&gt; [1] &quot;宮沢&quot; &quot;賢治&quot; &quot;訳述&quot; 1.2 gibasaの使い方 1.2.1 tokenize 前節で見たように、tokenizers::tokenize_wordsやこれを利用しているtidytext::unnest_tokensは、日本語のテキストであっても機械的にトークンのかたちに整形する（分かち書きする）ことができます。 tokenizersパッケージの分かち書きは、内部的には、ICUのBoundary Analysisによるものです。この単語境界判定は、たとえば新聞記事のような、比較的整った文体の文章ではおおむね期待通り分かち書きがおこなわれ、また、日本語と英語などが混ざっている文章であってもとくに気にすることなく、高速に分かち書きできるという強みがあります。 しかし、手元にある辞書に収録されている語の通りに分かち書きしたい場合や、品詞情報などがほしい場合には、やはりMeCabのような形態素解析器による分かち書きが便利なこともあります。 gibasaは、そのようなケースにおいて、tidytext::unnest_tokensの代わりに使用できる機能を提供するために開発しているパッケージです。この機能はgibasa::tokenizeという関数として提供していて、次のように使うことができます。 dat &lt;- gibasa::tokenize(dat_txt, text, doc_id) str(dat) #&gt; tibble [26,849 × 5] (S3: tbl_df/tbl/data.frame) #&gt; $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... #&gt; $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... #&gt; $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... #&gt; $ token : chr [1:26849] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... #&gt; $ feature : chr [1:26849] &quot;名詞,一般,*,*,*,*,*&quot; &quot;助詞,連体化,*,*,*,*,の,ノ,ノ&quot; &quot;名詞,一般,*,*,*,*,広場,ヒロバ,ヒロバ&quot; &quot;名詞,固有名詞,人名,姓,*,*,宮沢,ミヤザワ,ミヤザワ&quot; ... 1.2.2 prettify gibasa::tokenizeの戻り値のデータフレームは、それぞれのトークンについて、MeCabから返される素性情報のすべてを含んでいるfeatureという列を持っています。 MeCabから返される素性情報は、使用している辞書によって異なります。たとえば、IPA辞書やUniDic（2.1.2, aka unidic-lite）の素性は、次のような情報を持っています。 gibasa::get_dict_features(&quot;ipa&quot;) #&gt; [1] &quot;POS1&quot; &quot;POS2&quot; &quot;POS3&quot; &quot;POS4&quot; &quot;X5StageUse1&quot; &quot;X5StageUse2&quot; &quot;Original&quot; &quot;Yomi1&quot; &quot;Yomi2&quot; gibasa::get_dict_features(&quot;unidic26&quot;) #&gt; [1] &quot;POS1&quot; &quot;POS2&quot; &quot;POS3&quot; &quot;POS4&quot; &quot;cType&quot; &quot;cForm&quot; &quot;lForm&quot; &quot;lemma&quot; &quot;orth&quot; &quot;pron&quot; #&gt; [11] &quot;orthBase&quot; &quot;pronBase&quot; &quot;goshu&quot; &quot;iType&quot; &quot;iForm&quot; &quot;fType&quot; &quot;fForm&quot; &quot;kana&quot; &quot;kanaBase&quot; &quot;form&quot; #&gt; [21] &quot;formBase&quot; &quot;iConType&quot; &quot;fConType&quot; &quot;aType&quot; &quot;aConType&quot; &quot;aModeType&quot; こうした素性情報をデータフレームの列にパースするには、gibasa::prettifyという関数を利用できます。 デフォルトではすべての素性についてパースしますが、col_select引数に残したい列名を指定することにより、特定の素性情報だけをパースすることもできます。このかたちのデータフレームは、解析するテキストの文章量によっては、数十万から数百万くらいの行からなることもよくあります。そのような規模のデータフレームについて、いちいちすべての素性をパースしていると、それだけでメモリを余計に消費してしまいます。メモリの消費を抑えるためにも、なるべく後で必要な素性だけをこまめに指定することをおすすめします。 str(gibasa::prettify(dat)) #&gt; tibble [26,849 × 13] (S3: tbl_df/tbl/data.frame) #&gt; $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... #&gt; $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... #&gt; $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... #&gt; $ token : chr [1:26849] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... #&gt; $ POS1 : chr [1:26849] &quot;名詞&quot; &quot;助詞&quot; &quot;名詞&quot; &quot;名詞&quot; ... #&gt; $ POS2 : chr [1:26849] &quot;一般&quot; &quot;連体化&quot; &quot;一般&quot; &quot;固有名詞&quot; ... #&gt; $ POS3 : chr [1:26849] NA NA NA &quot;人名&quot; ... #&gt; $ POS4 : chr [1:26849] NA NA NA &quot;姓&quot; ... #&gt; $ X5StageUse1: chr [1:26849] NA NA NA NA ... #&gt; $ X5StageUse2: chr [1:26849] NA NA NA NA ... #&gt; $ Original : chr [1:26849] NA &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... #&gt; $ Yomi1 : chr [1:26849] NA &quot;ノ&quot; &quot;ヒロバ&quot; &quot;ミヤザワ&quot; ... #&gt; $ Yomi2 : chr [1:26849] NA &quot;ノ&quot; &quot;ヒロバ&quot; &quot;ミヤザワ&quot; ... str(gibasa::prettify(dat, col_select = c(1, 2))) #&gt; tibble [26,849 × 6] (S3: tbl_df/tbl/data.frame) #&gt; $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... #&gt; $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... #&gt; $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... #&gt; $ token : chr [1:26849] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... #&gt; $ POS1 : chr [1:26849] &quot;名詞&quot; &quot;助詞&quot; &quot;名詞&quot; &quot;名詞&quot; ... #&gt; $ POS2 : chr [1:26849] &quot;一般&quot; &quot;連体化&quot; &quot;一般&quot; &quot;固有名詞&quot; ... str(gibasa::prettify(dat, col_select = c(&quot;POS1&quot;, &quot;Original&quot;))) #&gt; tibble [26,849 × 6] (S3: tbl_df/tbl/data.frame) #&gt; $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... #&gt; $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... #&gt; $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... #&gt; $ token : chr [1:26849] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... #&gt; $ POS1 : chr [1:26849] &quot;名詞&quot; &quot;助詞&quot; &quot;名詞&quot; &quot;名詞&quot; ... #&gt; $ Original : chr [1:26849] NA &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... 1.2.3 pack gibasa::packという関数を使うと、トークンの形式のデータフレームから、各トークンを半角スペースで区切ったコーパスの形式のデータフレームにすることができます。 dat_corpus &lt;- dat |&gt; gibasa::pack() str(dat_corpus) #&gt; tibble [899 × 2] (S3: tbl_df/tbl/data.frame) #&gt; $ doc_id: Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ text : chr [1:899] &quot;ポラーノ の 広場&quot; &quot;宮沢 賢治&quot; &quot;前 十 七 等 官 レオーノ・キュースト 誌&quot; &quot;宮沢 賢治 訳述&quot; ... このかたちのデータフレームはTIFに準拠していたため、他のパッケージと組み合わせて使うのに便利なことがあります。たとえば、このかたちから、次のようにtidytext::unnest_tokensと組み合わせて、もう一度トークンの形式のデータフレームに戻すことができます。 dat_corpus |&gt; tidytext::unnest_tokens(token, text, token = \\(x) { strsplit(x, &quot; +&quot;) }) |&gt; head(4) #&gt; # A tibble: 4 × 2 #&gt; doc_id token #&gt; &lt;fct&gt; &lt;chr&gt; #&gt; 1 1 ポラーノ #&gt; 2 1 の #&gt; 3 1 広場 #&gt; 4 2 宮沢 あるいは、次のようにquantedaと組み合わせて使うこともできます。 dat_corpus |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;, remove_punct = FALSE) #&gt; Tokens consisting of 899 documents. #&gt; 1 : #&gt; [1] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; #&gt; #&gt; 2 : #&gt; [1] &quot;宮沢&quot; &quot;賢治&quot; #&gt; #&gt; 3 : #&gt; [1] &quot;前&quot; &quot;十&quot; &quot;七&quot; &quot;等&quot; &quot;官&quot; #&gt; [6] &quot;レオーノ・キュースト&quot; &quot;誌&quot; #&gt; #&gt; 4 : #&gt; [1] &quot;宮沢&quot; &quot;賢治&quot; &quot;訳述&quot; #&gt; #&gt; 5 : #&gt; [1] &quot;その&quot; &quot;ころ&quot; &quot;わたくし&quot; &quot;は&quot; &quot;、&quot; &quot;モリーオ&quot; &quot;市&quot; &quot;の&quot; &quot;博物&quot; &quot;局&quot; &quot;に&quot; #&gt; [12] &quot;勤め&quot; #&gt; [ ... and 5 more ] #&gt; #&gt; 6 : #&gt; [1] &quot;十&quot; &quot;八&quot; &quot;等&quot; &quot;官&quot; &quot;でし&quot; &quot;た&quot; &quot;から&quot; &quot;役所&quot; &quot;の&quot; &quot;なか&quot; &quot;でも&quot; &quot;、&quot; #&gt; [ ... and 219 more ] #&gt; #&gt; [ reached max_ndoc ... 893 more documents ] 1.2.4 lazy_dtなどと組み合わせて使う場合 gibasa::prettifyはデータフレームにしか使えないため、data.tableなどと組み合わせて使う場合にはtidyr::separateを代わりに使ってください。 dat_toks &lt;- dat |&gt; dtplyr::lazy_dt() |&gt; tidyr::separate(feature, into = gibasa::get_dict_features(), sep = &quot;,&quot;, extra = &quot;merge&quot;, fill = &quot;right&quot; ) |&gt; dplyr::mutate( token = dplyr::if_else(Original == &quot;*&quot;, token, Original), token = stringr::str_c(token, POS1, POS2, sep = &quot;/&quot;) ) |&gt; dplyr::select(doc_id, sentence_id, token_id, token) |&gt; dplyr::as_tibble() |&gt; dplyr::mutate(across(where(is.character), ~ dplyr::na_if(., &quot;*&quot;))) str(dat_toks) #&gt; tibble [26,849 × 4] (S3: tbl_df/tbl/data.frame) #&gt; $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... #&gt; $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... #&gt; $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... #&gt; $ token : chr [1:26849] &quot;ポラーノ/名詞/一般&quot; &quot;の/助詞/連体化&quot; &quot;広場/名詞/一般&quot; &quot;宮沢/名詞/固有名詞&quot; ... "],["dtm.html", "Chapter 2 トークンの集計と文書単語行列への整形 2.1 トークンの集計 2.2 文書単語行列への整形", " Chapter 2 トークンの集計と文書単語行列への整形 2.1 トークンの集計 2.1.1 品詞などにもとづくしぼりこみ トークンを簡単に集計するには、dplyrの関数群を利用するのが便利です。 たとえば、集計に先立って特定のトークンを素性情報にもとづいて選択するにはdplyr::filterを使います。 dat |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;Original&quot;)) |&gt; dplyr::filter(POS1 %in% c(&quot;名詞&quot;, &quot;動詞&quot;, &quot;形容詞&quot;)) |&gt; dplyr::slice_head(n = 30L) |&gt; reactable::reactable(compact = TRUE) 一方で、以下で紹介するようなトークンの再結合を後からやりたい場合には、この方法は適切ではありません。dplyr::filterを使うとデータフレーム中のトークンを抜き取ってしまうため、この操作をした後では、実際の文書のなかでは隣り合っていないトークンどうしが隣接しているように扱われてしまいます。 品詞などの情報にもとづいてトークンを取捨選択しつつも、トークンの位置関係はとりあえず保持したいという場合には、gibasa::mute_tokensを使います。この関数は、条件にマッチしたトークンをNA_character_に置き換えます（reactableによる出力のなかでは空白として表示されています）。 dat |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;Original&quot;)) |&gt; gibasa::mute_tokens(!POS1 %in% c(&quot;名詞&quot;, &quot;動詞&quot;, &quot;形容詞&quot;)) |&gt; dplyr::slice_head(n = 30L) |&gt; reactable::reactable(compact = TRUE) 2.1.2 品詞などにもとづくトークンの再結合 トークンを集計する目的によっては、形態素解析された結果の単語では単位として短すぎることがあります。 たとえば、IPA辞書では「小田急線」は「小田急（名詞・固有名詞）+線（名詞・接尾）」として解析され、「小田急線」という単語としては解析されません。このように、必ずしも直感的な解析結果がえられないことは、UniDicを利用している場合により頻繁に発生します。実際、UniDicでは「水族館」も「水族（名詞・普通名詞）+館（接尾辞・名詞的）」として解析されるなど、IPA辞書よりもかなり細かな単位に解析されます。 # IPA辞書による解析の例 gibasa::tokenize(c( &quot;佐藤さんはそのとき小田急線で江の島水族館に向かっていた&quot;, &quot;秒速5センチメートルは新海誠が監督した映画作品&quot;, &quot;辛そうで辛くない少し辛いラー油の辛さ&quot; )) |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;POS2&quot;, &quot;POS3&quot;)) |&gt; reactable::reactable(compact = TRUE) 分析の関心によっては、こうした細かくなりすぎたトークンをまとめあげて、もっと長い単位の単語として扱えると便利かもしれません。 gibasa::collapse_tokensを使うと、渡された条件にマッチする一連のトークンをまとめあげて、新しいトークンにすることができます。 gibasa::tokenize(c( &quot;佐藤さんはそのとき小田急線で江の島水族館に向かっていた&quot;, &quot;秒速5センチメートルは新海誠が監督した映画作品&quot;, &quot;辛そうで辛くない少し辛いラー油の辛さ&quot; )) |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;POS2&quot;, &quot;POS3&quot;)) |&gt; gibasa::collapse_tokens( (POS1 %in% c(&quot;名詞&quot;, &quot;接頭詞&quot;) &amp; !stringr::str_detect(token, &quot;^[あ-ン]+$&quot;)) | (POS1 %in% c(&quot;名詞&quot;, &quot;形容詞&quot;) &amp; POS2 %in% c(&quot;自立&quot;, &quot;接尾&quot;, &quot;数接続&quot;)) ) |&gt; reactable::reactable(compact = TRUE) この機能は強力ですが、条件を書くには、利用している辞書の品詞体系について理解している必要があります。また、機械的に処理しているにすぎないため、一部のトークンは、かえって意図しないかたちにまとめあげられてしまう場合があります。あるいは、機械学習の特徴量をつくるのが目的であるケースなどでは、単純にNgramを利用したほうが便利かもしれません。 2.1.3 原形の集計 dplyr::countでトークンを文書ごとに集計します。ここでは、IPA辞書の見出し語がある語については「原形（Original）」を、見出し語がない語（未知語）については表層形を数えています。 MeCabは、未知語であっても品詞の推定をおこないますが、未知語の場合には「読み（Yomi1, Yomi2）」のような一部の素性については情報を返しません。このような未知語の素性については、prettifyした結果のなかでは、NA_character_になっていることに注意してください。 dat_count &lt;- dat |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;Original&quot;)) |&gt; dplyr::filter(POS1 %in% c(&quot;名詞&quot;, &quot;動詞&quot;, &quot;形容詞&quot;)) |&gt; dplyr::mutate( doc_id = forcats::fct_drop(doc_id), token = dplyr::if_else(is.na(Original), token, Original) ) |&gt; dplyr::count(doc_id, token) str(dat_count) #&gt; tibble [9,697 × 3] (S3: tbl_df/tbl/data.frame) #&gt; $ doc_id: Factor w/ 876 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 2 2 3 3 3 3 3 3 ... #&gt; $ token : chr [1:9697] &quot;ポラーノ&quot; &quot;広場&quot; &quot;宮沢&quot; &quot;賢治&quot; ... #&gt; $ n : int [1:9697] 1 1 1 1 1 1 1 1 1 1 ... 2.2 文書単語行列への整形 こうして集計した縦持ちの頻度表を横持ちにすると、いわゆる文書単語行列になります。 dtm &lt;- dat_count |&gt; tidyr::pivot_wider( id_cols = doc_id, names_from = token, values_from = n, values_fill = 0 ) dim(dtm) #&gt; [1] 876 2168 ただし、このようにtidyr::pivot_widerで単純に横持ちにすることは、非常に大量の列を持つ巨大なデータフレームを作成することになるため、おすすめしません。文書単語行列を作成するには、tidytext::cast_sparseやtidytext::cast_dfmなどを使って、疎行列のオブジェクトにしましょう。 dtm &lt;- dat_count |&gt; tidytext::cast_sparse(doc_id, token, n) dim(dtm) #&gt; [1] 876 2167 "],["ngram.html", "Chapter 3 N-gram 3.1 dplyrを使ってNgramを数える方法 3.2 quantedaにNgramを持ちこむ方法 3.3 quantedaでNgramを数える方法", " Chapter 3 N-gram 3.1 dplyrを使ってNgramを数える方法 dplyrを使って簡単にやる場合、次のようにすると2-gramを集計できます。 bigram &lt;- gibasa::ngram_tokenizer(2) dat_ngram &lt;- dat |&gt; gibasa::prettify(col_select = &quot;Original&quot;) |&gt; dplyr::mutate( token = dplyr::if_else(is.na(Original), token, Original) ) |&gt; dplyr::reframe(token = bigram(token, sep = &quot;-&quot;), .by = doc_id) |&gt; dplyr::count(doc_id, token) str(dat_ngram) #&gt; tibble [24,398 × 3] (S3: tbl_df/tbl/data.frame) #&gt; $ doc_id: Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 2 3 3 3 3 3 3 4 ... #&gt; $ token : chr [1:24398] &quot;の-広場&quot; &quot;ポラーノ-の&quot; &quot;宮沢-賢治&quot; &quot;レオーノ・キュースト-誌&quot; ... #&gt; $ n : int [1:24398] 1 1 1 1 1 1 1 1 1 1 ... 3.2 quantedaにNgramを持ちこむ方法 gibasa::packを使ってNgramの分かち書きをつくることもできます。この場合、次のようにquantedaの枠組みの中でNgramをトークンとして数えることで集計することができます。 dat_ngram &lt;- dat |&gt; gibasa::prettify(col_select = &quot;Original&quot;) |&gt; dplyr::mutate(token = dplyr::if_else(is.na(Original), token, Original)) |&gt; gibasa::pack(n = 2) str(dat_ngram) #&gt; tibble [899 × 2] (S3: tbl_df/tbl/data.frame) #&gt; $ doc_id: Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ text : chr [1:899] &quot;ポラーノ-の の-広場&quot; &quot;宮沢-賢治&quot; &quot;前-十 十-七 七-等 等-官 官-レオーノ・キュースト レオーノ・キュースト-誌&quot; &quot;宮沢-賢治 賢治-訳述&quot; ... dat_ngram |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::dfm() #&gt; Document-feature matrix of: 899 documents, 10,890 features (99.75% sparse) and 0 docvars. #&gt; features #&gt; docs ポラーノ-の の-広場 宮沢-賢治 前-十 十-七 七-等 等-官 官-レオーノ・キュースト レオーノ・キュースト-誌 賢治-訳述 #&gt; 1 1 1 0 0 0 0 0 0 0 0 #&gt; 2 0 0 1 0 0 0 0 0 0 0 #&gt; 3 0 0 0 1 1 1 1 1 1 0 #&gt; 4 0 0 1 0 0 0 0 0 0 1 #&gt; 5 0 0 0 0 0 0 0 0 0 0 #&gt; 6 0 0 0 0 0 0 1 0 0 0 #&gt; [ reached max_ndoc ... 893 more documents, reached max_nfeat ... 10,880 more features ] 3.3 quantedaでNgramを数える方法 また、quantedaの枠組みの中でNgramをつくりながら数えて集計することもできます。 dat_ngram &lt;- dat |&gt; gibasa::prettify(col_select = &quot;Original&quot;) |&gt; dplyr::mutate(token = dplyr::if_else(is.na(Original), token, Original)) |&gt; gibasa::pack() str(dat_ngram) #&gt; tibble [899 × 2] (S3: tbl_df/tbl/data.frame) #&gt; $ doc_id: Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ text : chr [1:899] &quot;ポラーノ の 広場&quot; &quot;宮沢 賢治&quot; &quot;前 十 七 等 官 レオーノ・キュースト 誌&quot; &quot;宮沢 賢治 訳述&quot; ... dat_ngram |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::tokens_ngrams(n = 2) |&gt; quanteda::dfm() #&gt; Document-feature matrix of: 899 documents, 10,890 features (99.75% sparse) and 0 docvars. #&gt; features #&gt; docs ポラーノ_の の_広場 宮沢_賢治 前_十 十_七 七_等 等_官 官_レオーノ・キュースト レオーノ・キュースト_誌 賢治_訳述 #&gt; 1 1 1 0 0 0 0 0 0 0 0 #&gt; 2 0 0 1 0 0 0 0 0 0 0 #&gt; 3 0 0 0 1 1 1 1 1 1 0 #&gt; 4 0 0 1 0 0 0 0 0 0 1 #&gt; 5 0 0 0 0 0 0 0 0 0 0 #&gt; 6 0 0 0 0 0 0 1 0 0 0 #&gt; [ reached max_ndoc ... 893 more documents, reached max_nfeat ... 10,880 more features ] "],["weighting.html", "Chapter 4 単語頻度の重みづけ 4.1 tidytextによる重みづけ 4.2 gibasaによる重みづけ 4.3 udpipeによる重みづけ 4.4 tidyloによる重みづけ", " Chapter 4 単語頻度の重みづけ 4.1 tidytextによる重みづけ tidytext::bind_tf_idfを使うと単語頻度からTF-IDFを算出することができます。 dat_count |&gt; tidytext::bind_tf_idf(token, doc_id, n) |&gt; dplyr::slice_max(tf_idf, n = 5L) #&gt; # A tibble: 5 × 6 #&gt; doc_id token n tf idf tf_idf #&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 139 去年 1 1 6.78 6.78 #&gt; 2 880 ざあい 1 1 6.78 6.78 #&gt; 3 255 あわてる 1 1 6.08 6.08 #&gt; 4 713 こちら 1 1 6.08 6.08 #&gt; 5 288 こいつ 1 1 5.39 5.39 tidytextにおけるTFとIDFは、RMeCabにおけるそれとは採用している計算式が異なるため、計算結果が異なります。TFはRMeCabでは生の索引語頻度（tfの場合）ですが、tidytextでは文書内での相対頻度になります。また、IDFはRMeCabでは対数の底が2であるのに対して、tidytextでは底がexp(1)であるなどの違いがあります。 4.2 gibasaによる重みづけ gibasaはRMeCabにおける単語頻度の重みづけをtidytext::bind_tf_idfと同様のスタイルでおこなうことができる関数gibasa::bind_tf_idf2を提供しています。 bind_tf_idf2はおおむねRMeCabの単語頻度の重みづけの挙動を再現していますが、細かな点が異なります。たとえば、gibasaは「コサイン正規化」がIDFの計算時のみにおこなわれるため、norm=TRUEにしてもRMeCabの計算結果とは一致しません。 RMeCabは以下の単語頻度の重みづけをサポートしています。 局所的重み（TF） tf（索引語頻度） tf2（対数化索引語頻度） tf3（２進重み） 大域的重み（IDF） idf（文書頻度の逆数） idf2（大域的IDF） idf3（確率的IDF） idf4（エントロピー） 正規化 norm（コサイン正規化） gibasaはこれらの重みづけを再実装しています。ただし、tf=\"tf\"はgibasaでは相対頻度になるため、RMeCabのweight=\"tf*idf\"に相当する出力を得るには、たとえば次のように計算します。 dat_count |&gt; gibasa::bind_tf_idf2(token, doc_id, n) |&gt; dplyr::mutate( tf_idf = n / idf ) |&gt; dplyr::slice_max(tf_idf, n = 5L) #&gt; # A tibble: 8 × 6 #&gt; doc_id token n tf idf tf_idf #&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 715 の 11 0.103 3.74 2.94 #&gt; 2 732 いる 8 0.0842 3.10 2.58 #&gt; 3 715 わたくし 8 0.0748 3.32 2.41 #&gt; 4 825 いる 6 0.0638 3.10 1.94 #&gt; 5 615 する 6 0.0408 3.21 1.87 #&gt; 6 715 する 6 0.0561 3.21 1.87 #&gt; 7 822 する 6 0.0659 3.21 1.87 #&gt; 8 825 する 6 0.0638 3.21 1.87 なお、注意点として、RMeCabの単語を数える機能は、品詞情報（POS1とPOS2まで）を確認しながら単語を数えているようなので、ここでのように原形だけを見て数えた結果とは必ずしも一致しません。 4.3 udpipeによる重みづけ udpipeを使っても単語頻度とTF-IDFを算出できます。また、udpipe::document_term_frequencies_statisticsでは、TF、IDFとTF-IDFにくわえて、Okapi BM25を計算することができます。 udpipe::document_term_frequencies_statisticsには、パラメータとしてkとbを渡すことができます。デフォルト値はそれぞれk=1.2、b=0.5です。kの値を大きくすると、単語の出現数の増加に対してBM25の値もより大きくなりやすくなります。 k=1.2というのは、Elasticsearchでもデフォルト値として採用されている値です。WikipediaやElasticsearchの技術記事によると、kは[1.2, 2.0]、b=.75とした場合に、多くのケースでよい結果が得られるとされています。 dplyrを使っていればあまり意識する必要はないと思いますが、udpipeのこのあたりの関数の戻り値はdata.tableである点に注意してください。 suppressPackageStartupMessages(require(dplyr)) dat |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;Original&quot;)) |&gt; dplyr::filter(POS1 %in% c(&quot;名詞&quot;, &quot;動詞&quot;, &quot;形容詞&quot;)) |&gt; dplyr::mutate( doc_id = forcats::fct_drop(doc_id), token = dplyr::if_else(is.na(Original), token, Original) ) |&gt; udpipe::document_term_frequencies(document = &quot;doc_id&quot;, term = &quot;token&quot;) |&gt; udpipe::document_term_frequencies_statistics(b = .75) |&gt; dplyr::slice_max(tf_bm25, n = 5L) #&gt; doc_id term freq tf idf tf_idf tf_bm25 bm25 #&gt; 1: 381 呑む 2 0.5000000 3.639872 1.8199359 1.698658 6.182898 #&gt; 2: 398 決闘 2 0.5000000 4.829456 2.4147280 1.698658 8.203595 #&gt; 3: 864 呑む 2 0.5000000 3.639872 1.8199359 1.698658 6.182898 #&gt; 4: 859 呑む 3 0.3333333 3.639872 1.2132906 1.669563 6.076996 #&gt; 5: 60 ん 2 0.4000000 1.900169 0.7600675 1.652365 3.139773 #&gt; 6: 62 ん 2 0.4000000 1.900169 0.7600675 1.652365 3.139773 #&gt; 7: 233 ん 2 0.4000000 1.900169 0.7600675 1.652365 3.139773 #&gt; 8: 260 飛ぶ 2 0.4000000 5.165928 2.0663713 1.652365 8.535999 #&gt; 9: 428 する 2 0.4000000 1.533619 0.6134476 1.652365 2.534099 #&gt; 10: 521 君 2 0.4000000 4.578142 1.8312566 1.652365 7.564761 #&gt; 11: 742 ん 2 0.4000000 1.900169 0.7600675 1.652365 3.139773 4.4 tidyloによる重みづけ TF-IDFによる単語頻度の重みづけのモチベーションは、索引語のなかでも特定の文書だけに多く出現していて、ほかの文書ではそれほど出現しないような「注目に値する語」を調べることにあります。 こうしたことを実現するための値として、tidyloパッケージでは「重み付きログオッズ（weighted log odds）」を計算することができます。 dat_count |&gt; tidylo::bind_log_odds(set = doc_id, feature = token, n = n) |&gt; dplyr::filter(is.finite(log_odds_weighted)) |&gt; dplyr::slice_max(log_odds_weighted, n = 5L) #&gt; # A tibble: 5 × 4 #&gt; doc_id token n log_odds_weighted #&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 536 する 1 117. #&gt; 2 430 する 1 105. #&gt; 3 824 わたくし 1 102. #&gt; 4 577 いる 1 94.5 #&gt; 5 465 する 1 94.0 ここで用いているデータは小説を改行ごとに一つの文書と見なしていたため、中には次のような極端に短い文書が含まれています。こうした文書では、直観的にはそれほどレアには思われない単語についてもオッズが極端に高くなってしまっているように見えます。 dat_txt |&gt; dplyr::filter(doc_id %in% c(430, 536, 577, 824)) |&gt; dplyr::pull(text) #&gt; [1] &quot;「承知しました。」&quot; &quot;「起訴するぞ。」&quot; #&gt; [3] &quot;「きっと遠くでございますわ。もし生きていれば。」&quot; &quot;わたくしは思わずはねあがりました。&quot; weighted log oddsについてはこの資料などを参照してください。 "],["collocation.html", "Chapter 5 コロケーション 5.1 文書内での共起 5.2 任意のウィンドウ内での共起", " Chapter 5 コロケーション 5.1 文書内での共起 共起関係を数える機能はgibasaには実装されていません。文書内での共起を簡単に数えるには、たとえば次のようにします。 dat_fcm &lt;- dat_count |&gt; tidytext::cast_dfm(doc_id, token, n) |&gt; quanteda::fcm() dat_fcm #&gt; Feature co-occurrence matrix of: 2,167 by 2,167 features. #&gt; features #&gt; features ポラーノ 広場 宮沢 賢治 レオーノ・キュースト 七 十 官 等 誌 #&gt; ポラーノ 5 52 0 0 0 0 2 0 0 0 #&gt; 広場 0 5 0 0 0 0 2 0 0 0 #&gt; 宮沢 0 0 0 2 0 0 0 0 0 0 #&gt; 賢治 0 0 0 0 0 0 0 0 0 0 #&gt; レオーノ・キュースト 0 0 0 0 0 1 3 3 3 1 #&gt; 七 0 0 0 0 0 0 9 1 1 1 #&gt; 十 0 0 0 0 0 0 8 6 6 1 #&gt; 官 0 0 0 0 0 0 0 0 5 1 #&gt; 等 0 0 0 0 0 0 0 0 0 1 #&gt; 誌 0 0 0 0 0 0 0 0 0 0 #&gt; [ reached max_feat ... 2,157 more features, reached max_nfeat ... 2,157 more features ] 5.2 任意のウィンドウ内での共起 5.2.1 共起の集計 RMeCab::collocateのような任意のウィンドウの中での共起を集計するには、次のようにする必要があります。ここではwindowは前後5個のトークンを見るようにします。 dat_corpus &lt;- dat |&gt; gibasa::pack() dat_fcm &lt;- dat_corpus |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::fcm(context = &quot;window&quot;, window = 5) こうすると、nodeについて共起しているtermとその頻度を確認できます。以下では、「わたくし」というnodeと共起しているtermで頻度が上位20までであるものを表示しています。 dat_fcm &lt;- dat_fcm |&gt; tidytext::tidy() |&gt; dplyr::rename(node = document, term = term) |&gt; dplyr::filter(node == &quot;わたくし&quot;) |&gt; dplyr::slice_max(count, n = 20) dat_fcm #&gt; # A tibble: 20 × 3 #&gt; node term count #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 わたくし は 205 #&gt; 2 わたくし 。 122 #&gt; 3 わたくし た 110 #&gt; 4 わたくし て 99 #&gt; 5 わたくし 、 91 #&gt; 6 わたくし まし 90 #&gt; 7 わたくし を 62 #&gt; 8 わたくし に 61 #&gt; 9 わたくし が 51 #&gt; 10 わたくし し 37 #&gt; 11 わたくし も 35 #&gt; 12 わたくし で 33 #&gt; 13 わたくし ども 28 #&gt; 14 わたくし と 23 #&gt; 15 わたくし 」 23 #&gt; 16 わたくし です 22 #&gt; 17 わたくし へ 17 #&gt; 18 わたくし い 15 #&gt; 19 わたくし 「 15 #&gt; 20 わたくし から 14 5.2.2 T値やMI値の算出 T値やMI値は、たとえば次のようにして計算できます。 T値については「1.65」を越える場合、その共起が偶然ではないと考える大まかな目安となるそうです。また、MI値については「1.58」を越える場合に共起関係の大まかな目安となります（いずれの値についても「2」などを目安とする場合もあります）。 ntok &lt;- dat_corpus |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::ntoken() |&gt; sum() total &lt;- dat_corpus |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::tokens_select(c(&quot;わたくし&quot;, dat_fcm$term)) |&gt; quanteda::dfm() |&gt; quanteda::colSums() dat_fcm |&gt; dplyr::select(-node) |&gt; dplyr::mutate( expect = total[term] / ntok * total[&quot;わたくし&quot;] * 5 * 2, ## 5はwindowのサイズ t = (count - expect) / sqrt(count), mi = log2(count / expect) ) #&gt; # A tibble: 20 × 5 #&gt; term count expect t mi #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 は 205 88.1 8.17 1.22 #&gt; 2 。 122 161. -3.53 -0.400 #&gt; 3 た 110 108. 0.190 0.0264 #&gt; 4 て 99 106. -0.694 -0.0972 #&gt; 5 、 91 102. -1.13 -0.162 #&gt; 6 まし 90 64.1 2.73 0.489 #&gt; 7 を 62 67.4 -0.689 -0.121 #&gt; 8 に 61 68.8 -1.00 -0.174 #&gt; 9 が 51 51.9 -0.126 -0.0252 #&gt; 10 し 37 24.1 2.11 0.616 #&gt; 11 も 35 31.4 0.615 0.158 #&gt; 12 で 33 34.7 -0.290 -0.0710 #&gt; 13 ども 28 3.11 4.70 3.17 #&gt; 14 と 23 28.7 -1.18 -0.317 #&gt; 15 」 23 54.8 -6.63 -1.25 #&gt; 16 です 22 15.4 1.40 0.512 #&gt; 17 へ 17 16.5 0.114 0.0403 #&gt; 18 い 15 17.4 -0.628 -0.217 #&gt; 19 「 15 56.2 -10.6 -1.91 #&gt; 20 から 14 20.4 -1.72 -0.546 注意点として、quantedaは全角スペースなどをトークンとして数えないようなので、ここでの総語数（ntok）は、RMeCabの計算で使われる総語数よりも少なくなることがあります。RMeCabでの計算結果と概ね一致させたい場合は、総語数としてgibasa::tokenizeの戻り値の行数を使ってください。 "],["sessioninfo.html", "Chapter 6 セッション情報 6.1 更新情報 6.2 セッション情報", " Chapter 6 セッション情報 6.1 更新情報 2023-12-13 見た目を調整しました。内容に変更はありません 2023-12-12 「想定する知識など」に参考となる他の資料へのリンクを追加しました コードブロックの表示のされ方を調整しました 2023-12-03 「Chapter 4 単語頻度の重みづけ」の内容を更新しました 「Chapter 7 Appendix」に「MeCabの辞書をビルドするには」という節を追加しました 2023-08-02 「tidytextによる重みづけ」についての記述に誤りがあったため、修正しました 「Posit Cloud（旧・RStudio Cloud）でのgibasaの使い方」の内容を更新しました 6.2 セッション情報 sessioninfo::session_info() #&gt; ─ Session info ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────── #&gt; setting value #&gt; version R version 4.3.2 (2023-10-31) #&gt; os Ubuntu 22.04.3 LTS #&gt; system x86_64, linux-gnu #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; ctype en_US.UTF-8 #&gt; tz Etc/UTC #&gt; date 2023-12-13 #&gt; pandoc 3.1.1 @ /usr/local/bin/ (via rmarkdown) #&gt; #&gt; ─ Packages ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── #&gt; package * version date (UTC) lib source #&gt; audubon 0.5.1 2023-05-02 [1] RSPM #&gt; bit 4.0.5 2022-11-15 [1] RSPM (R 4.3.0) #&gt; bit64 4.0.5 2020-08-30 [1] RSPM (R 4.3.0) #&gt; bookdown 0.37 2023-12-01 [1] RSPM #&gt; bslib 0.5.1 2023-08-11 [1] RSPM (R 4.3.0) #&gt; cachem 1.0.8 2023-05-01 [1] RSPM (R 4.3.0) #&gt; cli 3.6.1 2023-03-23 [1] RSPM (R 4.3.0) #&gt; crayon 1.5.2 2022-09-29 [1] RSPM (R 4.3.0) #&gt; curl 5.1.0 2023-10-02 [1] RSPM (R 4.3.0) #&gt; data.table 1.14.8 2023-02-17 [1] RSPM (R 4.3.0) #&gt; digest 0.6.33 2023-07-07 [1] RSPM (R 4.3.0) #&gt; dplyr * 1.1.3 2023-09-03 [1] RSPM (R 4.3.0) #&gt; dtplyr 1.3.1 2023-03-22 [1] RSPM (R 4.3.0) #&gt; ellipsis 0.3.2 2021-04-29 [1] RSPM (R 4.3.0) #&gt; evaluate 0.22 2023-09-29 [1] RSPM (R 4.3.0) #&gt; fansi 1.0.5 2023-10-08 [1] RSPM (R 4.3.0) #&gt; fastmap 1.1.1 2023-02-24 [1] RSPM (R 4.3.0) #&gt; fastmatch 1.1-4 2023-08-18 [1] RSPM #&gt; forcats 1.0.0 2023-01-29 [1] RSPM (R 4.3.0) #&gt; generics 0.1.3 2022-07-05 [1] RSPM (R 4.3.0) #&gt; gibasa 1.0.1 2023-12-03 [1] RSPM #&gt; glue 1.6.2 2022-02-24 [1] RSPM (R 4.3.0) #&gt; hms 1.1.3 2023-03-21 [1] RSPM (R 4.3.0) #&gt; htmltools 0.5.6.1 2023-10-06 [1] RSPM (R 4.3.0) #&gt; htmlwidgets 1.6.2 2023-03-17 [1] RSPM (R 4.3.0) #&gt; httpuv 1.6.12 2023-10-23 [1] RSPM (R 4.3.0) #&gt; janeaustenr 1.0.0 2022-08-26 [1] RSPM #&gt; jquerylib 0.1.4 2021-04-26 [1] RSPM (R 4.3.0) #&gt; jsonlite 1.8.8 2023-12-04 [1] RSPM #&gt; knitr 1.44 2023-09-11 [1] RSPM (R 4.3.0) #&gt; later 1.3.1 2023-05-02 [1] RSPM (R 4.3.0) #&gt; lattice 0.21-9 2023-10-01 [2] CRAN (R 4.3.2) #&gt; lifecycle 1.0.3 2022-10-07 [1] RSPM (R 4.3.0) #&gt; magrittr 2.0.3 2022-03-30 [1] RSPM (R 4.3.0) #&gt; Matrix 1.6-1.1 2023-09-18 [2] CRAN (R 4.3.2) #&gt; memoise 2.0.1 2021-11-26 [1] RSPM (R 4.3.0) #&gt; mime 0.12 2021-09-28 [1] RSPM (R 4.3.0) #&gt; pillar 1.9.0 2023-03-22 [1] RSPM (R 4.3.0) #&gt; pkgconfig 2.0.3 2019-09-22 [1] RSPM (R 4.3.0) #&gt; promises 1.2.1 2023-08-10 [1] RSPM (R 4.3.0) #&gt; purrr 1.0.2 2023-08-10 [1] RSPM (R 4.3.0) #&gt; quanteda 3.3.1 2023-05-18 [1] RSPM #&gt; R.cache 0.16.0 2022-07-21 [1] RSPM #&gt; R.methodsS3 1.8.2 2022-06-13 [1] RSPM #&gt; R.oo 1.25.0 2022-06-12 [1] RSPM #&gt; R.utils 2.12.2 2022-11-11 [1] RSPM #&gt; R6 2.5.1 2021-08-19 [1] RSPM (R 4.3.0) #&gt; Rcpp 1.0.11 2023-07-06 [1] RSPM (R 4.3.0) #&gt; RcppParallel 5.1.7 2023-02-27 [1] RSPM #&gt; reactable 0.4.4 2023-03-12 [1] RSPM #&gt; reactR 0.5.0 2023-10-11 [1] RSPM #&gt; readr 2.1.4 2023-02-10 [1] RSPM (R 4.3.0) #&gt; rlang 1.1.2 2023-11-04 [1] RSPM #&gt; rmarkdown 2.25 2023-09-18 [1] RSPM #&gt; rstudioapi 0.15.0 2023-07-07 [1] RSPM (R 4.3.0) #&gt; sass 0.4.7 2023-07-15 [1] RSPM (R 4.3.0) #&gt; servr 0.27 2023-05-02 [1] RSPM (R 4.3.0) #&gt; sessioninfo 1.2.2 2021-12-06 [1] RSPM #&gt; SnowballC 0.7.1 2023-04-25 [1] RSPM #&gt; stopwords 2.3 2021-10-28 [1] RSPM #&gt; stringi 1.7.12 2023-01-11 [1] RSPM (R 4.3.0) #&gt; stringr 1.5.0 2022-12-02 [1] RSPM (R 4.3.0) #&gt; styler 1.10.2 2023-08-29 [1] RSPM #&gt; tibble 3.2.1 2023-03-20 [1] RSPM (R 4.3.0) #&gt; tidylo 0.2.0 2022-03-22 [1] RSPM #&gt; tidyr 1.3.0 2023-01-24 [1] RSPM (R 4.3.0) #&gt; tidyselect 1.2.0 2022-10-10 [1] RSPM (R 4.3.0) #&gt; tidytext 0.4.1 2023-01-07 [1] RSPM #&gt; tokenizers 0.3.0 2022-12-22 [1] RSPM #&gt; tzdb 0.4.0 2023-05-12 [1] RSPM (R 4.3.0) #&gt; udpipe 0.8.11 2023-01-06 [1] RSPM #&gt; utf8 1.2.4 2023-10-22 [1] RSPM (R 4.3.0) #&gt; V8 4.4.1 2023-12-04 [1] RSPM #&gt; vctrs 0.6.4 2023-10-12 [1] RSPM (R 4.3.0) #&gt; vroom 1.6.4 2023-10-02 [1] RSPM (R 4.3.0) #&gt; withr 2.5.1 2023-09-26 [1] RSPM (R 4.3.0) #&gt; xfun 0.40 2023-08-09 [1] RSPM (R 4.3.0) #&gt; yaml 2.3.7 2023-01-23 [1] RSPM (R 4.3.0) #&gt; #&gt; [1] /usr/local/lib/R/site-library #&gt; [2] /usr/local/lib/R/library #&gt; #&gt; ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── "],["misc.html", "Chapter 7 Appendix 7.1 Posit Cloud（旧・RStudio Cloud）でのgibasaの使い方 7.2 MeCabの辞書をビルドするには", " Chapter 7 Appendix 7.1 Posit Cloud（旧・RStudio Cloud）でのgibasaの使い方 Rによるデータ分析を手軽に試したい場合には、Posit Cloud（旧・RStudio Cloud）のようなクラウド環境が便利かもしれません。 一方で、Posit Cloudはユーザー権限しかない環境のため、gibasaを使えるようにするまでにはややコツが要ります。とはいえ、gibasaはRMeCabとは異なり、MeCabのバイナリはなくても使える（辞書とmecabrcがあればよい）ので、RMeCabを使う場合ほど複雑なことをする必要はないはずです。 ここでは、Posit Cloudでgibasaを利用できるようにするための手順を簡単に説明します（RMeCabもあわせて試したいという場合には、MeCabのバイナリを自分でビルドする必要があります。その場合はこの記事などを参考にしてください）。 7.1.1 辞書（ipadic, unidic-lite）の配置 MeCabの辞書は、Terminalタブからpipでインストールできます。ここでは、IPA辞書（ipadic）とunidic-liteをインストールします。 python3 -m pip install ipadic unidic-lite python3 -c &quot;import ipadic; print(&#39;dicdir=&#39; + ipadic.DICDIR);&quot; &gt; ~/.mecabrc 7.1.2 gibasaのインストール gibasaをインストールします。 install.packages(&quot;gibasa&quot;) 7.1.3 試すには うまくいっていると、辞書を指定しない場合はIPA辞書が使われます。unidic-liteはsys_dic引数にフルパスを指定することで使用できます。 gibasa::tokenize(&quot;こんにちは&quot;) unidic_lite &lt;- path.expand(&quot;~/.local/lib/python3.8/site-packages/unidic_lite/dicdir&quot;) gibasa::tokenize(&quot;こんにちは&quot;, sys_dic = unidic_lite) |&gt; gibasa::prettify(into = gibasa::get_dict_features(&quot;unidic26&quot;)) 7.2 MeCabの辞書をビルドするには v1.0.1から、gibasaを使ってMeCabのシステム辞書やユーザー辞書をビルドできるようになりました。以下では、gibasaを使ってMeCabの辞書をビルドする方法を紹介します。 MeCabの辞書は、各行が次のようなデータからなる「ヘッダーなしCSVファイル」を用意して、それらをもとに生成します。 ...の部分は見出し語の品詞情報で、ビルドしたい辞書によって異なります。IPA辞書の場合、品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音をこの通りの順番で記述します。 表層形,左文脈ID,右文脈ID,コスト,... 左文脈IDと右文脈IDは、品詞情報が正確に書かれている場合、空にしておくと自動で補完されます。 しかし、品詞情報を正確に書くには、おそらく当の左文脈IDと右文脈IDを含む出力を確認する必要があるため、ふつうに確認した値で埋めてしまったほうが確実です。 ここでは例として、「月ノ美兎」（ANYCOLOR社が運営する「にじさんじ」所属のVTuberの名前）という語彙を含む文をIPA辞書を使いつつ狙いどおりに解析してみましょう。 7.2.1 必要な品詞情報を確認する gibasa::posDebugRcppは、与えられた文字列について、MeCabの-aオプションに相当する解析結果（解析結果になりえるすべての形態素の組み合わせ）を出力する関数です。 ここでの最適解（is_best == \"01\"）である結果について確認すると、「月ノ美兎」という語彙は次のように複数の形態素に分割されてしまっていることがわかります。 gibasa::posDebugRcpp(&quot;月ノ美兎は箱の中&quot;) |&gt; dplyr::filter(is_best == &quot;01&quot;) #&gt; doc_id pos_id surface feature stat lcAttr rcAttr alpha beta is_best prob wcost cost #&gt; 1 1 0 BOS/EOS,*,*,*,*,*,*,*,* 02 0 0 0.00 -22144.50 01 0 0 0 #&gt; 2 1 38 月 名詞,一般,*,*,*,*,月,ツキ,ツキ 00 1285 1285 -6190.50 -15954.00 01 1 8537 8254 #&gt; 3 1 4 ノ 記号,一般,*,*,*,*,ノ,ノ,ノ 00 5 5 -8874.75 -13269.75 01 1 4929 11833 #&gt; 4 1 44 美 名詞,固有名詞,人名,名,*,*,美,ヨシ,ヨシ 00 1291 1291 -13974.00 -8170.50 01 1 7885 18632 #&gt; 5 1 38 兎 名詞,一般,*,*,*,*,兎,ウサギ,ウサギ 00 1285 1285 -18080.25 -4064.25 01 1 5290 24107 #&gt; 6 1 16 は 助詞,係助詞,*,*,*,*,は,ハ,ワ 00 261 261 -18095.25 -4049.25 01 1 3865 24127 #&gt; 7 1 38 箱 名詞,一般,*,*,*,*,箱,ハコ,ハコ 00 1285 1285 -22729.50 585.00 01 1 6142 30306 #&gt; 8 1 24 の 助詞,連体化,*,*,*,*,の,ノ,ノ 00 368 368 -23010.00 865.50 01 1 4816 30680 #&gt; 9 1 66 中 名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ 00 1313 1313 -24007.50 1863.00 01 1 6528 32010 #&gt; 10 1 0 BOS/EOS,*,*,*,*,*,*,*,* 03 0 0 -22144.50 0.00 01 1 0 29526 このような語については、こちらのビネットで説明しているように、制約付き解析を使って次のように強制的に抽出することもできます。 gibasa::posDebugRcpp(&quot;月ノ\\t*\\n美兎\\t*\\nは箱の中&quot;, partial = TRUE) #&gt; doc_id pos_id surface feature stat lcAttr rcAttr alpha beta is_best prob wcost cost #&gt; 1 1 0 BOS/EOS,*,*,*,*,*,*,*,* 02 0 0 0.00 -19563.75 01 0 0 0 #&gt; 2 1 38 月ノ 名詞,一般,*,*,*,*,* 01 1285 1285 -6883.50 -12680.25 01 1 9461 9178 #&gt; 3 1 38 美兎 名詞,一般,*,*,*,*,* 01 1285 1285 -15499.50 -4064.25 01 1 11426 20666 #&gt; 4 1 16 は 助詞,係助詞,*,*,*,*,は,ハ,ワ 00 261 261 -15514.50 -4049.25 01 1 3865 20686 #&gt; 5 1 38 箱 名詞,一般,*,*,*,*,箱,ハコ,ハコ 00 1285 1285 -20148.75 585.00 01 1 6142 26865 #&gt; 6 1 24 の 助詞,連体化,*,*,*,*,の,ノ,ノ 00 368 368 -20429.25 865.50 01 1 4816 27239 #&gt; 7 1 66 中 名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ 00 1313 1313 -21426.75 1863.00 01 1 6528 28569 #&gt; 8 1 0 BOS/EOS,*,*,*,*,*,*,*,* 03 0 0 -19563.75 0.00 01 1 0 26085 一方で、IPA辞書には、たとえば「早見」のような名詞,固有名詞,人名,姓,...という品詞と、「沙織」のような名詞,固有名詞,人名,名,...という品詞があります。 このような解析結果としてより望ましい品詞を確認するには、正しく解析させたい語（ここでは「月ノ美兎」）と同じような使われ方をする語（たとえば「早見沙織」）を実際に解析してみて、その結果を確認するとよいでしょう。 gibasa::posDebugRcpp(&quot;早見沙織のラジオ番組&quot;) |&gt; dplyr::filter(is_best == &quot;01&quot;) #&gt; doc_id pos_id surface feature stat lcAttr rcAttr alpha beta is_best prob wcost #&gt; 1 1 0 BOS/EOS,*,*,*,*,*,*,*,* 02 0 0 0.00 -10104.75 01 0 0 #&gt; 2 1 43 早見 名詞,固有名詞,人名,姓,*,*,早見,ハヤミ,ハヤミ 00 1290 1290 -4362.75 -5742.00 01 1 7472 #&gt; 3 1 44 沙織 名詞,固有名詞,人名,名,*,*,沙織,サオリ,サオリ 00 1291 1291 -5452.50 -4652.25 01 1 8462 #&gt; 4 1 24 の 助詞,連体化,*,*,*,*,の,ノ,ノ 00 368 368 -6658.50 -3446.25 01 1 4816 #&gt; 5 1 38 ラジオ 名詞,一般,*,*,*,*,ラジオ,ラジオ,ラジオ 00 1285 1285 -7886.25 -2218.50 01 1 3942 #&gt; 6 1 38 番組 名詞,一般,*,*,*,*,番組,バングミ,バングミ 00 1285 1285 -10534.50 429.75 01 1 3469 #&gt; 7 1 0 BOS/EOS,*,*,*,*,*,*,*,* 03 0 0 -10104.75 0.00 01 1 0 #&gt; cost #&gt; 1 0 #&gt; 2 5817 #&gt; 3 7270 #&gt; 4 8878 #&gt; 5 10515 #&gt; 6 14046 #&gt; 7 13473 この結果は狙いどおりのものであるため、「月ノ美兎」を正しく解析するために用意するCSVファイルは、仮に次のように作成しておくことができそうです。 writeLines( c( &quot;月ノ,1290,1290,7472,名詞,固有名詞,人名,姓,*,*,月ノ,ツキノ,ツキノ&quot;, &quot;美兎,1291,1291,8462,名詞,固有名詞,人名,名,*,*,美兎,ミト,ミト&quot; ), con = (csv_file &lt;- tempfile(fileext = &quot;.csv&quot;)) ) 7.2.2 ユーザー辞書のビルド 試しに、ユーザー辞書をビルドしてみましょう。gibasaでユーザー辞書をビルドするには、gibasa::build_user_dicを使います。 ユーザー辞書をビルドするにはシステム辞書が必要なため、あらかじめシステム辞書（ここではIPA辞書）が適切に配置されていることを確認しておいてください。 次のようにしてユーザー辞書をビルドできます。 gibasa::build_user_dic( dic_dir = stringr::str_sub(gibasa::dictionary_info()$file_path, end = -8), file = (user_dic &lt;- tempfile(fileext = &quot;.dic&quot;)), csv_file = csv_file, encoding = &quot;utf8&quot; ) #&gt; reading /tmp/Rtmp3PPYTw/file3c9724d85bf.csv ... 2 #&gt; #&gt; done! なお、gibasaによる辞書のビルド時の注意点として、「MeCab: 単語の追加方法」で案内されている「コストの自動推定機能」はgibasaからは利用できません。 追加したい見出し語の生起コストは空にせず、必ず適当な値で埋めるようにしてください。 さて、ビルドしたユーザー辞書を使ってみましょう。 gibasa::dictionary_info(user_dic = user_dic) #&gt; file_path charset lsize rsize size type version #&gt; 1 /usr/local/lib/python3.10/dist-packages/ipadic/dicdir/sys.dic utf8 1316 1316 392126 0 102 #&gt; 2 /tmp/Rtmp3PPYTw/file3c9689fd198.dic utf8 1316 1316 2 1 102 gibasa::tokenize(&quot;月ノ美兎は箱の中&quot;, user_dic = user_dic) #&gt; # A tibble: 6 × 5 #&gt; doc_id sentence_id token_id token feature #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 1 1 月ノ 名詞,固有名詞,人名,姓,*,*,月ノ,ツキノ,ツキノ #&gt; 2 1 1 2 美兎 名詞,固有名詞,人名,名,*,*,美兎,ミト,ミト #&gt; 3 1 1 3 は 助詞,係助詞,*,*,*,*,は,ハ,ワ #&gt; 4 1 1 4 箱 名詞,一般,*,*,*,*,箱,ハコ,ハコ #&gt; 5 1 1 5 の 助詞,連体化,*,*,*,*,の,ノ,ノ #&gt; 6 1 1 6 中 名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ 狙いどおりに解析できているようです。 7.2.3 生起コストを調整する ここまでに紹介したようなやり方で辞書を整備することで、おおむね狙いどおりの解析結果を得られるようになると思われますが、 追加した語のかたちによっては、生起コストをより小さな値に調整しないと、一部の文において正しく切り出されない場合があるかもしれません。 たとえば、こちらの記事で紹介されているように、 仮に高等学校,1285,1285,5078,名詞,一般,*,*,*,*,高等学校,コウトウガッコウ,コートーガッコーという見出し語を追加したとしても、 与える文によっては高等学校が狙いどおりに切り出されません。 writeLines( c( &quot;高等学校,1285,1285,5078,名詞,一般,*,*,*,*,高等学校,コウトウガッコウ,コートーガッコー&quot; ), con = (csv_file &lt;- tempfile(fileext = &quot;.csv&quot;)) ) gibasa::build_user_dic( dic_dir = stringr::str_sub(gibasa::dictionary_info()$file_path, end = -8), file = (user_dic &lt;- tempfile(fileext = &quot;.dic&quot;)), csv_file = csv_file, encoding = &quot;utf8&quot; ) #&gt; reading /tmp/Rtmp3PPYTw/file3c911d9d8b8.csv ... 1 #&gt; #&gt; done! gibasa::tokenize( c( &quot;九州高等学校ゴルフ選手権&quot;, &quot;地元の高等学校に進学した&quot;, &quot;帝京高等学校のエースとして活躍&quot;, &quot;開成高等学校117人が現役合格&quot;, &quot;マンガを高等学校の授業で使う&quot; ), user_dic = user_dic ) |&gt; gibasa::pack() #&gt; # A tibble: 5 × 2 #&gt; doc_id text #&gt; &lt;fct&gt; &lt;chr&gt; #&gt; 1 1 九州 高等 学校 ゴルフ 選手権 #&gt; 2 2 地元 の 高等 学校 に 進学 し た #&gt; 3 3 帝京 高等 学校 の エース として 活躍 #&gt; 4 4 開成 高等 学校 117 人 が 現役 合格 #&gt; 5 5 マンガ を 高等 学校 の 授業 で 使う この例のように、複数の既存の見出し語のほうが優先されてしまう場合には、追加する見出し語の生起コストを小さくすることによって、 狙いどおりの解析結果を得ることができます。 先ほどの記事のなかで紹介されているのと同じやり方で、適切な生起コストをgibasaを使って求めるには、たとえば次のような関数を用意します（やや複雑なのでバグがあるかもしれません）。 calc_adjusted_cost &lt;- \\(sentences, target_word, sys_dic = &quot;&quot;, user_dic = &quot;&quot;) { sentences_mod &lt;- stringi::stri_replace_all_regex( sentences, pattern = paste0(&quot;(?&lt;target&gt;(&quot;, target_word, &quot;))&quot;), replacement = &quot;\\n${target}\\t*\\n&quot;, vectorize_all = FALSE ) calc_cumcost &lt;- \\(x) { ret &lt;- gibasa::posDebugRcpp(x, sys_dic = sys_dic, user_dic = user_dic, partial = TRUE) |&gt; dplyr::mutate( lcAttr = dplyr::lead(lcAttr, default = 0), cost = purrr::map2_dbl( rcAttr, lcAttr, ~ gibasa::get_transition_cost(.x, .y, sys_dic = sys_dic, user_dic = user_dic) ), wcost = cumsum(wcost), cost = cumsum(cost), ## 1行目のBOS/EOS-&gt;BOS/EOS間の連接コストを足しすぎてしまうので、引く total_cost = wcost + cost - gibasa::get_transition_cost(0, 0, sys_dic = sys_dic, user_dic = user_dic), .by = doc_id ) |&gt; dplyr::slice_tail(n = 1, by = doc_id) |&gt; dplyr::pull(&quot;total_cost&quot;) ret } cost1 &lt;- calc_cumcost(sentences) cost2 &lt;- calc_cumcost(sentences_mod) gibasa::posDebugRcpp(sentences_mod, sys_dic = sys_dic, user_dic = user_dic, partial = TRUE) |&gt; dplyr::filter(surface %in% target_word) |&gt; dplyr::reframe( stat = stat, surface = surface, pos_id = pos_id, feature = feature, lcAttr = lcAttr, rcAttr = rcAttr, current_cost = wcost, adjusted_cost = wcost + (cost1[doc_id] - cost2[doc_id] - 1) ) |&gt; dplyr::slice_min(adjusted_cost, n = 1, by = surface) } この関数を使って、実際に適切な生起コストを計算してみます。 adjusted_cost &lt;- calc_adjusted_cost( c( &quot;九州高等学校ゴルフ選手権&quot;, &quot;地元の高等学校に進学した&quot;, &quot;帝京高等学校のエースとして活躍&quot;, &quot;開成高等学校117人が現役合格&quot;, &quot;マンガを高等学校の授業で使う&quot; ), target_word = &quot;高等学校&quot;, user_dic = user_dic ) この生起コストを使って改めてユーザー辞書をビルドし、結果を確認してみましょう。 adjusted_cost |&gt; tidyr::unite( csv_body, surface, lcAttr, rcAttr, adjusted_cost, feature, sep = &quot;,&quot; ) |&gt; dplyr::pull(&quot;csv_body&quot;) |&gt; writeLines(con = (csv_file &lt;- tempfile(fileext = &quot;.csv&quot;))) gibasa::build_user_dic( dic_dir = stringr::str_sub(gibasa::dictionary_info()$file_path, end = -8), file = (user_dic &lt;- tempfile(fileext = &quot;.dic&quot;)), csv_file = csv_file, encoding = &quot;utf8&quot; ) #&gt; reading /tmp/Rtmp3PPYTw/file3c933dc86d.csv ... 1 #&gt; #&gt; done! gibasa::tokenize( c( &quot;九州高等学校ゴルフ選手権&quot;, &quot;地元の高等学校に進学した&quot;, &quot;帝京高等学校のエースとして活躍&quot;, &quot;開成高等学校117人が現役合格&quot;, &quot;マンガを高等学校の授業で使う&quot; ), user_dic = user_dic ) |&gt; gibasa::pack() #&gt; # A tibble: 5 × 2 #&gt; doc_id text #&gt; &lt;fct&gt; &lt;chr&gt; #&gt; 1 1 九州 高等学校 ゴルフ 選手権 #&gt; 2 2 地元 の 高等学校 に 進学 し た #&gt; 3 3 帝京 高等学校 の エース として 活躍 #&gt; 4 4 開成 高等学校 117 人 が 現役 合格 #&gt; 5 5 マンガ を 高等学校 の 授業 で 使う 今度はうまくいっていそうです。 7.2.4 システム辞書のビルド ユーザー辞書ではなく、システム辞書をビルドすることもできます。 ただ、ふつうに入手できるIPA辞書のソースの文字コードはEUC-JPであり、UTF-8のCSVファイルと混在させることができないため、扱いに注意が必要です。 また、UniDicの2.3.xについては、同梱されているファイルに問題があるようで、そのままではビルドできないという話があるようです（参考）。 UniDicはIPA辞書に比べるとビルドするのにそれなりにメモリが必要なことからも、とくに事情がないかぎりはビルド済みのバイナリ辞書をダウンロードしてきて使ったほうがよいでしょう。 ipadic_temp &lt;- tempfile(fileext = &quot;.tar.gz&quot;) download.file(&quot;https://github.com/shogo82148/mecab/releases/download/v0.996.9/mecab-ipadic-2.7.0-20070801.tar.gz&quot;, destfile = ipadic_temp) untar(ipadic_temp, exdir = tempdir()) gibasa::build_sys_dic( dic_dir = file.path(tempdir(), &quot;mecab-ipadic-2.7.0-20070801&quot;), out_dir = tempdir(), encoding = &quot;euc-jp&quot; ) #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/unk.def ... 40 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Noun.proper.csv ... 27327 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Adnominal.csv ... 135 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Filler.csv ... 19 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Interjection.csv ... 252 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Noun.number.csv ... 42 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Noun.csv ... 60477 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Postp-col.csv ... 91 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Adj.csv ... 27210 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Prefix.csv ... 221 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Noun.adjv.csv ... 3328 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Noun.adverbal.csv ... 795 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Adverb.csv ... 3032 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Suffix.csv ... 1393 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Noun.place.csv ... 72999 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Noun.demonst.csv ... 120 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Others.csv ... 2 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Symbol.csv ... 208 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Noun.org.csv ... 16668 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Postp.csv ... 146 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Noun.others.csv ... 151 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Noun.nai.csv ... 42 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Noun.verbal.csv ... 12146 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Noun.name.csv ... 34202 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Auxil.csv ... 199 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Verb.csv ... 130750 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/Conjunction.csv ... 171 #&gt; reading /tmp/Rtmp3PPYTw/mecab-ipadic-2.7.0-20070801/matrix.def ... 1316x1316 #&gt; #&gt; done! # `dicrc`ファイルをビルドした辞書のあるディレクトリにコピーする file.copy(file.path(tempdir(), &quot;mecab-ipadic-2.7.0-20070801/dicrc&quot;), tempdir()) #&gt; [1] FALSE # ここでは`mecabrc`ファイルが適切な位置に配置されていないという想定で、 # `mecabrc`ファイルを偽装している。 withr::with_envvar( c( &quot;MECABRC&quot; = if (.Platform$OS.type == &quot;windows&quot;) { &quot;nul&quot; } else { &quot;/dev/null&quot; } ), gibasa::tokenize(&quot;月ノ美兎は箱の中&quot;, sys_dic = tempdir()) ) #&gt; # A tibble: 8 × 5 #&gt; doc_id sentence_id token_id token feature #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 1 1 月 名詞,一般,*,*,*,*,月,ツキ,ツキ #&gt; 2 1 1 2 ノ 記号,一般,*,*,*,*,ノ,ノ,ノ #&gt; 3 1 1 3 美 名詞,固有名詞,人名,名,*,*,美,ヨシ,ヨシ #&gt; 4 1 1 4 兎 名詞,一般,*,*,*,*,兎,ウサギ,ウサギ #&gt; 5 1 1 5 は 助詞,係助詞,*,*,*,*,は,ハ,ワ #&gt; 6 1 1 6 箱 名詞,一般,*,*,*,*,箱,ハコ,ハコ #&gt; 7 1 1 7 の 助詞,連体化,*,*,*,*,の,ノ,ノ #&gt; 8 1 1 8 中 名詞,非自立,副詞可能,*,*,*,中,ナカ,ナカ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
