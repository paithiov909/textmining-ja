[["index.html", "RとMeCabによる日本語テキストマイニングの前処理 はじめに 0.1 この資料について 0.2 Rでテキストマイニングするということ", " RとMeCabによる日本語テキストマイニングの前処理 Akiru Kato 2022-10-07 はじめに 0.1 この資料について 0.1.1 この資料でやりたいこと gibasaやその他のRパッケージを使って、RMeCabでできるようなテキストマイニングの前処理をより見通しよくおこなうやり方を紹介します。 0.1.2 想定する知識など R言語の基本的な使い方の説明はしません。 0.2 Rでテキストマイニングするということ 0.2.1 テキストを分析して何がしたいのか テキストマイニングに関する入門的な本だと、「テキストマイニングとは何か」みたいな話から入るような気がします。ここでは必ずしも入門的な内容をめざしてはいませんが、しかし、すこし考えてみましょう。テキストマイニングとはなんでしょうか。 自然言語処理というのは、まあいろいろと思想はあるでしょうが、総じて「テキストを機械的に処理してごにょごにょする」技術のことだと思います。自然言語処理界隈の論文などを眺めていると、その範囲はかなり広くて、文書要約から文書生成といったタスクまで含まれるようです。 そのなかでもテキストマイニングというと、「テキストから特徴量をつくって何かを分析する」みたいな部分にフォーカスしてくるのではないでしょうか。 素人考えですが、テキストマイニングとはしたがってデータ分析のことです。そのため、前提としてテキストを分析して何がしたいのか（＝何ができるのか）を見通しよくしておくと、嬉しいことが多い気がします。 0.2.2 テキストマイニングでめざすこと・できること CRISP-DM (Cross-Industry Standard Process for Data Mining) は、IBMを中心としたコンソーシアムが提案したデータマイニングのための標準プロセスです。 これはデータ分析をビジネスに活かすことを念頭においてつくられた「課題ドリブン」なプロセスであるため、場合によってはそのまま採用できないかもしれませんが、こうした標準プロセスを押さえておくことは、分析プロセスを設計するうえで有用だと思います。 CRISP-DMは以下の6つの段階（phases）を行ったり来たりすることで進められていきます。 Business Understanding Data Understanding Data Preparation Modeling Evaluation Deployment CRISP-DMはデータ分析を通じて達成したいことから分析をスタートしていく、ある意味でトップダウン的なプロセスです。しかし、データからの知見の発掘はそんなにトップダウン一直線にはうまくいかないものです。いわばボトムアップ的にも、段階を「行ったり来たり」しながら分析を進めるためには、データ分析でとれるカードをなんとなく把握しておく必要があります。 これも素人考えですが、私たちがデータ分析でとれるカードというのは、だいたい次の３つくらいのものです。 モデルをつくって何かの回帰をする モデルをつくって何かの分類をする グループに分けて違いを評価する そのために、これらの落としどころに持ち込むためのテキストの特徴量をどうにかしてつくること（前処理）が、私たちが実際におこなうテキストマイニングの大きな部分を占めるように思います。 そして、それらの特徴量は、テキストについて何かを数えた頻度または比率とそれらを変換したものだと思っておくとすっきりします。数を数える「何か」というのは、たとえば語だったり品詞だったり、それらのNgramだったり、その他のタグ付けされた情報だったりします。 0.2.3 テキストマイニングの流れ テキストマイニングの大まかな流れは、イメージ的には、次のような感じになります。 分析したいテキストをいっぱい集める 1.1. 分析して何がしたいか考える 1.2. そのためにつくるべき特徴量を考える 特徴量をつくる 2.1. 正規化などの文字列処理 2.2. トークナイズ・ステミング・レメタイズ 2.3. 集計 2.4. 特徴量の変換や補完 分析する 3.1. 特徴量をつかってデータ分析する 3.2. 得られた結果を評価する （必要に応じて）得られた知見を活かす この資料では、この流れのなかでも、2にとくにフォーカスして、テキストの前処理のやり方を説明します。 "],["intro.html", "Chapter 1 gibasaの基本的な使い方 1.1 テキストデータ 1.2 gibasaの使い方", " Chapter 1 gibasaの基本的な使い方 1.1 テキストデータ ここでは、audubonパッケージに含まれているaudubon::polanoというデータを例にgibasaの基本的な使い方を説明していきます。このデータは、青空文庫で公開されている、宮沢賢治の「ポラーノの広場」という小説を、改行ごとにひとつの要素としてベクトルにしたものです。 このデータを、次のようなかたちのデータフレーム（tibble）にします。 dat_txt &lt;- tibble::tibble( doc_id = seq_along(audubon::polano) |&gt; as.character(), text = audubon::polano ) |&gt; dplyr::mutate(text = audubon::strj_normalize(text)) str(dat_txt) ## tibble [899 × 2] (S3: tbl_df/tbl/data.frame) ## $ doc_id: chr [1:899] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ text : chr [1:899] &quot;ポラーノの広場&quot; &quot;宮沢賢治&quot; &quot;前十七等官レオーノ・キュースト誌&quot; &quot;宮沢賢治訳述&quot; ... このかたちのデータフレームは、Text Interchange Formats（TIF）という仕様を念頭においている形式です（ちなみに、このかたちのデータフレームはreadtextパッケージを使うと簡単に得ることができます）。 Text Interchange Formats（TIF）は、2017年にrOpenSci Text Workshopで整備された、テキスト分析用のRパッケージのデザインパターンのようなものです。 TIFでは、コーパス（corpus）、文書単語行列（dtm）、トークン（token）という3種類のオブジェクトの形式が定義されており、異なるパッケージ間で同様の形式を扱うようにすることで、複数のパッケージを通じて便利にテキスト分析を進められるようになっています。 上のdat_txtは、文書の集合であるコーパスをデータフレームのかたちで保持したものです。この形式のデータフレームは、次のように、tidytextやtokenizersの関数にそのまま渡すことができます。なお、これらの形式のオブジェクトは、TIFの枠組みのなかではトークンと呼ばれます。 dat_txt |&gt; tidytext::unnest_tokens(token, text) |&gt; head(4) ## # A tibble: 4 × 2 ## doc_id token ## &lt;chr&gt; &lt;chr&gt; ## 1 1 ポラーノ ## 2 1 の ## 3 1 広場 ## 4 2 宮沢 dat_txt |&gt; tokenizers::tokenize_words() |&gt; head(4) ## $`1` ## [1] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; ## ## $`2` ## [1] &quot;宮沢&quot; &quot;賢治&quot; ## ## $`3` ## [1] &quot;前&quot; &quot;十七&quot; &quot;等&quot; &quot;官&quot; &quot;レ&quot; &quot;オー&quot; &quot;ノ&quot; &quot;キュー&quot; ## [9] &quot;スト&quot; &quot;誌&quot; ## ## $`4` ## [1] &quot;宮沢&quot; &quot;賢治&quot; &quot;訳述&quot; 1.2 gibasaの使い方 1.2.1 tokenize 前節で見たように、tokenizers::tokenize_wordsやこれを利用しているtidytext::unnest_tokensは、日本語のテキストであっても機械的にトークンのかたちに整形する（分かち書きする）ことができます。 tokenizersパッケージの分かち書きは、内部的には、ICUのBoundary Analysisによるものです。この単語境界判定は、たとえば新聞記事のような、比較的整った文体の文章ではおおむね期待通り分かち書きがおこなわれ、また、日本語と英語などが混ざっている文章であってもとくに気にすることなく、高速に分かち書きできるという強みがあります。 しかし、手元にある辞書に収録されている語の通りに分かち書きしたい場合や、品詞情報などがほしい場合には、やはりMeCabのような形態素解析器による分かち書きが便利なこともあります。 gibasaは、そのようなケースにおいて、tidytext::unnest_tokensの代わりに使用できる機能を提供するために開発しているパッケージです。この機能はgibasa::tokenizeという関数として提供していて、次のように使うことができます（ここではtibbleを渡したため戻り値がtibbleになっていますが、data.frameを渡すと戻り値はdata.frameになります）。 dat &lt;- gibasa::tokenize(dat_txt, text, doc_id) str(dat) ## tibble [26,849 × 5] (S3: tbl_df/tbl/data.frame) ## $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... ## $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... ## $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... ## $ token : chr [1:26849] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... ## $ feature : chr [1:26849] &quot;名詞,一般,*,*,*,*,*&quot; &quot;助詞,連体化,*,*,*,*,の,ノ,ノ&quot; &quot;名詞,一般,*,*,*,*,広場,ヒロバ,ヒロバ&quot; &quot;名詞,固有名詞,人名,姓,*,*,宮沢,ミヤザワ,ミヤザワ&quot; ... 1.2.2 prettify gibasa::tokenizeの戻り値のデータフレームは、それぞれのトークンについて、MeCabから返される素性情報のすべてを含んでいるfeatureという列を持っています。 MeCabから返される素性情報は、使用している辞書によって異なります。たとえば、IPA辞書やUniDic（unidic-lite）の素性は、次のような情報を持っています。 gibasa::get_dict_features(&quot;ipa&quot;) ## [1] &quot;POS1&quot; &quot;POS2&quot; &quot;POS3&quot; &quot;POS4&quot; &quot;X5StageUse1&quot; ## [6] &quot;X5StageUse2&quot; &quot;Original&quot; &quot;Yomi1&quot; &quot;Yomi2&quot; gibasa::get_dict_features(&quot;unidic26&quot;) ## [1] &quot;POS1&quot; &quot;POS2&quot; &quot;POS3&quot; &quot;POS4&quot; &quot;cType&quot; &quot;cForm&quot; ## [7] &quot;lForm&quot; &quot;lemma&quot; &quot;orth&quot; &quot;pron&quot; &quot;orthBase&quot; &quot;pronBase&quot; ## [13] &quot;goshu&quot; &quot;iType&quot; &quot;iForm&quot; &quot;fType&quot; &quot;fForm&quot; &quot;kana&quot; ## [19] &quot;kanaBase&quot; &quot;form&quot; &quot;formBase&quot; &quot;iConType&quot; &quot;fConType&quot; &quot;aType&quot; ## [25] &quot;aConType&quot; &quot;aModeType&quot; こうした素性情報をデータフレームの列にパースするには、gibasa::prettifyという関数を利用できます。 デフォルトではすべての素性についてパースしますが、col_select引数に残したい列名を指定することにより、特定の素性情報だけをパースすることもできます。このかたちのデータフレームは、解析するテキストの文章量によっては、数十万から数百万くらいの行からなることもよくあります。そのような規模のデータフレームについて、いちいちすべての素性をパースしていると、それだけでメモリを余計に消費してしまいます。メモリの消費を抑えるためにも、なるべく後で必要な素性だけをこまめに指定することをおすすめします。 str(gibasa::prettify(dat)) ## tibble [26,849 × 13] (S3: tbl_df/tbl/data.frame) ## $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... ## $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... ## $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... ## $ token : chr [1:26849] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... ## $ POS1 : chr [1:26849] &quot;名詞&quot; &quot;助詞&quot; &quot;名詞&quot; &quot;名詞&quot; ... ## $ POS2 : chr [1:26849] &quot;一般&quot; &quot;連体化&quot; &quot;一般&quot; &quot;固有名詞&quot; ... ## $ POS3 : chr [1:26849] NA NA NA &quot;人名&quot; ... ## $ POS4 : chr [1:26849] NA NA NA &quot;姓&quot; ... ## $ X5StageUse1: chr [1:26849] NA NA NA NA ... ## $ X5StageUse2: chr [1:26849] NA NA NA NA ... ## $ Original : chr [1:26849] NA &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... ## $ Yomi1 : chr [1:26849] NA &quot;ノ&quot; &quot;ヒロバ&quot; &quot;ミヤザワ&quot; ... ## $ Yomi2 : chr [1:26849] NA &quot;ノ&quot; &quot;ヒロバ&quot; &quot;ミヤザワ&quot; ... str(gibasa::prettify(dat, col_select = c(1, 2))) ## tibble [26,849 × 6] (S3: tbl_df/tbl/data.frame) ## $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... ## $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... ## $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... ## $ token : chr [1:26849] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... ## $ POS1 : chr [1:26849] &quot;名詞&quot; &quot;助詞&quot; &quot;名詞&quot; &quot;名詞&quot; ... ## $ POS2 : chr [1:26849] &quot;一般&quot; &quot;連体化&quot; &quot;一般&quot; &quot;固有名詞&quot; ... str(gibasa::prettify(dat, col_select = c(&quot;POS1&quot;, &quot;Original&quot;))) ## tibble [26,849 × 6] (S3: tbl_df/tbl/data.frame) ## $ doc_id : Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 2 2 3 3 3 3 3 ... ## $ sentence_id: int [1:26849] 1 1 1 2 2 3 3 3 3 3 ... ## $ token_id : int [1:26849] 1 2 3 1 2 1 2 3 4 5 ... ## $ token : chr [1:26849] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... ## $ POS1 : chr [1:26849] &quot;名詞&quot; &quot;助詞&quot; &quot;名詞&quot; &quot;名詞&quot; ... ## $ Original : chr [1:26849] NA &quot;の&quot; &quot;広場&quot; &quot;宮沢&quot; ... 1.2.3 pack gibasa::packという関数を使うと、トークンの形式のデータフレームから、各トークンを半角スペースで区切ったコーパスの形式のデータフレームにすることができます。 dat_corpus &lt;- dat |&gt; gibasa::pack() str(dat_corpus) ## &#39;data.frame&#39;: 899 obs. of 2 variables: ## $ doc_id: chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ text : chr &quot;ポラーノ の 広場&quot; &quot;宮沢 賢治&quot; &quot;前 十 七 等 官 レオーノ・キュースト 誌&quot; &quot;宮沢 賢治 訳述&quot; ... このかたちのデータフレームはTIFに準拠していたため、他のパッケージと組み合わせて使うのに便利なことがあります。たとえば、このかたちから、次のようにtidytext::unnest_tokensと組み合わせて、もう一度トークンの形式のデータフレームに戻すことができます。 dat_corpus |&gt; tidytext::unnest_tokens(token, text, token = \\(x) { strsplit(x, &quot; +&quot;) }) |&gt; head(4) ## doc_id token ## 1 1 ポラーノ ## 2 1 の ## 3 1 広場 ## 4 2 宮沢 あるいは、次のようにquantedaと組み合わせて使うこともできます。 dat_corpus |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;, remove_punct = FALSE) ## Tokens consisting of 899 documents. ## 1 : ## [1] &quot;ポラーノ&quot; &quot;の&quot; &quot;広場&quot; ## ## 2 : ## [1] &quot;宮沢&quot; &quot;賢治&quot; ## ## 3 : ## [1] &quot;前&quot; &quot;十&quot; &quot;七&quot; ## [4] &quot;等&quot; &quot;官&quot; &quot;レオーノ・キュースト&quot; ## [7] &quot;誌&quot; ## ## 4 : ## [1] &quot;宮沢&quot; &quot;賢治&quot; &quot;訳述&quot; ## ## 5 : ## [1] &quot;その&quot; &quot;ころ&quot; &quot;わたくし&quot; &quot;は&quot; &quot;、&quot; &quot;モリーオ&quot; &quot;市&quot; ## [8] &quot;の&quot; &quot;博物&quot; &quot;局&quot; &quot;に&quot; &quot;勤め&quot; ## [ ... and 5 more ] ## ## 6 : ## [1] &quot;十&quot; &quot;八&quot; &quot;等&quot; &quot;官&quot; &quot;でし&quot; &quot;た&quot; &quot;から&quot; &quot;役所&quot; &quot;の&quot; &quot;なか&quot; &quot;でも&quot; ## [12] &quot;、&quot; ## [ ... and 219 more ] ## ## [ reached max_ndoc ... 893 more documents ] "],["dtm.html", "Chapter 2 トークンの集計と文書単語行列への整形 2.1 トークンの集計 2.2 文書単語行列への整形", " Chapter 2 トークンの集計と文書単語行列への整形 2.1 トークンの集計 2.1.1 品詞などにもとづくしぼりこみ トークンを簡単に集計するには、dplyrの関数群を利用するのが便利です。 たとえば、集計に先立って特定のトークンを素性情報にもとづいて選択するにはdplyr::filterを使います。 dat |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;Original&quot;)) |&gt; dplyr::filter(POS1 %in% c(&quot;名詞&quot;, &quot;動詞&quot;, &quot;形容詞&quot;)) |&gt; dplyr::slice_head(n = 5) ## # A tibble: 5 × 6 ## doc_id sentence_id token_id token POS1 Original ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 1 ポラーノ 名詞 &lt;NA&gt; ## 2 1 1 3 広場 名詞 広場 ## 3 2 2 1 宮沢 名詞 宮沢 ## 4 2 2 2 賢治 名詞 賢治 ## 5 3 3 2 十 名詞 十 2.1.2 原形の集計 dplyr::countでトークンを文書ごとに集計します。ここでは、IPA辞書の見出し語がある語については「原形（Original）」を、見出し語がない語（未知語）については表層形を数えています。 MeCabは、未知語であっても品詞の推定をおこないますが、未知語の場合には「読み（Yomi1, Yomi2）」のような一部の素性については情報を返しません。このような未知語の素性については、prettifyした結果のなかでは、NA_character_になっていることに注意してください。 dat_count &lt;- dat |&gt; gibasa::prettify(col_select = c(&quot;POS1&quot;, &quot;Original&quot;)) |&gt; dplyr::filter(POS1 %in% c(&quot;名詞&quot;, &quot;動詞&quot;, &quot;形容詞&quot;)) |&gt; dplyr::mutate( doc_id = forcats::fct_drop(doc_id), token = dplyr::if_else(is.na(Original), token, Original) ) |&gt; dplyr::count(doc_id, token) str(dat_count) ## tibble [9,723 × 3] (S3: tbl_df/tbl/data.frame) ## $ doc_id: Factor w/ 876 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 2 2 3 3 3 3 3 3 ... ## $ token : chr [1:9723] &quot;ポラーノ&quot; &quot;広場&quot; &quot;宮沢&quot; &quot;賢治&quot; ... ## $ n : int [1:9723] 1 1 1 1 1 1 1 1 1 1 ... 2.2 文書単語行列への整形 こうして集計した縦持ちの頻度表を横持ちにすると、いわゆる文書単語行列になります。 dtm &lt;- dat_count |&gt; tidyr::pivot_wider( doc_id, names_from = token, values_from = n, values_fill = 0 ) dim(dtm) ## [1] 876 2173 ただし、このようにtidyr::pivot_widerで単純に横持ちにすることは、非常に大量の列を持つ巨大なデータフレームを作成することになるため、おすすめしません。文書単語行列を作成するには、tidytext::cast_sparseやtidytext::cast_dfmなどを使って、疎行列のオブジェクトにしましょう。 dtm &lt;- dat_count |&gt; tidytext::cast_sparse(doc_id, token, n) dim(dtm) ## [1] 876 2172 "],["ngram.html", "Chapter 3 N-gram 3.1 Ngramの集計", " Chapter 3 N-gram 3.1 Ngramの集計 3.1.1 dplyrを使う方法 dplyrを使って簡単にやる場合、次のようにすると2-gramを集計できます。 bigram &lt;- audubon::ngram_tokenizer(2) dat_ngram &lt;- dat |&gt; gibasa::prettify(col_select = &quot;Original&quot;) |&gt; dplyr::mutate( token = dplyr::if_else(is.na(Original), token, Original) ) |&gt; dplyr::group_by(doc_id) |&gt; dplyr::summarise(token = bigram(token, sep = &quot;-&quot;), .groups = &quot;keep&quot;) |&gt; dplyr::count(token) |&gt; dplyr::ungroup() str(dat_ngram) ## tibble [24,398 × 3] (S3: tbl_df/tbl/data.frame) ## $ doc_id: Factor w/ 899 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 2 3 3 3 3 3 3 4 ... ## $ token : chr [1:24398] &quot;の-広場&quot; &quot;ポラーノ-の&quot; &quot;宮沢-賢治&quot; &quot;レオーノ・キュースト-誌&quot; ... ## $ n : int [1:24398] 1 1 1 1 1 1 1 1 1 1 ... 3.1.2 quantedaにNgramを持ちこむ方法 gibasa::packを使ってNgramの分かち書きをつくることもできます。この場合、次のようにquantedaの枠組みの中でNgramをトークンとして数えることで集計することができます。 dat_ngram &lt;- dat |&gt; gibasa::prettify(col_select = &quot;Original&quot;) |&gt; dplyr::mutate(token = dplyr::if_else(is.na(Original), token, Original)) |&gt; gibasa::pack(n = 2) str(dat_ngram) ## &#39;data.frame&#39;: 899 obs. of 2 variables: ## $ doc_id: chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ text : chr &quot;ポラーノ-の の-広場&quot; &quot;宮沢-賢治&quot; &quot;前-十 十-七 七-等 等-官 官-レオーノ・キュースト レオーノ・キュースト-誌&quot; &quot;宮沢-賢治 賢治-訳述&quot; ... dat_ngram |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::dfm() ## Document-feature matrix of: 899 documents, 10,890 features (99.75% sparse) and 0 docvars. ## features ## docs ポラーノ-の の-広場 宮沢-賢治 前-十 十-七 七-等 等-官 官-レオーノ・キュースト ## 1 1 1 0 0 0 0 0 0 ## 2 0 0 1 0 0 0 0 0 ## 3 0 0 0 1 1 1 1 1 ## 4 0 0 1 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 1 0 ## features ## docs レオーノ・キュースト-誌 賢治-訳述 ## 1 0 0 ## 2 0 0 ## 3 1 0 ## 4 0 1 ## 5 0 0 ## 6 0 0 ## [ reached max_ndoc ... 893 more documents, reached max_nfeat ... 10,880 more features ] 3.1.3 quantedaでNgramを数える方法 また、quantedaの枠組みの中でNgramをつくりながら数えて集計することもできます。 dat_ngram &lt;- dat |&gt; gibasa::prettify(col_select = &quot;Original&quot;) |&gt; dplyr::mutate(token = dplyr::if_else(is.na(Original), token, Original)) |&gt; gibasa::pack() str(dat_ngram) ## &#39;data.frame&#39;: 899 obs. of 2 variables: ## $ doc_id: chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ text : chr &quot;ポラーノ の 広場&quot; &quot;宮沢 賢治&quot; &quot;前 十 七 等 官 レオーノ・キュースト 誌&quot; &quot;宮沢 賢治 訳述&quot; ... dat_ngram |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::tokens_ngrams(n = 2) |&gt; quanteda::dfm() ## Document-feature matrix of: 899 documents, 10,890 features (99.75% sparse) and 0 docvars. ## features ## docs ポラーノ_の の_広場 宮沢_賢治 前_十 十_七 七_等 等_官 官_レオーノ・キュースト ## 1 1 1 0 0 0 0 0 0 ## 2 0 0 1 0 0 0 0 0 ## 3 0 0 0 1 1 1 1 1 ## 4 0 0 1 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 1 0 ## features ## docs レオーノ・キュースト_誌 賢治_訳述 ## 1 0 0 ## 2 0 0 ## 3 1 0 ## 4 0 1 ## 5 0 0 ## 6 0 0 ## [ reached max_ndoc ... 893 more documents, reached max_nfeat ... 10,880 more features ] "],["weighting.html", "Chapter 4 単語頻度の重みづけ 4.1 tidytextによる単語頻度の重みづけ 4.2 gibasaによる単語頻度の重みづけ", " Chapter 4 単語頻度の重みづけ 4.1 tidytextによる単語頻度の重みづけ tidytext::bind_tf_idfを使うと単語頻度からTF-IDFを算出することができます。 dat_count |&gt; tidytext::bind_tf_idf(token, doc_id, n) |&gt; dplyr::slice_head(n = 5) ## # A tibble: 5 × 6 ## doc_id token n tf idf tf_idf ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 ポラーノ 1 0.5 3.16 1.58 ## 2 1 広場 1 0.5 2.73 1.37 ## 3 2 宮沢 1 0.5 6.08 3.04 ## 4 2 賢治 1 0.5 6.08 3.04 ## 5 3 レオーノ・キュースト 1 0.167 5.17 0.861 tidytextにおけるTFとIDFは、RMeCabにおけるそれとは採用している計算式が異なるため、計算結果が異なります。TFはRMeCabでは生の索引語頻度（tfの場合）ですが、tidytextでは文書内での相対頻度になります。また、IDFはRMeCabでは対数の底が2であるのに対して、tidytextでは底が10であるなどの違いがあります。 4.2 gibasaによる単語頻度の重みづけ gibasaはRMeCabにおける単語頻度の重みづけをtidytext::bind_tf_idfと同様のスタイルでおこなうことができる関数gibasa::bind_tf_idf2を提供しています。 bind_tf_idf2はおおむねRMeCabの単語頻度の重みづけの挙動を再現していますが、細かな点が異なります。まず、gibasaは「コサイン正規化」がIDFの計算時のみにおこなわれるため、norm=TRUEにしてもRMeCabの計算結果とは一致しません。また、IDF=idf3時の計算結果はRMeCabとは一致しません（これはRMeCab側が間違った結果を返すためです）。 RMeCabは以下の単語頻度の重みづけをサポートしています（ただし、idf3は指定してもidf2になるため、実際には利用できません）。 局所的重み（TF） tf（索引語頻度） tf2（対数化索引語頻度） tf3（２進重み） 大域的重み（IDF） idf（文書頻度の逆数） idf2（大域的IDF） idf3（確率的IDF） idf4（エントロピー） 正規化 norm（コサイン正規化） gibasaはこれらの重みづけを再実装しています。ただし、tf=\"tf\"はgibasaでは相対頻度になるため、RMeCabのweight=\"tf*idf\"に相当する出力を得るには、たとえば次のように計算します。 dat_count |&gt; gibasa::bind_tf_idf2(token, doc_id, n) |&gt; dplyr::mutate( tf_idf = n / idf ) |&gt; dplyr::slice_head(n = 5) ## # A tibble: 5 × 6 ## doc_id token n tf idf tf_idf ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 ポラーノ 1 0.5 5.57 0.180 ## 2 1 広場 1 0.5 4.94 0.202 ## 3 2 宮沢 1 0.5 9.77 0.102 ## 4 2 賢治 1 0.5 9.77 0.102 ## 5 3 レオーノ・キュースト 1 0.167 8.45 0.118 なお、注意点として、RMeCabの単語を数える機能は、品詞情報（POS1とPOS2まで）を確認しながら単語を数えているようなので、ここでのように原形だけを見て数えた結果とは必ずしも一致しません。 "],["collocation.html", "Chapter 5 コロケーション 5.1 文書内での共起 5.2 任意のウィンドウ内での共起", " Chapter 5 コロケーション 5.1 文書内での共起 共起関係を数える機能はgibasaには実装されていません。文書内での共起を簡単に数えるには、たとえば次のようにします。 dat_fcm &lt;- dat_count |&gt; tidytext::cast_dfm(doc_id, token, n) |&gt; quanteda::fcm() dat_fcm ## Feature co-occurrence matrix of: 2,172 by 2,172 features. ## features ## features ポラーノ 広場 宮沢 賢治 レオーノ・キュースト 官 誌 七 十 等 ## ポラーノ 5 52 0 0 0 0 0 0 2 0 ## 広場 0 5 0 0 0 0 0 0 2 0 ## 宮沢 0 0 0 2 0 0 0 0 0 0 ## 賢治 0 0 0 0 0 0 0 0 0 0 ## レオーノ・キュースト 0 0 0 0 0 3 1 1 3 3 ## 官 0 0 0 0 0 0 1 1 6 5 ## 誌 0 0 0 0 0 0 0 1 1 1 ## 七 0 0 0 0 0 0 0 0 9 1 ## 十 0 0 0 0 0 0 0 0 8 6 ## 等 0 0 0 0 0 0 0 0 0 0 ## [ reached max_feat ... 2,162 more features, reached max_nfeat ... 2,162 more features ] 5.2 任意のウィンドウ内での共起 5.2.1 共起の集計 RMeCab::collocateのような任意のウィンドウの中での共起を集計するには、次のようにする必要があります。ここではwindowは前後5個のトークンを見るようにします。 dat_corpus &lt;- dat |&gt; gibasa::pack() dat_fcm &lt;- dat_corpus |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::fcm(context = &quot;window&quot;, window = 5) こうすると、nodeについて共起しているtermとその頻度を確認できます。以下では、「わたくし」というnodeと共起しているtermで頻度が上位20までであるものを表示しています。 dat_fcm &lt;- dat_fcm |&gt; tidytext::tidy() |&gt; dplyr::rename(node = document, term = term) |&gt; dplyr::filter(node == &quot;わたくし&quot;) |&gt; dplyr::slice_max(count, n = 20) dat_fcm ## # A tibble: 20 × 3 ## node term count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 わたくし は 205 ## 2 わたくし 。 122 ## 3 わたくし た 110 ## 4 わたくし て 99 ## 5 わたくし 、 91 ## 6 わたくし まし 90 ## 7 わたくし を 62 ## 8 わたくし に 61 ## 9 わたくし が 51 ## 10 わたくし し 37 ## 11 わたくし も 35 ## 12 わたくし で 33 ## 13 わたくし ども 28 ## 14 わたくし と 23 ## 15 わたくし 」 23 ## 16 わたくし です 22 ## 17 わたくし へ 17 ## 18 わたくし い 15 ## 19 わたくし 「 15 ## 20 わたくし から 14 5.2.2 T値やMI値の算出 T値やMI値は、たとえば次のようにして計算できます。 ntok &lt;- dat_corpus |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::ntoken() |&gt; sum() total &lt;- dat_corpus |&gt; quanteda::corpus() |&gt; quanteda::tokens(what = &quot;fastestword&quot;) |&gt; quanteda::tokens_select(c(&quot;わたくし&quot;, dat_fcm$term)) |&gt; quanteda::dfm() |&gt; quanteda::colSums() dat_fcm |&gt; dplyr::select(-node) |&gt; dplyr::mutate( expect = count / (total[term] / ntok * total[&quot;わたくし&quot;] * 5 * 2), t = (count - expect) / sqrt(count), mi = log2(count / expect) ) ## # A tibble: 20 × 5 ## term count expect t mi ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 は 205 2.33 14.2 6.46 ## 2 。 122 0.758 11.0 7.33 ## 3 た 110 1.02 10.4 6.75 ## 4 て 99 0.935 9.86 6.73 ## 5 、 91 0.894 9.45 6.67 ## 6 まし 90 1.40 9.34 6.00 ## 7 を 62 0.920 7.76 6.08 ## 8 に 61 0.886 7.70 6.10 ## 9 が 51 0.983 7.00 5.70 ## 10 し 37 1.53 5.83 4.59 ## 11 も 35 1.12 5.73 4.97 ## 12 で 33 0.952 5.58 5.12 ## 13 ども 28 9.02 3.59 1.64 ## 14 と 23 0.803 4.63 4.84 ## 15 」 23 0.420 4.71 5.78 ## 16 です 22 1.43 4.39 3.95 ## 17 へ 17 1.03 3.87 4.05 ## 18 い 15 0.860 3.65 4.12 ## 19 「 15 0.267 3.80 5.81 ## 20 から 14 0.685 3.56 4.35 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
