# gibasa・MeCabの使い方 {#misc}

## Posit Cloud（旧・RStudio Cloud）でのgibasaの使い方

Rによるデータ分析を手軽に試したい場合には、Posit Cloud（旧・RStudio Cloud）のようなクラウド環境が便利かもしれません。

一方で、Posit Cloudはユーザー権限しかない環境のため、gibasaを使えるようにするまでにはややコツが要ります。とはいえ、gibasaはRMeCabとは異なり、MeCabのバイナリはなくても使える（辞書とmecabrcがあればよい）ので、RMeCabを使う場合ほど複雑なことをする必要はないはずです。

ここでは、Posit Cloudでgibasaを利用できるようにするための手順を簡単に説明します（RMeCabもあわせて試したいという場合には、MeCabのバイナリを自分でビルドする必要があります。その場合は[この記事](https://blog.mana.bi/2022/12/21/posit-cloud-mecab/)などを参考にしてください）。

### 辞書（ipadic, unidic-lite）の配置

MeCabの辞書は、Terminalタブからpipでインストールできます。ここでは、IPA辞書（[ipadic](https://pypi.org/project/ipadic/)）と[unidic-lite](https://pypi.org/project/unidic-lite/)をインストールします。

```bash
python3 -m pip install ipadic unidic-lite
python3 -c "import ipadic; print('dicdir=' + ipadic.DICDIR);" > ~/.mecabrc
```

### gibasaのインストール

[gibasa](https://github.com/paithiov909/gibasa)をインストールします。

```r
install.packages("gibasa")
```

### 試すには

うまくいっていると、辞書を指定しない場合はIPA辞書が使われます。unidic-liteは`sys_dic`引数にフルパスを指定することで使用できます。

```r
gibasa::tokenize("こんにちは")
unidic_lite <- path.expand("~/.local/lib/python3.8/site-packages/unidic_lite/dicdir")
gibasa::tokenize("こんにちは", sys_dic = unidic_lite) |>
  gibasa::prettify(into = gibasa::get_dict_features("unidic26"))
```


## MeCabの辞書をビルドするには

v1.0.1から、gibasaを使ってMeCabのシステム辞書やユーザー辞書をビルドできるようになりました。以下では、gibasaを使ってMeCabの辞書をビルドする方法を紹介します。

MeCabの辞書は、各行が次のようなデータからなる「ヘッダーなしCSVファイル」を用意して、それらをもとに生成します。
`...`の部分は見出し語の品詞情報で、ビルドしたい辞書によって異なります。IPA辞書の場合、`品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音`をこの通りの順番で記述します。

```
表層形,左文脈ID,右文脈ID,コスト,...
```

左文脈IDと右文脈IDは、品詞情報が正確に書かれている場合、空にしておくと自動で補完されます。
しかし、品詞情報を正確に書くには、おそらく当の左文脈IDと右文脈IDを含む出力を確認する必要があるため、ふつうに確認した値で埋めてしまったほうが確実です。

ここでは例として、「[月ノ美兎](https://www.nijisanji.jp/talents/l/mito-tsukino)」（ANYCOLOR社が運営する「にじさんじ」所属のVTuberの名前）という語彙を含む文をIPA辞書を使いつつ狙いどおりに解析してみましょう。

### 必要な品詞情報を確認する

`gibasa::posDebugRcpp`は、与えられた文字列について、MeCabの`-a`オプションに相当する解析結果（解析結果になりえるすべての形態素の組み合わせ）を出力する関数です。
ここでの最適解（`is_best == "01"`）である結果について確認すると、「月ノ美兎」という語彙は次のように複数の形態素に分割されてしまっていることがわかります。

```{r}
gibasa::posDebugRcpp("月ノ美兎は箱の中") |>
  dplyr::filter(is_best == "01")
```

このような語については、[こちらのビネット](https://paithiov909.github.io/gibasa/articles/partial.html)で説明しているように、制約付き解析を使って次のように強制的に抽出することもできます。

```{r}
gibasa::posDebugRcpp("月ノ\t*\n美兎\t*\nは箱の中", partial = TRUE)
```

一方で、IPA辞書には、たとえば「早見」のような`名詞,固有名詞,人名,姓,...`という品詞と、「沙織」のような`名詞,固有名詞,人名,名,...`という品詞があります。
このような解析結果としてより望ましい品詞を確認するには、正しく解析させたい語（ここでは「月ノ美兎」）と同じような使われ方をする語（たとえば「早見沙織」）を実際に解析してみて、その結果を確認するとよいでしょう。

```{r}
gibasa::posDebugRcpp("早見沙織のラジオ番組") |>
  dplyr::filter(is_best == "01")
```

この結果は狙いどおりのものであるため、「月ノ美兎」を正しく解析するために用意するCSVファイルは、仮に次のように作成しておくことができそうです。

```{r}
writeLines(
  c(
    "月ノ,1290,1290,7472,名詞,固有名詞,人名,姓,*,*,月ノ,ツキノ,ツキノ",
    "美兎,1291,1291,8462,名詞,固有名詞,人名,名,*,*,美兎,ミト,ミト"
  ),
  con = (csv_file <- tempfile(fileext = ".csv"))
)
```

### ユーザー辞書のビルド

試しに、ユーザー辞書をビルドしてみましょう。gibasaでユーザー辞書をビルドするには、`gibasa::build_user_dic`を使います。
ユーザー辞書をビルドするにはシステム辞書が必要なため、あらかじめシステム辞書（ここではIPA辞書）が適切に配置されていることを確認しておいてください。

次のようにしてユーザー辞書をビルドできます。

```{r}
gibasa::build_user_dic(
  dic_dir = stringr::str_sub(gibasa::dictionary_info()$file_path, end = -8),
  file = (user_dic <- tempfile(fileext = ".dic")),
  csv_file = csv_file,
  encoding = "utf8"
)
```

なお、gibasaによる辞書のビルド時の注意点として、「[MeCab: 単語の追加方法](https://taku910.github.io/mecab/dic.html)」で案内されている「コストの自動推定機能」はgibasaからは利用できません。
追加したい見出し語の生起コストは空にせず、必ず適当な値で埋めるようにしてください。

さて、ビルドしたユーザー辞書を使ってみましょう。

```{r}
gibasa::dictionary_info(user_dic = user_dic)
gibasa::tokenize("月ノ美兎は箱の中", user_dic = user_dic)
```

狙いどおりに解析できているようです。


### 生起コストを調整する

ここまでに紹介したようなやり方で辞書を整備することで、おおむね狙いどおりの解析結果を得られるようになると思われますが、
追加した語のかたちによっては、生起コストをより小さな値に調整しないと、一部の文において正しく切り出されない場合があるかもしれません。

たとえば、[こちらの記事](https://analytics-note.xyz/programming/mecab-cost-fix/)で紹介されているように、
仮に`高等学校,1285,1285,5078,名詞,一般,*,*,*,*,高等学校,コウトウガッコウ,コートーガッコー`という見出し語を追加したとしても、
与える文によっては`高等学校`が狙いどおりに切り出されません。

```{r}
writeLines(
  c(
    "高等学校,1285,1285,5078,名詞,一般,*,*,*,*,高等学校,コウトウガッコウ,コートーガッコー"
  ),
  con = (csv_file <- tempfile(fileext = ".csv"))
)
gibasa::build_user_dic(
  dic_dir = stringr::str_sub(gibasa::dictionary_info()$file_path, end = -8),
  file = (user_dic <- tempfile(fileext = ".dic")),
  csv_file = csv_file,
  encoding = "utf8"
)
gibasa::tokenize(
  c("九州高等学校ゴルフ選手権",
    "地元の高等学校に進学した",
    "帝京高等学校のエースとして活躍",
    "開成高等学校117人が現役合格",
    "マンガを高等学校の授業で使う"),
  user_dic = user_dic
) |>
  gibasa::pack()
```

この例のように、複数の既存の見出し語のほうが優先されてしまう場合には、追加する見出し語の生起コストを小さくすることによって、
狙いどおりの解析結果を得ることができます。

先ほどの記事のなかで紹介されているのと同じやり方で、適切な生起コストをgibasaを使って求めるには、たとえば次のような関数を用意します（やや複雑なのでバグがあるかもしれません）。

```{r}
calc_adjusted_cost <- \(sentences, target_word, sys_dic = "", user_dic = "") {
  sentences_mod <-
    stringi::stri_replace_all_regex(
      sentences,
      pattern = paste0("(?<target>(", target_word, "))"),
      replacement = "\n${target}\t*\n",
      vectorize_all = FALSE
    )
  calc_cumcost <- \(x) {
    ret <-
      gibasa::posDebugRcpp(x, sys_dic = sys_dic, user_dic = user_dic, partial = TRUE) |>
      dplyr::mutate(
        lcAttr = dplyr::lead(lcAttr, default = 0),
        cost = purrr::map2_dbl(rcAttr, lcAttr,
        ~ gibasa::get_transition_cost(.x, .y, sys_dic = sys_dic, user_dic = user_dic)),
        wcost = cumsum(wcost),
        cost = cumsum(cost),
        ## 1行目のBOS/EOS->BOS/EOS間の連接コストを足しすぎてしまうので、引く
        total_cost = wcost + cost - gibasa::get_transition_cost(0, 0, sys_dic = sys_dic, user_dic = user_dic),
        .by = doc_id
      ) |>
      dplyr::slice_tail(n = 1, by = doc_id) |>
      dplyr::pull("total_cost")
    ret
  }
  cost1 <- calc_cumcost(sentences)
  cost2 <- calc_cumcost(sentences_mod)

  gibasa::posDebugRcpp(sentences_mod, sys_dic = sys_dic, user_dic = user_dic, partial = TRUE) |>
    dplyr::filter(surface %in% target_word) |>
    dplyr::reframe(
      stat = stat,
      surface = surface,
      pos_id = pos_id,
      feature = feature,
      lcAttr = lcAttr,
      rcAttr = rcAttr,
      current_cost = wcost,
      adjusted_cost = wcost + (cost1[doc_id] - cost2[doc_id] - 1)
    ) |>
    dplyr::slice_min(adjusted_cost, n = 1, by = surface)
}
```

この関数を使って、実際に適切な生起コストを計算してみます。

```{r}
adjusted_cost <-
  calc_adjusted_cost(
    c("九州高等学校ゴルフ選手権",
      "地元の高等学校に進学した",
      "帝京高等学校のエースとして活躍",
      "開成高等学校117人が現役合格",
      "マンガを高等学校の授業で使う"),
    target_word = "高等学校",
    user_dic = user_dic
  )
```

この生起コストを使って改めてユーザー辞書をビルドし、結果を確認してみましょう。

```{r}
adjusted_cost |>
  tidyr::unite(
    csv_body,
    surface, lcAttr, rcAttr, adjusted_cost, feature,
    sep = ","
  ) |>
  dplyr::pull("csv_body") |>
  writeLines(con = (csv_file <- tempfile(fileext = ".csv")))

gibasa::build_user_dic(
  dic_dir = stringr::str_sub(gibasa::dictionary_info()$file_path, end = -8),
  file = (user_dic <- tempfile(fileext = ".dic")),
  csv_file = csv_file,
  encoding = "utf8"
)

gibasa::tokenize(
  c("九州高等学校ゴルフ選手権",
    "地元の高等学校に進学した",
    "帝京高等学校のエースとして活躍",
    "開成高等学校117人が現役合格",
    "マンガを高等学校の授業で使う"),
  user_dic = user_dic
) |>
  gibasa::pack()
```

今度はうまくいっていそうです。


### システム辞書のビルド

ユーザー辞書ではなく、システム辞書をビルドすることもできます。
ただ、ふつうに入手できるIPA辞書のソースの文字コードは`EUC-JP`であり、UTF-8のCSVファイルと混在させることができないため、扱いに注意が必要です。

また、UniDicの2.3.xについては、同梱されているファイルに問題があるようで、そのままではビルドできないという話があるようです（[参考](https://ja.stackoverflow.com/questions/74178/)）。
UniDicはIPA辞書に比べるとビルドするのにそれなりにメモリが必要なことからも、とくに事情がないかぎりはビルド済みのバイナリ辞書をダウンロードしてきて使ったほうがよいでしょう。

```{r}
ipadic_temp <- tempfile(fileext = ".tar.gz")
download.file("https://github.com/shogo82148/mecab/releases/download/v0.996.9/mecab-ipadic-2.7.0-20070801.tar.gz", destfile = ipadic_temp)
untar(ipadic_temp, exdir = tempdir())

gibasa::build_sys_dic(
  dic_dir = file.path(tempdir(), "mecab-ipadic-2.7.0-20070801"),
  out_dir = tempdir(),
  encoding = "euc-jp"
)

# `dicrc`ファイルをビルドした辞書のあるディレクトリにコピーする
file.copy(file.path(tempdir(), "mecab-ipadic-2.7.0-20070801/dicrc"), tempdir())

# ここでは`mecabrc`ファイルが適切な位置に配置されていないという想定で、
# `mecabrc`ファイルを偽装している。
withr::with_envvar(
  c(
    "MECABRC" = if (.Platform$OS.type == "windows") {
      "nul"
    } else {
      "/dev/null"
    }
  ),
  gibasa::tokenize("月ノ美兎は箱の中", sys_dic = tempdir())
)
```
